{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc1d2c92",
   "metadata": {},
   "source": [
    "See reference: https://github.com/microsoft/Phi-3CookBook/blob/main/code/04.Finetuning/Phi-3-finetune-qlora-python.ipynb\n",
    "\n",
    "He Zhang, Oct. 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc0989f",
   "metadata": {},
   "source": [
    "## Installing and loading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "922f6a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade bitsandbytes\n",
    "#%pip install --upgrade transformers\n",
    "#%pip install --upgrade peft \n",
    "#%pip install --upgrade accelerate \n",
    "#%pip install --upgrade datasets \n",
    "#%pip install --upgrade trl \n",
    "#%pip install --upgrade flash_attn \n",
    "#%pip install --upgrade torch \n",
    "#%pip install --upgrade wandb\n",
    "\n",
    "#%pip install huggingface_hub\n",
    "#%pip install python-dotenv\n",
    "#%pip install ipywidgets\n",
    "\n",
    "#%pip install --upgrade absl-py \n",
    "#%pip install --upgrade nltk \n",
    "#%pip install --upgrade rouge_score\n",
    "#%pip install --upgrade evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c769da74",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccb19ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from random import randrange\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, TaskType, PeftModel\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b36e0f9",
   "metadata": {},
   "source": [
    "## Setting Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "064911b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'model_id' and 'model_name' are the identifiers for the pre-trained model that you want to fine-tune. \n",
    "# In this case, it's the 'Phi-3-mini-4k-instruct' model from Microsoft.\n",
    "# microsoft/Phi-3-mini-4k-instruct\n",
    "# microsoft/Phi-3-small-128k-instruct\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# In this case, it's the 'python_code_instructions_18k_alpaca' dataset from iamtarun (Ex: iamtarun/python_code_instructions_18k_alpaca).\n",
    "# Update Dataset Name to your dataset name\n",
    "dataset_name = \"iamtarun/python_code_instructions_18k_alpaca\"\n",
    "\n",
    "# 'dataset_split' is the split of the dataset that you want to use for training. \n",
    "# In this case, it's the 'train' split.\n",
    "dataset_split= \"train\"\n",
    "\n",
    "# 'new_model' is the name that you want to give to the fine-tuned model.\n",
    "new_model = \"phi-3-mini-4k-qlora-ft-code-gen\"\n",
    "\n",
    "# 'hf_model_repo' is the repository on the Hugging Face Model Hub where the fine-tuned model will be saved. Update UserName to your Hugging Face Username\n",
    "hf_model_repo=\"HeZ/\"+new_model\n",
    "\n",
    "# 'device_map' is a dictionary that maps the model to the GPU device. \n",
    "# In this case, the entire model is loaded on GPU 0.\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# Bits and Bytes configuration for the model\n",
    "\n",
    "# 'use_4bit' is a boolean that controls whether 4-bit precision should be used for loading the base model.\n",
    "use_4bit = True\n",
    "\n",
    "# 'bnb_4bit_compute_dtype' is the data type that should be used for computations with the 4-bit base model. In this case, it is set to 'bfloat16'.\n",
    "bnb_4bit_compute_dtype = \"bfloat16\"\n",
    "\n",
    "# 'bnb_4bit_quant_type' is the type of quantization that should be used for the 4-bit base model. In this case, it is set to 'nf4'.\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# 'use_double_quant' is a boolean that controls whether nested quantization should be used for the 4-bit base model.\n",
    "use_double_quant = True\n",
    "\n",
    "# The following are parameters for the LoRA (Learning from Random Architecture) model.\n",
    "\n",
    "# 'lora_r' is the dimension of the LoRA attention.\n",
    "lora_r = 16\n",
    "\n",
    "# 'lora_alpha' is the alpha parameter for LoRA scaling.\n",
    "lora_alpha = 16\n",
    "\n",
    "# 'lora_dropout' is the dropout probability for LoRA layers.\n",
    "lora_dropout = 0.05\n",
    "\n",
    "# 'target_modules' is a list of the modules in the model that will be replaced with LoRA layers.\n",
    "target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    "\n",
    "# 'set_seed' is a function that sets the seed for generating random numbers, \n",
    "# which is used for reproducibility of the results.\n",
    "set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92720e9d",
   "metadata": {},
   "source": [
    "## Connect to Huggingface Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfcaf7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/azureuser/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# This code block is used to log in to the Hugging Face Model Hub using an API token stored in an environment variable.\n",
    "\n",
    "# 'login' is a function from the 'huggingface_hub' library that logs you in to the Hugging Face Model Hub using an API token.\n",
    "from huggingface_hub import login, model_info\n",
    "\n",
    "# 'load_dotenv' is a function from the 'python-dotenv' library that loads environment variables from a .env file.\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 'os' is a standard Python library that provides functions for interacting with the operating system.\n",
    "import os\n",
    "\n",
    "# Call the 'load_dotenv' function to load the environment variables from the .env file.\n",
    "load_dotenv(\"hf_token.env\")\n",
    "\n",
    "# Call the 'login' function with the 'HF_HUB_TOKEN' environment variable to log in to the Hugging Face Model Hub.\n",
    "# 'os.getenv' is a function that gets the value of an environment variable.\n",
    "login(token=os.getenv(\"HF_HUB_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83c5ee86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelInfo(id='HeZ/test-repo-v1', author='HeZ', sha='379f90c9789e65163946e8b0518641e365fcbfec', created_at=datetime.datetime(2024, 10, 1, 8, 34, 7, tzinfo=datetime.timezone.utc), last_modified=datetime.datetime(2024, 10, 1, 8, 34, 7, tzinfo=datetime.timezone.utc), private=False, disabled=False, downloads=0, downloads_all_time=None, gated=False, gguf=None, inference=None, likes=0, library_name=None, tags=['region:us'], pipeline_tag=None, mask_token=None, card_data=None, widget_data=None, model_index=None, config=None, transformers_info=None, trending_score=None, siblings=[RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)], spaces=[], safetensors=None)\n"
     ]
    }
   ],
   "source": [
    "# login check to see some model information\n",
    "test_model_id = \"HeZ/test-repo-v1\" \n",
    "info = model_info(test_model_id)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1790e437",
   "metadata": {},
   "source": [
    "## Load the dataset with the instruction set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1bfe2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'prompt'],\n",
       "    num_rows: 18612\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code block is used to load a dataset from the Hugging Face Dataset Hub, print its size, and show a random example from the dataset.\n",
    "\n",
    "# 'load_dataset' is a function from the 'datasets' library that loads a dataset from the Hugging Face Dataset Hub.\n",
    "# 'dataset_name' is the name of the dataset to load, and 'dataset_split' is the split of the dataset to load (e.g., 'train', 'test').\n",
    "dataset = load_dataset(dataset_name, split=dataset_split)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "840b7579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 18612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Create a function to calculate the sum of a sequence of integers.',\n",
       " 'input': '[1, 2, 3, 4, 5]',\n",
       " 'output': '# Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum',\n",
       " 'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCreate a function to calculate the sum of a sequence of integers.\\n\\n### Input:\\n[1, 2, 3, 4, 5]\\n\\n### Output:\\n# Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Create a function to calculate the sum of a sequence of integers.\n",
      "\n",
      "### Input:\n",
      "[1, 2, 3, 4, 5]\n",
      "\n",
      "### Output:\n",
      "# Python code\n",
      "def sum_sequence(sequence):\n",
      "  sum = 0\n",
      "  for num in sequence:\n",
      "    sum += num\n",
      "  return sum\n"
     ]
    }
   ],
   "source": [
    "# The 'len' function is used to get the size of the dataset, which is then printed.\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "\n",
    "display(dataset[0])\n",
    "print(dataset[0][\"prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bf799c",
   "metadata": {},
   "source": [
    "## Load the tokenizer to prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bb24ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'tokenizer_id' is the identifier for the tokenizer that you want to load. In this case, it is set to the value of 'model_id', which means that the tokenizer associated with the pre-trained model will be loaded.\n",
    "\n",
    "# 'AutoTokenizer' is a class from the 'transformers' library that provides a generic tokenizer class from which all other tokenizer classes inherit.\n",
    "\n",
    "# 'from_pretrained' is a method of the 'AutoTokenizer' class that loads a tokenizer from the Hugging Face Model Hub.\n",
    "\n",
    "# 'tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)' loads the tokenizer associated with 'tokenizer_id' from the Hugging Face Model Hub and assigns it to the variable 'tokenizer'.\n",
    "\n",
    "# 'tokenizer.padding_side' is a property of the 'tokenizer' object that determines on which side of the input sequences padding should be added. It can be set to either 'left' or 'right'.\n",
    "\n",
    "# 'tokenizer.padding_side = 'right'' sets 'tokenizer.padding_side' to 'right', which means that padding will be added to the right side of the input sequences. This is done to prevent warnings that can occur when 'tokenizer.padding_side' is set to 'left'.\n",
    "tokenizer_id = model_id\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "tokenizer.padding_side = 'right' # to prevent warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdbb9c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'create_message_column' is a function that takes a row from a dataset and returns a dictionary with a single key-value pair. The key is 'messages' and the value is a list of dictionaries, each representing a message.\n",
    "\n",
    "# 'row' is the input to the 'create_message_column' function. It is expected to be a dictionary with keys 'instruction', 'input', and 'output'.\n",
    "\n",
    "# 'messages' is a list that will contain the messages.\n",
    "\n",
    "# 'user' is a dictionary that represents a user message. The 'content' key contains the instruction and input from the row, and the 'role' key is set to 'user'.\n",
    "\n",
    "# 'messages.append(user)' adds the user message to the 'messages' list.\n",
    "\n",
    "# 'assistant' is a dictionary that represents an assistant message. The 'content' key contains the output from the row, and the 'role' key is set to 'assistant'.\n",
    "\n",
    "# 'messages.append(assistant)' adds the assistant message to the 'messages' list.\n",
    "\n",
    "# 'return {\"messages\": messages}' returns a dictionary with a single key-value pair. The key is 'messages' and the value is the 'messages' list.\n",
    "\n",
    "# 'format_dataset_chatml' is a function that takes a row from a dataset and returns a dictionary with a single key-value pair. The key is 'text' and the value is the result of applying the chat template to the messages in the row.\n",
    "\n",
    "# 'row' is the input to the 'format_dataset_chatml' function. It is expected to be a dictionary with a key 'messages'.\n",
    "\n",
    "# 'return {\"text\": tokenizer.apply_chat_template(row[\"messages\"], add_generation_prompt=False, tokenize=False)}' returns a dictionary with a single key-value pair. The key is 'text' and the value is the result of applying the chat template to the messages in the row. The 'add_generation_prompt' parameter is set to False, which means that no generation prompt will be added to the end of the text. The 'tokenize' parameter is set to False, which means that the text will not be tokenized.\n",
    "def create_message_column(row):\n",
    "    messages = []\n",
    "    user = {\n",
    "        \"content\": f\"{row['instruction']}\\n Input: {row['input']}\",\n",
    "        \"role\": \"user\"\n",
    "    }\n",
    "    messages.append(user)\n",
    "    assistant = {\n",
    "        \"content\": f\"{row['output']}\",\n",
    "        \"role\": \"assistant\"\n",
    "    }\n",
    "    messages.append(assistant)\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def format_dataset_chatml(row):\n",
    "    return {\"text\": tokenizer.apply_chat_template(row[\"messages\"], add_generation_prompt=False, tokenize=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "435c3aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to prepare the 'dataset' for training a chat model.\n",
    "\n",
    "# 'dataset.map' is a method that applies a function to each example in the 'dataset'.\n",
    "# 'create_message_column' is a function that formats each example into a 'messages' format suitable for a chat model.\n",
    "# The result is a new 'dataset_chatml' with the formatted examples.\n",
    "dataset_chatml = dataset.map(create_message_column)\n",
    "\n",
    "# 'dataset_chatml.map' is a method that applies a function to each example in the 'dataset_chatml'.\n",
    "# 'format_dataset_chatml' is a function that further formats each example into a single string of chat messages.\n",
    "# The result is an updated 'dataset_chatml' with the further formatted examples.\n",
    "dataset_chatml = dataset_chatml.map(format_dataset_chatml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af2ab46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'prompt', 'messages', 'text'],\n",
       "    num_rows: 18612\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Create a function to calculate the sum of a sequence of integers.',\n",
       " 'input': '[1, 2, 3, 4, 5]',\n",
       " 'output': '# Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum',\n",
       " 'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCreate a function to calculate the sum of a sequence of integers.\\n\\n### Input:\\n[1, 2, 3, 4, 5]\\n\\n### Output:\\n# Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum',\n",
       " 'messages': [{'content': 'Create a function to calculate the sum of a sequence of integers.\\n Input: [1, 2, 3, 4, 5]',\n",
       "   'role': 'user'},\n",
       "  {'content': '# Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum',\n",
       "   'role': 'assistant'}],\n",
       " 'text': '<|user|>\\nCreate a function to calculate the sum of a sequence of integers.\\n Input: [1, 2, 3, 4, 5]<|end|>\\n<|assistant|>\\n# Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum<|end|>\\n<|endoftext|>'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Create a function to calculate the sum of a sequence of integers.\n",
      " Input: [1, 2, 3, 4, 5]<|end|>\n",
      "<|assistant|>\n",
      "# Python code\n",
      "def sum_sequence(sequence):\n",
      "  sum = 0\n",
      "  for num in sequence:\n",
      "    sum += num\n",
      "  return sum<|end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# This line of code is used to access and display the first example from the 'dataset_chatml'.\n",
    "\n",
    "# 'dataset_chatml[0]' uses indexing to access the first example in the 'dataset_chatml'.\n",
    "# In Python, indexing starts at 0, so 'dataset_chatml[0]' refers to the first example.\n",
    "# The result is a dictionary with a 'text' key and a string of formatted chat messages as its value.\n",
    "display(dataset_chatml)\n",
    "\n",
    "display(dataset_chatml[0])\n",
    "\n",
    "print(dataset_chatml[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7607bfb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'prompt', 'messages', 'text'],\n",
       "        num_rows: 17681\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'prompt', 'messages', 'text'],\n",
       "        num_rows: 931\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code block is used to split the 'dataset_chatml' into training and testing sets.\n",
    "\n",
    "# 'dataset_chatml.train_test_split' is a method that splits the 'dataset_chatml' into a training set and a testing set.\n",
    "# 'test_size' is a parameter that specifies the proportion of the 'dataset_chatml' to include in the testing set. Here it's set to 0.05, meaning that 5% of the 'dataset_chatml' will be included in the testing set.\n",
    "# 'seed' is a parameter that sets the seed for the random number generator. This is used to ensure that the split is reproducible. Here it's set to 1234.\n",
    "dataset_chatml = dataset_chatml.train_test_split(test_size=0.05, seed=1234)\n",
    "\n",
    "# This line of code is used to display the structure of the 'dataset_chatml' after the split.\n",
    "# It will typically show information such as the number of rows in the training set and the testing set.\n",
    "dataset_chatml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeef216b",
   "metadata": {},
   "source": [
    "## Instruction fine-tune a Phi-3-mini model using QLORA and trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55d23fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flash_attention_2\n"
     ]
    }
   ],
   "source": [
    "# This code block is used to set the compute data type and attention implementation based on whether bfloat16 is supported on the current CUDA device.\n",
    "\n",
    "# 'torch.cuda.is_bf16_supported()' is a function that checks if bfloat16 is supported on the current CUDA device.\n",
    "# If bfloat16 is supported, 'compute_dtype' is set to 'torch.bfloat16' and 'attn_implementation' is set to 'flash_attention_2'.\n",
    "if torch.cuda.is_bf16_supported():\n",
    "    compute_dtype = torch.bfloat16\n",
    "    attn_implementation = 'flash_attention_2'\n",
    "# If bfloat16 is not supported, 'compute_dtype' is set to 'torch.float16' and 'attn_implementation' is set to 'sdpa'.\n",
    "else:\n",
    "    compute_dtype = torch.float16\n",
    "    attn_implementation = 'sdpa'\n",
    "\n",
    "# This line of code is used to print the value of 'attn_implementation', which indicates the chosen attention implementation.\n",
    "print(attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fca81e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b982519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set flash attention to be 'eager' mode, if you run this notebook using Nvidia V100 GPU\n",
    "attn_implementation = 'eager'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b390a35",
   "metadata": {},
   "source": [
    "## Load the tokenizer and model to finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31a82099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a096a703706469a84cc91cd75d86f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 'AutoTokenizer' is a class from the Hugging Face Transformers library that provides a tokenizer for a given pre-trained model.\n",
    "\n",
    "# 'from_pretrained' is a method of the 'AutoTokenizer' class that loads a tokenizer from a pre-trained model.\n",
    "\n",
    "# 'model_name' is a variable that contains the name of the pre-trained model.\n",
    "\n",
    "# 'trust_remote_code=True' is a parameter that allows the execution of remote code when loading the tokenizer.\n",
    "\n",
    "# 'add_eos_token=True' is a parameter that adds an end-of-sentence token to the tokenizer.\n",
    "\n",
    "# 'use_fast=True' is a parameter that uses the fast version of the tokenizer, if available.\n",
    "\n",
    "# 'tokenizer.pad_token = tokenizer.unk_token' sets the padding token of the tokenizer to be the same as the unknown token.\n",
    "\n",
    "# 'tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)' sets the ID of the padding token to be the same as the ID of the padding token.\n",
    "\n",
    "# 'tokenizer.padding_side = 'left'' sets the side where padding will be added to be the left side.\n",
    "\n",
    "# 'BitsAndBytesConfig' is a class that provides a configuration for quantization.\n",
    "\n",
    "# 'bnb_config' is a variable that holds the configuration for quantization.\n",
    "\n",
    "# 'AutoModelForCausalLM' is a class from the Hugging Face Transformers library that provides a model for causal language modeling.\n",
    "\n",
    "# 'from_pretrained' is a method of the 'AutoModelForCausalLM' class that loads a model from a pre-trained model.\n",
    "\n",
    "# 'torch_dtype=compute_dtype' is a parameter that sets the data type of the model to be the same as 'compute_dtype'.\n",
    "\n",
    "# 'quantization_config=bnb_config' is a parameter that sets the configuration for quantization to be 'bnb_config'.\n",
    "\n",
    "# 'device_map=device_map' is a parameter that sets the device map of the model to be 'device_map'.\n",
    "\n",
    "# 'attn_implementation=attn_implementation' is a parameter that sets the type of attention implementation to be 'attn_implementation'.\n",
    "\n",
    "# 'prepare_model_for_kbit_training' is a function that prepares a model for k-bit training.\n",
    "\n",
    "# 'model = prepare_model_for_kbit_training(model)' prepares 'model' for k-bit training and assigns the result back to 'model'.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, add_eos_token=True, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=use_4bit,\n",
    "        bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=use_double_quant,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "          model_name, torch_dtype=compute_dtype, trust_remote_code=True, quantization_config=bnb_config, device_map=device_map,\n",
    "          attn_implementation=attn_implementation\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "925358ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to define the training arguments for the model.\n",
    "\n",
    "# 'TrainingArguments' is a class that holds the arguments for training a model.\n",
    "# 'output_dir' is the directory where the model and its checkpoints will be saved.\n",
    "# 'evaluation_strategy' is set to \"steps\", meaning that evaluation will be performed after a certain number of training steps.\n",
    "# 'do_eval' is set to True, meaning that evaluation will be performed.\n",
    "# 'optim' is set to \"adamw_torch\", meaning that the AdamW optimizer from PyTorch will be used.\n",
    "# 'per_device_train_batch_size' and 'per_device_eval_batch_size' are set to 8, meaning that the batch size for training and evaluation will be 8 per device.\n",
    "# 'gradient_accumulation_steps' is set to 4, meaning that gradients will be accumulated over 4 steps before performing a backward/update pass.\n",
    "# 'log_level' is set to \"debug\", meaning that all log messages will be printed.\n",
    "# 'save_strategy' is set to \"epoch\", meaning that the model will be saved after each epoch.\n",
    "# 'logging_steps' is set to 100, meaning that log messages will be printed every 100 steps.\n",
    "# 'learning_rate' is set to 1e-4, which is the learning rate for the optimizer.\n",
    "# 'fp16' is set to the opposite of whether bfloat16 is supported on the current CUDA device.\n",
    "# 'bf16' is set to whether bfloat16 is supported on the current CUDA device.\n",
    "# 'eval_steps' is set to 100, meaning that evaluation will be performed every 100 steps.\n",
    "# 'num_train_epochs' is set to 3, meaning that the model will be trained for 3 epochs.\n",
    "# 'warmup_ratio' is set to 0.1, meaning that 10% of the total training steps will be used for the warmup phase.\n",
    "# 'lr_scheduler_type' is set to \"linear\", meaning that a linear learning rate scheduler will be used.\n",
    "# 'report_to' is set to \"wandb\", meaning that training and evaluation metrics will be reported to Weights & Biases.\n",
    "# 'seed' is set to 42, which is the seed for the random number generator.\n",
    "\n",
    "# LoraConfig object is created with the following parameters:\n",
    "# 'r' (rank of the low-rank approximation) is set to 16,\n",
    "# 'lora_alpha' (scaling factor) is set to 16,\n",
    "# 'lora_dropout' dropout probability for Lora layers is set to 0.05,\n",
    "# 'task_type' (set to TaskType.CAUSAL_LM indicating the task type),\n",
    "# 'target_modules' (the modules to which LoRA is applied) choosing linear layers except the output layer..\n",
    "\n",
    "args = TrainingArguments(\n",
    "        output_dir=\"./phi-3-mini-QLoRA\",\n",
    "        eval_strategy=\"steps\",\n",
    "        do_eval=True,\n",
    "        optim=\"adamw_torch\",\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=4,\n",
    "        per_device_eval_batch_size=8,\n",
    "        log_level=\"debug\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=10,\n",
    "        learning_rate=1e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        eval_steps=10,\n",
    "        num_train_epochs=1,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        report_to=\"wandb\",\n",
    "        seed=42,\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=target_modules,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7645c160",
   "metadata": {},
   "source": [
    "## Establish a connection with wandb and enlist the project and experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdbbfb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhezhang33\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code block is used to initialize Weights & Biases (wandb), a tool for tracking and visualizing machine learning experiments.\n",
    "\n",
    "# 'import wandb' is used to import the wandb library.\n",
    "import wandb\n",
    "\n",
    "# 'wandb.login()' is a method that logs you into your Weights & Biases account.\n",
    "# If you're not already logged in, it will prompt you to log in.\n",
    "# Once you're logged in, you can use Weights & Biases to track and visualize your experiments.\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19dae216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/batch/tasks/shared/LS_root/mounts/clusters/gpu-nc6s-v3/code/Users/Phi3_Fine_Tuning/wandb/run-20241009_101340-4cp0bmpi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hezhang33/Phi3-mini-qlora-ft-python-code/runs/4cp0bmpi' target=\"_blank\">phi-3-mini-qlora-ft-py-3e-runs</a></strong> to <a href='https://wandb.ai/hezhang33/Phi3-mini-qlora-ft-python-code' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hezhang33/Phi3-mini-qlora-ft-python-code' target=\"_blank\">https://wandb.ai/hezhang33/Phi3-mini-qlora-ft-python-code</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hezhang33/Phi3-mini-qlora-ft-python-code/runs/4cp0bmpi' target=\"_blank\">https://wandb.ai/hezhang33/Phi3-mini-qlora-ft-python-code/runs/4cp0bmpi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hezhang33/Phi3-mini-qlora-ft-python-code/runs/4cp0bmpi?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fe4b944c160>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code block is used to initialize a Weights & Biases (wandb) run.\n",
    "\n",
    "# 'project_name' is set to the name of the project in Weights & Biases.\n",
    "project_name = \"Phi3-mini-qlora-ft-python-code\"\n",
    "\n",
    "# 'wandb.init' is a method that initializes a new Weights & Biases run.\n",
    "# 'project' is set to 'project_name', meaning that the run will be associated with this project.\n",
    "# 'name' is set to \"phi-3-mini-ft-py-3e\", which is the name of the run.\n",
    "# Each run has a unique name which can be used to identify it in the Weights & Biases dashboard.\n",
    "wandb.init(project=project_name, name=\"phi-3-mini-qlora-ft-py-3e-runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b76447b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "# 'SFTTrainer' is a class that provides a trainer for fine-tuning a model.\n",
    "\n",
    "# 'trainer' is a variable that holds the trainer.\n",
    "\n",
    "# 'model=model' is a parameter that sets the model to be trained to be 'model'.\n",
    "\n",
    "# 'train_dataset=dataset_chatml['train']' is a parameter that sets the training dataset to be 'dataset_chatml['train']'.\n",
    "\n",
    "# 'eval_dataset=dataset_chatml['test']' is a parameter that sets the evaluation dataset to be 'dataset_chatml['test']'.\n",
    "\n",
    "# 'peft_config=peft_config' is a parameter that sets the configuration for the Lora layer to be 'peft_config'.\n",
    "\n",
    "# 'dataset_text_field=\"text\"' is a parameter that sets the field in the dataset that contains the text to be 'text'.\n",
    "\n",
    "# 'max_seq_length=512' is a parameter that sets the maximum sequence length for the model to be 512.\n",
    "\n",
    "# 'tokenizer=tokenizer' is a parameter that sets the tokenizer to be 'tokenizer'.\n",
    "\n",
    "# 'args=args' is a parameter that sets the training arguments to be 'args'.\n",
    "\n",
    "# This line of code is used to create a trainer for fine-tuning the model with the specified parameters.\n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset_chatml['train'],\n",
    "        eval_dataset=dataset_chatml['test'],\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=512,\n",
    "        tokenizer=tokenizer,\n",
    "        args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b44831",
   "metadata": {},
   "source": [
    "#### Note by He Zhang: total number of training steps = (total samples * number of epochs) / gradient accumulation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e10d337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently training with a batch size of: 8\n",
      "***** Running training *****\n",
      "  Num examples = 17,681\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 552\n",
      "  Number of trainable parameters = 8,912,896\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n",
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='451' max='552' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [451/552 7:19:05 < 1:38:46, 0.02 it/s, Epoch 0.81/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.019300</td>\n",
       "      <td>1.058206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.040800</td>\n",
       "      <td>1.012917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.995700</td>\n",
       "      <td>0.909531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.804700</td>\n",
       "      <td>0.771116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.699000</td>\n",
       "      <td>0.668948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.614300</td>\n",
       "      <td>0.638497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.647200</td>\n",
       "      <td>0.617485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.607700</td>\n",
       "      <td>0.607318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.602800</td>\n",
       "      <td>0.601198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.592900</td>\n",
       "      <td>0.597763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.611700</td>\n",
       "      <td>0.594804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.590400</td>\n",
       "      <td>0.592480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.585200</td>\n",
       "      <td>0.590881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.566200</td>\n",
       "      <td>0.589548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.618300</td>\n",
       "      <td>0.587992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.587200</td>\n",
       "      <td>0.587285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.580700</td>\n",
       "      <td>0.586268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.616900</td>\n",
       "      <td>0.585295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.570500</td>\n",
       "      <td>0.584114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.614300</td>\n",
       "      <td>0.583490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.570500</td>\n",
       "      <td>0.582827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.568300</td>\n",
       "      <td>0.582051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.607700</td>\n",
       "      <td>0.581769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.586000</td>\n",
       "      <td>0.581144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.572400</td>\n",
       "      <td>0.580426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.594100</td>\n",
       "      <td>0.579852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.598900</td>\n",
       "      <td>0.579776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.558200</td>\n",
       "      <td>0.579309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.579800</td>\n",
       "      <td>0.579232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.554500</td>\n",
       "      <td>0.578518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.597000</td>\n",
       "      <td>0.578263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.609300</td>\n",
       "      <td>0.577873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.573600</td>\n",
       "      <td>0.577751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.569800</td>\n",
       "      <td>0.577218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.565900</td>\n",
       "      <td>0.576904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.587700</td>\n",
       "      <td>0.576350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.583700</td>\n",
       "      <td>0.576317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.585800</td>\n",
       "      <td>0.576134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.587700</td>\n",
       "      <td>0.575994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.580200</td>\n",
       "      <td>0.575585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.600900</td>\n",
       "      <td>0.575446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.571300</td>\n",
       "      <td>0.575075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.550900</td>\n",
       "      <td>0.575124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.564600</td>\n",
       "      <td>0.575008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='117' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 48/117 01:59 < 02:56, 0.39 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 8\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This code block is used to train the model and save it locally.\n",
    "\n",
    "# 'trainer.train()' is a method that starts the training of the model.\n",
    "# It uses the training dataset, evaluation dataset, and training arguments that were provided when the trainer was initialized.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8ae241d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HeZ/phi-3-mini-4k-qlora-ft-code-gen'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3874c420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi-3-mini-QLoRA\n",
      "loading configuration file config.json from cache at /home/azureuser/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./phi-3-mini-QLoRA/tokenizer_config.json\n",
      "Special tokens file saved in ./phi-3-mini-QLoRA/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# 'trainer.save_model()' is a method that saves the trained model locally.\n",
    "# The model will be saved in the directory specified by 'output_dir' in the training arguments.\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e904c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi-3-mini-QLoRA\n",
      "loading configuration file config.json from cache at /home/azureuser/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./phi-3-mini-QLoRA/tokenizer_config.json\n",
      "Special tokens file saved in ./phi-3-mini-QLoRA/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "419a48cc82e548338b4eb0a5bd307005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4d90950bc54a6f8b6888e278425223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/35.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6ee61b4fc44b09a9140fb816635ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/HeZ/phi-3-mini-QLoRA/commit/eef1a66c9c56efa74218208302f444a74e24f006', commit_message='HeZ/phi-3-mini-4k-qlora-ft-code-gen', commit_description='', oid='eef1a66c9c56efa74218208302f444a74e24f006', pr_url=None, repo_url=RepoUrl('https://huggingface.co/HeZ/phi-3-mini-QLoRA', endpoint='https://huggingface.co', repo_type='model', repo_id='HeZ/phi-3-mini-QLoRA'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code block is used to save the adapter to the Hugging Face Model Hub.\n",
    "\n",
    "# 'trainer.push_to_hub' is a method that pushes the trained model (or adapter in this case) to the Hugging Face Model Hub.\n",
    "# In this example, hf_model_repo=\"HeZ/phi-3-mini-4k-ft-code-gen\", i.e. the name of the repository on the Hugging Face Model Hub where the adapter will be saved.\n",
    "trainer.push_to_hub(hf_model_repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f687cbe4",
   "metadata": {},
   "source": [
    "## Combine the model and the adapters, then save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "131a452e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'del model' and 'del trainer' are lines of code that delete the 'model' and 'trainer' objects. This frees up the memory that was used by these objects.\n",
    "\n",
    "# 'import gc' is a line of code that imports the 'gc' module, which provides an interface to the garbage collector.\n",
    "\n",
    "# 'gc.collect()' is a function that triggers a full garbage collection. It frees up memory by collecting all the objects that are no longer in use.\n",
    "\n",
    "# This block of code is used to empty the VRAM (Video Random Access Memory) by deleting the 'model' and 'trainer' objects and then triggering a full garbage collection.\n",
    "# Empty VRAM\n",
    "del model\n",
    "del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d4dd145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'torch.cuda.empty_cache()' is a function from the PyTorch library that releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.\n",
    "\n",
    "# It's a PyTorch specific function to manage GPU memory and it doesn't affect the GPU memory usage by PyTorch tensors.\n",
    "\n",
    "# This line of code is used to empty the cache memory that's used by PyTorch on the GPU.\n",
    "torch.cuda.empty_cache() # PyTorch thing\n",
    "torch.cuda.empty_cache() # PyTorch thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a569a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('microsoft/Phi-3-mini-4k-instruct', 'HeZ/phi-3-mini-QLoRA', torch.bfloat16)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'hf_adapter_repo' is a variable that holds the repository name for the Hugging Face model adapter.\n",
    "\n",
    "# 'edumunozsala/phi-3-mini-QLoRA' is the repository name, where 'edumunozsala' is the username of the repository owner and 'phi-3-mini-QLoRA' is the name of the model adapter.\n",
    "\n",
    "# 'model_name, hf_adapter_repo, compute_dtype' is a line of code that returns the values of the 'model_name', 'hf_adapter_repo', and 'compute_dtype' variables.\n",
    "\n",
    "# This block of code is used to set the repository name for the Hugging Face model adapter and then return the values of the 'model_name', 'hf_adapter_repo', and 'compute_dtype' variables.\n",
    "hf_adapter_repo = \"HeZ/phi-3-mini-QLoRA\"\n",
    "\n",
    "model_name, hf_adapter_repo, compute_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d24bd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/azureuser/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/config.json\n",
      "loading configuration file config.json from cache at /home/azureuser/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/azureuser/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/model.safetensors.index.json\n",
      "Instantiating Phi3ForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a930dd9610c42a3b819b0f049cbb645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
      "\n",
      "All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/azureuser/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5595a68d1ce4abca09c60500e32ceab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e365a7ac6bc04d7ca02cf3c3abba5f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/35.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 'peft_model_id' and 'tr_model_id' are variables that hold the identifiers for the PEFT model and the transformer model, respectively.\n",
    "\n",
    "# 'AutoModelForCausalLM.from_pretrained(tr_model_id, trust_remote_code=True, torch_dtype=compute_dtype)' is a function that loads a pre-trained transformer model for causal language modeling. 'tr_model_id' is the identifier for the pre-trained model, 'trust_remote_code=True' allows the execution of code from the model file, and 'torch_dtype=compute_dtype' sets the data type for the PyTorch tensors.\n",
    "\n",
    "# 'PeftModel.from_pretrained(model, peft_model_id)' is a function that loads a pre-trained PEFT model. 'model' is the transformer model and 'peft_model_id' is the identifier for the pre-trained PEFT model.\n",
    "\n",
    "# 'model.merge_and_unload()' is a method that merges the PEFT model with the transformer model and then unloads the PEFT model.\n",
    "\n",
    "# This block of code is used to load a pre-trained transformer model and a pre-trained PEFT model, merge the two models, and then unload the PEFT model.\n",
    "peft_model_id = hf_adapter_repo\n",
    "tr_model_id = model_name\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(tr_model_id, trust_remote_code=True, torch_dtype=compute_dtype)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8d1034a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4800b42bdc64115b9bccbca736ff4e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd8eee19a1c446aae4aff86a814abd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.62M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a16cb45c7fb4fb1bd1b13330fea4284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/447 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at None\n",
      "loading file tokenizer.json from cache at /home/azureuser/.cache/huggingface/hub/models--HeZ--phi-3-mini-QLoRA/snapshots/eef1a66c9c56efa74218208302f444a74e24f006/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/azureuser/.cache/huggingface/hub/models--HeZ--phi-3-mini-QLoRA/snapshots/eef1a66c9c56efa74218208302f444a74e24f006/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/azureuser/.cache/huggingface/hub/models--HeZ--phi-3-mini-QLoRA/snapshots/eef1a66c9c56efa74218208302f444a74e24f006/tokenizer_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# 'tokenizer' is a variable that holds the tokenizer.\n",
    "\n",
    "# 'AutoTokenizer.from_pretrained(peft_model_id)' is a function from the Hugging Face Transformers library that loads a pre-trained tokenizer. 'peft_model_id' is the identifier for the pre-trained tokenizer.\n",
    "\n",
    "# This line of code is used to load a pre-trained tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d975c3a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HeZ/phi-3-mini-4k-qlora-ft-code-gen'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'hf_model_repo' is a variable that holds the repository name for the Hugging Face model.\n",
    "\n",
    "# This line of code is used to reference the repository name for the Hugging Face model.\n",
    "hf_model_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b58e84aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /tmp/tmpkki8w97i/config.json\n",
      "Configuration saved in /tmp/tmpkki8w97i/generation_config.json\n",
      "The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/tmpkki8w97i/model.safetensors.index.json.\n",
      "Uploading the following files to HeZ/phi-3-mini-4k-qlora-ft-code-gen: model-00002-of-00002.safetensors,config.json,README.md,generation_config.json,model-00001-of-00002.safetensors,model.safetensors.index.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9369e9350be14c0da4a8a06de999f379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638d0b698d664eb6883c9cf5c2ada168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d2910df2c04f10af5e2888bb9393d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c05e359526e474b83d2a993553982b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in /tmp/tmpkte16ssg/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/tmpkte16ssg/special_tokens_map.json\n",
      "Uploading the following files to HeZ/phi-3-mini-4k-qlora-ft-code-gen: README.md,special_tokens_map.json,tokenizer.json,tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/HeZ/phi-3-mini-4k-qlora-ft-code-gen/commit/b4dd44f929efe05ce90b6ec9e368c6ddc0e1941c', commit_message='Upload tokenizer', commit_description='', oid='b4dd44f929efe05ce90b6ec9e368c6ddc0e1941c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/HeZ/phi-3-mini-4k-qlora-ft-code-gen', endpoint='https://huggingface.co', repo_type='model', repo_id='HeZ/phi-3-mini-4k-qlora-ft-code-gen'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'merged_model_id' is a variable that holds the identifier for the merged model.\n",
    "\n",
    "# 'hf_model_repo' is the repository name for the Hugging Face model.\n",
    "\n",
    "# 'model.push_to_hub(merged_model_id)' is a method that pushes the merged model to the Hugging Face Model Hub. 'merged_model_id' is the identifier for the merged model.\n",
    "\n",
    "# 'tokenizer.push_to_hub(merged_model_id)' is a method that pushes the tokenizer to the Hugging Face Model Hub. 'merged_model_id' is the identifier for the tokenizer.\n",
    "\n",
    "# This block of code is used to save the merged model and the tokenizer to the Hugging Face Model Hub.\n",
    "# SAve the model merged to the Hub\n",
    "merged_model_id = hf_model_repo\n",
    "model.push_to_hub(merged_model_id)\n",
    "tokenizer.push_to_hub(merged_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b009a5e",
   "metadata": {},
   "source": [
    "## Model Inference and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84c65897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HeZ/phi-3-mini-4k-qlora-ft-code-gen'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'hf_model_repo' is a variable that holds the name of the repository on the Hugging Face Model Hub.\n",
    "# This is where the trained and merged model, as well as the tokenizer, have been saved.\n",
    "hf_model_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67607b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'': 0}, torch.bfloat16)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'device_map' is a variable that holds the mapping of the devices that are used for computation.\n",
    "\n",
    "# 'compute_dtype' is a variable that holds the data type that is used for computation.\n",
    "\n",
    "# This line of code is used to return the values of the 'device_map' and 'compute_dtype' variables.\n",
    "device_map, compute_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7655f069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e76b4106203c4ac18e3d5ecef4a9eae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f65775b381c464ebe891bb3de2276ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.62M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9af0b4ec6dd4aacb3a66cd7d64d13a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/561 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at None\n",
      "loading file tokenizer.json from cache at /home/azureuser/.cache/huggingface/hub/models--HeZ--phi-3-mini-4k-qlora-ft-code-gen/snapshots/b4dd44f929efe05ce90b6ec9e368c6ddc0e1941c/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/azureuser/.cache/huggingface/hub/models--HeZ--phi-3-mini-4k-qlora-ft-code-gen/snapshots/b4dd44f929efe05ce90b6ec9e368c6ddc0e1941c/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/azureuser/.cache/huggingface/hub/models--HeZ--phi-3-mini-4k-qlora-ft-code-gen/snapshots/b4dd44f929efe05ce90b6ec9e368c6ddc0e1941c/tokenizer_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b28ae85ee3b4e0a979500a1e7d29dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/azureuser/.cache/huggingface/hub/models--HeZ--phi-3-mini-4k-qlora-ft-code-gen/snapshots/b4dd44f929efe05ce90b6ec9e368c6ddc0e1941c/config.json\n",
      "loading configuration file config.json from cache at /home/azureuser/.cache/huggingface/hub/models--HeZ--phi-3-mini-4k-qlora-ft-code-gen/snapshots/b4dd44f929efe05ce90b6ec9e368c6ddc0e1941c/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"HeZ/phi-3-mini-4k-qlora-ft-code-gen\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85bddbadd59a46ea8604a75aa5257405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file model.safetensors from cache at /home/azureuser/.cache/huggingface/hub/models--HeZ--phi-3-mini-4k-qlora-ft-code-gen/snapshots/b4dd44f929efe05ce90b6ec9e368c6ddc0e1941c/model.safetensors.index.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd48c1c8404447bdb0f61495b727ad61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a218217c66b4bffb83de278f515c12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06febb246004b69940daefa28fe20dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Instantiating Phi3ForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e41c5ad640441aca54599eb9e2d2dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
      "\n",
      "All the weights of Phi3ForCausalLM were initialized from the model checkpoint at HeZ/phi-3-mini-4k-qlora-ft-code-gen.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f1aa4bd4d14c9f8c141bb6e9564b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file generation_config.json from cache at /home/azureuser/.cache/huggingface/hub/models--HeZ--phi-3-mini-4k-qlora-ft-code-gen/snapshots/b4dd44f929efe05ce90b6ec9e368c6ddc0e1941c/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This block of code is used to import the necessary libraries, set the seed for reproducibility, and load a pre-trained tokenizer and model.\n",
    "\n",
    "# 'import torch' is a line of code that imports the PyTorch library, which is a popular open-source machine learning library.\n",
    "\n",
    "# 'from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed' is a line of code that imports the 'AutoTokenizer', 'AutoModelForCausalLM', and 'set_seed' functions from the Hugging Face Transformers library.\n",
    "\n",
    "# 'set_seed(1234)' is a line of code that sets the seed for the random number generator to '1234'. This is done to ensure that the results are reproducible.\n",
    "\n",
    "# 'tokenizer = AutoTokenizer.from_pretrained(hf_model_repo,trust_remote_code=True)' is a line of code that loads a pre-trained tokenizer from the Hugging Face Model Hub. 'hf_model_repo' is the repository name for the model and 'trust_remote_code=True' allows the execution of code from the model file.\n",
    "\n",
    "# 'model = AutoModelForCausalLM.from_pretrained(hf_model_repo, trust_remote_code=True, torch_dtype=compute_dtype, device_map=device_map)' is a line of code that loads a pre-trained model for causal language modeling from the Hugging Face Model Hub. 'hf_model_repo' is the repository name for the model, 'trust_remote_code=True' allows the execution of code from the model file, 'torch_dtype=compute_dtype' sets the data type for the PyTorch tensors, and 'device_map=device_map' sets the device mapping.\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "\n",
    "set_seed(1234)  # For reproducibility\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_repo,trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_model_repo, trust_remote_code=True, torch_dtype=compute_dtype, device_map=device_map) # compute \"auto\" dev_map \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a993c821",
   "metadata": {},
   "source": [
    "#### Inference without pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e6187b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To solve the equation 2x + 3 = 7, you need to isolate the variable x. First, subtract 3 from both sides of the equation to get 2x = 4. Then, divide both sides by 2 to get x = 2. So the solution to the equation is x = 2.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d18388",
   "metadata": {},
   "source": [
    "#### Inference with pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f8be12b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'prompt', 'messages', 'text'],\n",
       "        num_rows: 17681\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'prompt', 'messages', 'text'],\n",
       "        num_rows: 931\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_chatml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70787e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Create an algorithm in Python to sort an array of numbers.',\n",
       " 'input': '[9, 3, 5, 1, 6]',\n",
       " 'output': 'def sortArray(arr):\\n    for i in range(len(arr)):\\n        for j in range(i+1,len(arr)):\\n            if arr[i] > arr[j]:\\n                temp = arr[i]\\n                arr[i] = arr[j]\\n                arr[j] = temp\\n    return arr\\n\\narr = [9, 3, 5, 1, 6]\\nprint(sortArray(arr))',\n",
       " 'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCreate an algorithm in Python to sort an array of numbers.\\n\\n### Input:\\n[9, 3, 5, 1, 6]\\n\\n### Output:\\ndef sortArray(arr):\\n    for i in range(len(arr)):\\n        for j in range(i+1,len(arr)):\\n            if arr[i] > arr[j]:\\n                temp = arr[i]\\n                arr[i] = arr[j]\\n                arr[j] = temp\\n    return arr\\n\\narr = [9, 3, 5, 1, 6]\\nprint(sortArray(arr))',\n",
       " 'messages': [{'content': 'Create an algorithm in Python to sort an array of numbers.\\n Input: [9, 3, 5, 1, 6]',\n",
       "   'role': 'user'},\n",
       "  {'content': 'def sortArray(arr):\\n    for i in range(len(arr)):\\n        for j in range(i+1,len(arr)):\\n            if arr[i] > arr[j]:\\n                temp = arr[i]\\n                arr[i] = arr[j]\\n                arr[j] = temp\\n    return arr\\n\\narr = [9, 3, 5, 1, 6]\\nprint(sortArray(arr))',\n",
       "   'role': 'assistant'}],\n",
       " 'text': '<|user|>\\nCreate an algorithm in Python to sort an array of numbers.\\n Input: [9, 3, 5, 1, 6]<|end|>\\n<|assistant|>\\ndef sortArray(arr):\\n    for i in range(len(arr)):\\n        for j in range(i+1,len(arr)):\\n            if arr[i] > arr[j]:\\n                temp = arr[i]\\n                arr[i] = arr[j]\\n                arr[j] = temp\\n    return arr\\n\\narr = [9, 3, 5, 1, 6]\\nprint(sortArray(arr))<|end|>\\n<|endoftext|>'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'dataset_chatml['test'][0]' is used to access the first element of the test set in the 'dataset_chatml' dataset.\n",
    "# This can be used to inspect the first test sample to understand its structure and contents.\n",
    "dataset_chatml['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0eed8764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'pipeline' is a function from the 'transformers' library that creates a pipeline for text generation.\n",
    "# 'text-generation' is the task that the pipeline will perform.\n",
    "# 'model' is the pre-trained model that the pipeline will use.\n",
    "# 'tokenizer' is the tokenizer that the pipeline will use to tokenize the input text.\n",
    "# The created pipeline is stored in the 'pipe' variable.\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec8dc85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block defines a function 'test_inference' that performs inference on a given prompt.\n",
    "\n",
    "# 'prompt' is the input to the function. It is the text that the model will generate a response to.\n",
    "\n",
    "# 'pipe.tokenizer.apply_chat_template' is a method that applies the chat template to the prompt.\n",
    "# The prompt is wrapped in a list and formatted as a dictionary with \"role\" set to \"user\" and \"content\" set to the prompt.\n",
    "# 'tokenize' is set to False, which means that the method will not tokenize the prompt.\n",
    "# 'add_generation_prompt' is set to True, which means that the method will add a generation prompt to the prompt.\n",
    "# The formatted prompt is stored back in the 'prompt' variable.\n",
    "\n",
    "# 'pipe' is the text generation pipeline that was created earlier.\n",
    "# It is called with the formatted prompt and several parameters that control the text generation process.\n",
    "# 'max_new_tokens=256' limits the maximum number of new tokens that can be generated.\n",
    "# 'do_sample=True' enables sampling, which means that the model will generate diverse responses.\n",
    "# 'num_beams=1' sets the number of beams for beam search to 1, which means that the model will generate one response.\n",
    "# 'temperature=0.3' controls the randomness of the responses. Lower values make the responses more deterministic.\n",
    "# 'top_k=50' limits the number of highest probability vocabulary tokens to consider for each step.\n",
    "# 'top_p=0.95' enables nucleus sampling and sets the cumulative probability of parameter tokens to 0.95.\n",
    "# 'max_time=180' limits the maximum time for the generation process to 180 seconds.\n",
    "# The generated responses are stored in the 'outputs' variable.\n",
    "\n",
    "# The function returns the first generated response.\n",
    "# The response is stripped of the prompt and any leading or trailing whitespace.\n",
    "\n",
    "\n",
    "# Original version from the notebook - not able to call model pipeline\n",
    "\"\"\"\n",
    "def test_inference(prompt):\n",
    "    prompt = pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, num_beams=1, temperature=0.3, top_k=50, top_p=0.95,\n",
    "                   max_time= 180) #, eos_token_id=eos_token)\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()\n",
    "\"\"\"\n",
    "\n",
    "# Modified version by He Zhang Oct. 2024 - able to call model pipeline with success!\n",
    "def test_inference(prompt):\n",
    "    prompt = pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe([{\"role\":\"user\", \"content\":prompt}], max_new_tokens=256, do_sample=True, num_beams=1, temperature=0.3, top_k=50, top_p=0.95,\n",
    "                   max_time= 180) #, eos_token_id=eos_token)\n",
    "    return outputs[0]['generated_text'][1][\"content\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "75a92f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def sort_array(arr):\n",
      "    for i in range(len(arr)):\n",
      "        min_idx = i\n",
      "        for j in range(i+1, len(arr)):\n",
      "            if arr[min_idx] > arr[j]:\n",
      "                min_idx = j\n",
      "        arr[i], arr[min_idx] = arr[min_idx], arr[i]\n",
      "    return arr\n",
      "\n",
      "arr = [9, 3, 5, 1, 6]\n",
      "print(sort_array(arr))\n"
     ]
    }
   ],
   "source": [
    "# This code block calls the 'test_inference' function with the first message in the test set of 'dataset_chatml' as the prompt.\n",
    "# 'test_inference' performs inference on the prompt and returns a generated response.\n",
    "# The response is printed to the console.\n",
    "print(test_inference(dataset_chatml['test'][0]['messages'][0]['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a84a5094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "# He Zhang Oct. 2024: testing the phi3 generated response - works correctly\n",
    "def sort_array(arr):\n",
    "    for i in range(len(arr)):\n",
    "        min_idx = i\n",
    "        for j in range(i+1, len(arr)):\n",
    "            if arr[min_idx] > arr[j]:\n",
    "                min_idx = j\n",
    "        arr[i], arr[min_idx] = arr[min_idx], arr[i]\n",
    "    return arr\n",
    "\n",
    "arr = [9, 3, 5, 1, 6]\n",
    "print(sort_array(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987cbdee",
   "metadata": {},
   "source": [
    "## Evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "26800d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8dc5d460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'load_metric' is a function from the 'datasets' library that loads a metric for evaluating the model.\n",
    "# Metrics are used to measure the performance of the model on certain tasks.\n",
    "\n",
    "#from datasets import load_metric # note by He Zhang, Oct. 2024: after datasets version 3.0.0, there is no load_metric() function any more!\n",
    "\n",
    "import evaluate\n",
    "\n",
    "rouge_metric = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7f368f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.7058823529411764,\n",
       " 'rouge2': 0.5333333333333333,\n",
       " 'rougeL': 0.7058823529411764,\n",
       " 'rougeLsum': 0.7058823529411764}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the rouge score\n",
    "rouge_metric.compute(predictions=[\"hi, my name is He Zhang\"], \n",
    "                     references=[\"hi, i am a boy, and my name is He Zhang.\"], \n",
    "                     use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "951bc1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block defines a function 'calculate_rogue' that calculates the ROUGE score for a given row in the dataset.\n",
    "\n",
    "# 'row' is the input to the function. It is a row in the dataset that contains a message and its corresponding output.\n",
    "\n",
    "# 'test_inference(row['messages'][0]['content'])' calls the 'test_inference' function with the first message in the row as the prompt.\n",
    "# 'test_inference' performs inference on the prompt and returns a generated response.\n",
    "# The response is stored in the 'response' variable.\n",
    "\n",
    "# 'rouge_metric.compute' is a method that calculates the ROUGE score for the generated response and the corresponding output in the row.\n",
    "# 'predictions' is set to the generated response and 'references' is set to the output in the row.\n",
    "# 'use_stemmer' is set to True, which means that the method will use a stemmer to reduce words to their root form.\n",
    "# The calculated ROUGE score is stored in the 'result' variable.\n",
    "\n",
    "# The 'result' dictionary is updated to contain the F-measure of each ROUGE score multiplied by 100.\n",
    "# The F-measure is a measure of a test's accuracy that considers both the precision and the recall of the test.\n",
    "\n",
    "# The 'response' is added to the 'result' dictionary.\n",
    "\n",
    "# The function returns the 'result' dictionary.\n",
    "def calculate_rogue(row):\n",
    "    response = test_inference(row['messages'][0]['content'])\n",
    "    result = rouge_metric.compute(predictions=[response], references=[row['output']], use_stemmer=True)\n",
    "    #result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    result['response']=response\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "033ff0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function calculate_rogue at 0x7fe460764160> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function calculate_rogue at 0x7fe460764160> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c7e153a5e24384a77411869298b386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 7s, sys: 325 ms, total: 1min 7s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# '%%time' is a magic command in Jupyter notebooks that measures the execution time of the cell.\n",
    "\n",
    "# 'dataset_chatml['test'].select(range(0,500))' selects the first 500 elements from the test set in the 'dataset_chatml' dataset.\n",
    "\n",
    "# '.map(calculate_rogue, batched=False)' applies the 'calculate_rogue' function to each element in the selected subset.\n",
    "# 'calculate_rogue' calculates the ROUGE score for each element.\n",
    "# 'batched' is set to False, which means that the function will be applied to each element individually, not in batches.\n",
    "\n",
    "# The results are stored in the 'metricas' variable.\n",
    "metricas = dataset_chatml['test'].select(range(0, 10)).map(calculate_rogue, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4b736dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'prompt', 'messages', 'text', 'rouge1', 'rouge2', 'rougeL', 'rougeLsum', 'response'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b606ba64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Create an algorithm in Python to sort an array of numbers.',\n",
       " 'input': '[9, 3, 5, 1, 6]',\n",
       " 'output': 'def sortArray(arr):\\n    for i in range(len(arr)):\\n        for j in range(i+1,len(arr)):\\n            if arr[i] > arr[j]:\\n                temp = arr[i]\\n                arr[i] = arr[j]\\n                arr[j] = temp\\n    return arr\\n\\narr = [9, 3, 5, 1, 6]\\nprint(sortArray(arr))',\n",
       " 'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCreate an algorithm in Python to sort an array of numbers.\\n\\n### Input:\\n[9, 3, 5, 1, 6]\\n\\n### Output:\\ndef sortArray(arr):\\n    for i in range(len(arr)):\\n        for j in range(i+1,len(arr)):\\n            if arr[i] > arr[j]:\\n                temp = arr[i]\\n                arr[i] = arr[j]\\n                arr[j] = temp\\n    return arr\\n\\narr = [9, 3, 5, 1, 6]\\nprint(sortArray(arr))',\n",
       " 'messages': [{'content': 'Create an algorithm in Python to sort an array of numbers.\\n Input: [9, 3, 5, 1, 6]',\n",
       "   'role': 'user'},\n",
       "  {'content': 'def sortArray(arr):\\n    for i in range(len(arr)):\\n        for j in range(i+1,len(arr)):\\n            if arr[i] > arr[j]:\\n                temp = arr[i]\\n                arr[i] = arr[j]\\n                arr[j] = temp\\n    return arr\\n\\narr = [9, 3, 5, 1, 6]\\nprint(sortArray(arr))',\n",
       "   'role': 'assistant'}],\n",
       " 'text': '<|user|>\\nCreate an algorithm in Python to sort an array of numbers.\\n Input: [9, 3, 5, 1, 6]<|end|>\\n<|assistant|>\\ndef sortArray(arr):\\n    for i in range(len(arr)):\\n        for j in range(i+1,len(arr)):\\n            if arr[i] > arr[j]:\\n                temp = arr[i]\\n                arr[i] = arr[j]\\n                arr[j] = temp\\n    return arr\\n\\narr = [9, 3, 5, 1, 6]\\nprint(sortArray(arr))<|end|>\\n<|endoftext|>',\n",
       " 'rouge1': 0.7741935483870969,\n",
       " 'rouge2': 0.5494505494505494,\n",
       " 'rougeL': 0.6881720430107526,\n",
       " 'rougeLsum': 0.7741935483870969,\n",
       " 'response': 'def sort_array(arr):\\n    for i in range(len(arr)):\\n        min_index = i\\n        for j in range(i+1, len(arr)):\\n            if arr[min_index] > arr[j]:\\n                min_index = j\\n        arr[i], arr[min_index] = arr[min_index], arr[i]\\n    return arr\\n\\nprint(sort_array([9, 3, 5, 1, 6]))'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "64a06259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "503b84a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge 1 Mean:  0.5792650524219705\n",
      "Rouge 2 Mean:  0.3335120724777107\n",
      "Rouge L Mean:  0.5004273854576352\n",
      "Rouge Lsum Mean:  0.5776902492723642\n"
     ]
    }
   ],
   "source": [
    "# This code block prints the mean of the ROUGE-1, ROUGE-2, ROUGE-L, and ROUGE-Lsum scores in the 'metricas' dictionary.\n",
    "\n",
    "# 'np.mean(metricas['rouge1'])' calculates the mean of the ROUGE-1 scores.\n",
    "# 'np.mean(metricas['rouge2'])' calculates the mean of the ROUGE-2 scores.\n",
    "# 'np.mean(metricas['rougeL'])' calculates the mean of the ROUGE-L scores.\n",
    "# 'np.mean(metricas['rougeLsum'])' calculates the mean of the ROUGE-Lsum scores.\n",
    "\n",
    "# 'print' is used to print the calculated means to the console.\n",
    "print(\"Rouge 1 Mean: \",np.mean(metricas['rouge1']))\n",
    "print(\"Rouge 2 Mean: \",np.mean(metricas['rouge2']))\n",
    "print(\"Rouge L Mean: \",np.mean(metricas['rougeL']))\n",
    "print(\"Rouge Lsum Mean: \",np.mean(metricas['rougeLsum']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "630b7da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 7288.137MB\n"
     ]
    }
   ],
   "source": [
    "# check model size\n",
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535aaabd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e36b32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
