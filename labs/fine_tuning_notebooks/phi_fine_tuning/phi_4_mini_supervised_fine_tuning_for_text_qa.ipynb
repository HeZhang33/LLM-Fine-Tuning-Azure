{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d12753b0-2466-4b1a-97d5-b6dd461ecfee",
   "metadata": {},
   "source": [
    "## Supervise Fine-Tuning Phi-4 Open-Source Models for Text Q&A - A Python SDK Experience\n",
    "\n",
    "Learn how to fine-tune the <code>Phi-4-mini</code> model using Python Programming Language - An SDK / Code Experience. This notebook is based on the Azure Examples Github [here](https://github.com/Azure/azureml-examples/blob/main/sdk/python/foundation-models/system/finetune/chat-completion/chat-completion.ipynb), with important modifications for compatability.\n",
    "\n",
    "The last successful run is on an AML CPU Compute <code>Standard_D13_v2</code> with Kernel type <code>Python 3.10 - SDK v2</code>.\n",
    "\n",
    "He Zhang, Jul. 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0efc696-bf91-4bb6-b680-7c039447e211",
   "metadata": {},
   "source": [
    "## Chat Completion - Ultrachat-200k\n",
    "\n",
    "This sample shows how to use `chat-completion` components from the `azureml` system registry to fine-tune a model to complete a conversation between 2 people using `ultrachat_200k` dataset. We then deploy the fine-tuned model to an online endpoint for real time inference.\n",
    "\n",
    "### Training data\n",
    "We will use the [ultrachat_200k](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k) dataset. This is a heavily filtered version of the UltraChat dataset and was used to train `Zephyr-7B-Î²`, a state of the art `7B` chat model.\n",
    "\n",
    "### Model\n",
    "We will use the `Phi-4-mini-instruct` model to show how user can fine-tune a model for chat-completion task. If you opened this notebook from a specific model card, remember to replace the specific model name.\n",
    "\n",
    "### Outline\n",
    "* Setup pre-requisites such as compute.\n",
    "* Pick a model to fine-tune.\n",
    "* Pick and explore training data.\n",
    "* Configure the fine-tuning job.\n",
    "* Run the fine-tuning job.\n",
    "* Review training and evaluation metrics. \n",
    "* Register the fine-tuned model. \n",
    "* Deploy the fine-tuned model for real time inference.\n",
    "* Clean up resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63202547-cfa6-45a7-bbe9-f982a882cde1",
   "metadata": {},
   "source": [
    "### Step 1: Setup Pre-requisites\n",
    "* Install dependencies\n",
    "* Connect to AzureML Workspace. Learn more at [set up SDK authentication](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication?tabs=sdk). Replace  `<WORKSPACE_NAME>`, `<RESOURCE_GROUP>` and `<SUBSCRIPTION_ID>` below.\n",
    "* Connect to `azureml` system registry\n",
    "* Set an optional experiment name\n",
    "* Check or create compute.\n",
    "  * The recommended GPU for fine-tuning `Phi-4` models is the `A100` compute as described [here](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/gpu-accelerated/nca100v4-series?tabs=sizebasic#sizes-in-series). You can also find other GPU SKUs such as `V100` [here](https://learn.microsoft.com/en-us/azure/virtual-machines/ncv3-series) and [here](https://learn.microsoft.com/en-us/azure/virtual-machines/ndv2-series).\n",
    "  * A single GPU node can have multiple GPU cards. For example, in one node of `Standard_NC24rs_v3` there are 4 NVIDIA V100 GPUs while in `Standard_NC12s_v3`, there are 2 NVIDIA V100 GPUs. Refer to the [docs](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes-gpu) for this information. The number of GPU cards per node is set in the param `gpus_per_node` below. Setting this value correctly will ensure utilization of all GPUs in the node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a863816-d19e-499a-a243-ca200c788bea",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Install required Python libraries (if not done yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e24a38-38e8-4196-a87b-cbab19d59c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install azure-ai-ml\n",
    "%pip install azure-identit\n",
    "%pip install datasets\n",
    "%pip install mlflow\n",
    "%pip install azureml-mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6d4e53-7f12-4862-b190-eaa1d4336787",
   "metadata": {},
   "source": [
    "#### Import required Python libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170e2b71-7597-4b60-94c6-e79995f4c1fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import (\n",
    "    DefaultAzureCredential,\n",
    "    InteractiveBrowserCredential,\n",
    ")\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "import time\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "try:\n",
    "    workspace_ml_client = MLClient.from_config(credential=credential)\n",
    "except:\n",
    "    workspace_ml_client = MLClient(\n",
    "        credential,\n",
    "        subscription_id=\"<SUBSCRIPTION_ID>\",\n",
    "        resource_group_name=\"<RESOURCE_GROUP>\",\n",
    "        workspace_name=\"<WORKSPACE_NAME>\",\n",
    "    )\n",
    "\n",
    "# the models, fine tuning pipelines and environments are available in the AzureML registry, \"azureml\"\n",
    "registry_ml_client = MLClient(credential, registry_name=\"azureml\")\n",
    "experiment_name = \"Chat_Completion_Phi_4_Mini_Instruct_Text_QA\"\n",
    "\n",
    "# generating a unique timestamp that can be used for names and versions that need to be unique\n",
    "timestamp = str(int(time.time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab76163-68bd-4c2b-bfcf-89b01d7cb330",
   "metadata": {},
   "source": [
    "### Step 2: Pick A Foundation Model To Fine-Tune\n",
    "\n",
    "`Phi-4-mini-instruct` is a dense decoder-only Transformer model with `3.8B` parameters, offering key improvements over `Phi-3.5-Mini`, including a `200K` vocabulary, grouped-query attention, and shared embedding. It is designed for chat-completion prompts, generating text based on user input, with a context length of `128K tokens`. If you have opened this notebook for a different model, replace the model name and version accordingly. \n",
    "\n",
    "Note the model id property of the model. This will be passed as input to the fine-tuning job. This is also available as the `Model ID` field in model details page of `Model Catalog` in Azure Machine Learning Studio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58971d7-a4d3-42c2-bfdf-fee66bddfa3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"Phi-4-mini-instruct\"\n",
    "foundation_model = registry_ml_client.models.get(model_name, label=\"latest\")\n",
    "print(\n",
    "    \"\\n\\nUsing model name: {0}, version: {1}, id: {2} for fine tuning\".format(\n",
    "        foundation_model.name, foundation_model.version, foundation_model.id\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba7ab87-b8cf-488a-b670-de3fa4db243c",
   "metadata": {},
   "source": [
    "### Step 3: Create A Compute\n",
    "\n",
    "The finetune job works `ONLY` with `GPU` compute. The size of the compute depends on how big the model is and in most cases it becomes tricky to identify the right compute for the job. In this cell, we guide the user to select the right compute for the job.\n",
    "\n",
    "`NOTE1` The computes listed below work with the most optimized configuration. Any changes to the configuration might lead to Cuda Out Of Memory error. In such cases, try to upgrade the compute to a bigger compute size.\n",
    "\n",
    "`NOTE2` While selecting the compute_cluster_size below, make sure the compute is available in your resource group. If a particular compute is not available you can make a request to get access to the compute resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed70eb12-b8b5-4068-b392-5ad2a2a1dda8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "if \"finetune_compute_allow_list\" in foundation_model.tags:\n",
    "    computes_allow_list = ast.literal_eval(\n",
    "        foundation_model.tags[\"finetune_compute_allow_list\"]\n",
    "    )  # convert string to python list\n",
    "    print(f\"Please create a compute from the above list - {computes_allow_list}\")\n",
    "else:\n",
    "    computes_allow_list = None\n",
    "    print(\"`finetune_compute_allow_list` is not part of model tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a11bde-a45c-4067-83f9-aa59df15a14e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if you have a specific compute size to work with change it here. By default we use the 4 x A100 compute from the above list\n",
    "compute_cluster_size = \"Standard_NC96ads_A100_v4\"\n",
    "\n",
    "# if you already have a gpu cluster, mention it here. Else will create a new one with the name 'gpu-cluster-big'\n",
    "compute_cluster = \"gpu-cluster-nc96ads-a100-v4\"\n",
    "\n",
    "try:\n",
    "    compute = workspace_ml_client.compute.get(compute_cluster)\n",
    "    print(\"The compute cluster already exists! Reusing it for the current run\")\n",
    "except Exception as ex:\n",
    "    print(\n",
    "        f\"Looks like the compute cluster doesn't exist. Creating a new one with compute size {compute_cluster_size}!\"\n",
    "    )\n",
    "    try:\n",
    "        print(\"Attempt #1 - Trying to create a dedicated compute\")\n",
    "        compute = AmlCompute(\n",
    "            name=compute_cluster,\n",
    "            size=compute_cluster_size,\n",
    "            tier=\"Dedicated\",\n",
    "            max_instances=1,  # For multi node training set this to an integer value more than 1\n",
    "        )\n",
    "        workspace_ml_client.compute.begin_create_or_update(compute).wait()\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            print(\n",
    "                \"Attempt #2 - Trying to create a low priority compute. Since this is a low priority compute, the job could get pre-empted before completion.\"\n",
    "            )\n",
    "            compute = AmlCompute(\n",
    "                name=compute_cluster,\n",
    "                size=compute_cluster_size,\n",
    "                tier=\"LowPriority\",\n",
    "                max_instances=1,  # For multi node training set this to an integer value more than 1\n",
    "            )\n",
    "            workspace_ml_client.compute.begin_create_or_update(compute).wait()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise ValueError(\n",
    "                f\"WARNING! Compute size {compute_cluster_size} not available in workspace\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab80682-d492-4fbc-a8ea-174827b94627",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sanity check on the created compute\n",
    "compute = workspace_ml_client.compute.get(compute_cluster)\n",
    "if compute.provisioning_state.lower() == \"failed\":\n",
    "    raise ValueError(\n",
    "        f\"Provisioning failed, Compute '{compute_cluster}' is in failed state. \"\n",
    "        f\"please try creating a different compute\"\n",
    "    )\n",
    "\n",
    "if computes_allow_list is not None:\n",
    "    computes_allow_list_lower_case = [x.lower() for x in computes_allow_list]\n",
    "    if compute.size.lower() not in computes_allow_list_lower_case:\n",
    "        raise ValueError(\n",
    "            f\"VM size {compute.size} is not in the allow-listed computes for finetuning\"\n",
    "        )\n",
    "else:\n",
    "    # Computes with K80 GPUs are not supported\n",
    "    unsupported_gpu_vm_list = [\n",
    "        \"standard_nc6\",\n",
    "        \"standard_nc12\",\n",
    "        \"standard_nc24\",\n",
    "        \"standard_nc24r\",\n",
    "    ]\n",
    "    if compute.size.lower() in unsupported_gpu_vm_list:\n",
    "        raise ValueError(\n",
    "            f\"VM size {compute.size} is currently not supported for finetuning\"\n",
    "        )\n",
    "\n",
    "# this is the number of GPUs in a single node of the selected 'vm_size' compute.\n",
    "# setting this to less than the number of GPUs will result in underutilized GPUs, taking longer to train.\n",
    "# setting this to more than the number of GPUs will result in an error.\n",
    "gpu_count_found = False\n",
    "workspace_compute_sku_list = workspace_ml_client.compute.list_sizes()\n",
    "available_sku_sizes = []\n",
    "for compute_sku in workspace_compute_sku_list:\n",
    "    available_sku_sizes.append(compute_sku.name)\n",
    "    if compute_sku.name.lower() == compute.size.lower():\n",
    "        gpus_per_node = compute_sku.gpus\n",
    "        gpu_count_found = True\n",
    "# if gpu_count_found not found, then print an error\n",
    "if gpu_count_found:\n",
    "    print(f\"Number of GPU's in compute {compute.size}: {gpus_per_node}\")\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Number of GPU's in compute {compute.size} not found. Available skus are: {available_sku_sizes}.\"\n",
    "        f\"This should not happen. Please check the selected compute cluster: {compute_cluster} and try again.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee052fc-0a03-48f2-bf34-443171614b5a",
   "metadata": {},
   "source": [
    "### Step 4: Prepare Training & Validation Datasets\n",
    "\n",
    "We use the [ultrachat_200k](https://huggingface.co/datasets/samsum) dataset. The dataset has four splits, suitable for:\n",
    "* Supervised Fine-Tuning (sft).\n",
    "* Generation Ranking (gen).\n",
    "\n",
    "The number of examples per split is shown as follows:\n",
    "\n",
    "| train_sft | test_sft | train_gen | test_gen |\n",
    "| :- | :- | :- | :- |\n",
    "| 207865 | 23110 | 256032 | 28304 |\n",
    "\n",
    "The next few cells show basic data preparation for fine-tuning:\n",
    "* Visualize some data rows.\n",
    "* We want this sample to run quickly, so save `train_sft`, `test_sft` files containing 5% of the already trimmed rows. This means the fine-tuned model will have lower accuracy, hence it should not be put to real-world use.\n",
    "\n",
    "> The [download-dataset.py](./download-dataset.py) is used to download the ultrachat_200k dataset and transform the dataset into fine-tuning pipeline component consumable format. Also as the dataset is large, hence we here have only part of the dataset. \n",
    "\n",
    "> Running the below script only downloads 5% of the data. This can be increased by changing `dataset_split_pc` parameter to desired percenetage.\n",
    "\n",
    "> **Note** : Some language models have different language codes and hence the column names in the dataset should reflect the same.\n",
    "\n",
    "##### Here is an example of how the data should look like. \n",
    "\n",
    "The chat-completion dataset is stored in parquet format with each entry using the following schema:\n",
    "``` json\n",
    "{\n",
    "    \"prompt\": \"Create a fully-developed protagonist who is challenged to survive within a dystopian society under the rule of a tyrant. ...\",\n",
    "    \"messages\":[\n",
    "        {\n",
    "            \"content\": \"Create a fully-developed protagonist who is challenged to survive within a dystopian society under the rule of a tyrant. ...\",\n",
    "            \"role\": \"user\"\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"Name: Ava\\n\\n Ava was just 16 years old when the world as she knew it came crashing down. The government had collapsed, leaving behind a chaotic and lawless society. ...\",\n",
    "            \"role\": \"assistant\"\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"Wow, Ava's story is so intense and inspiring! Can you provide me with more details.  ...\",\n",
    "            \"role\": \"user\"\n",
    "        }, \n",
    "        {\n",
    "            \"content\": \"Certainly! ....\",\n",
    "            \"role\": \"assistant\"\n",
    "        }\n",
    "    ],\n",
    "    \"prompt_id\": \"d938b65dfe31f05f80eb8572964c6673eddbd68eff3db6bd234d7f1e3b86c2af\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6571b7-fed3-41a2-8183-dd418cf8b71a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download the dataset using the helper script. This needs datasets library: https://pypi.org/project/datasets/\n",
    "import os\n",
    "\n",
    "exit_status = os.system(\n",
    "    \"python ./download-dataset.py --dataset HuggingFaceH4/ultrachat_200k --download_dir ultrachat_200k_dataset --dataset_split_pc 5\"\n",
    ")\n",
    "\n",
    "if exit_status != 0:\n",
    "    raise Exception(\"Error downloading dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a7225e-5b0b-47a6-aafb-c7847d616988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the ./ultrachat_200k_dataset/train_sft.jsonl file into a pandas dataframe and show the first several rows\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\n",
    "    \"display.max_colwidth\", 0\n",
    ")  # set the max column width to 0 to display the full text\n",
    "df = pd.read_json(\"./ultrachat_200k_dataset/train_sft.jsonl\", lines=True)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e77958-b65f-4265-9e02-f71a69d227f8",
   "metadata": {},
   "source": [
    "### Step 5: Configure and Start Fine-Tuning Job\n",
    "\n",
    "Now you can submit your fine-tuning training job. \n",
    "\n",
    "The fine-tuning job will take some time to start and complete.\n",
    "\n",
    "You can use the job ID to monitor the status of the fine-tuning job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54170cac-d4f9-49c5-890e-cb3f82d2b49d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# default training parameters\n",
    "training_parameters = dict(\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    learning_rate=5e-6,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    ")\n",
    "# default optimization parameters\n",
    "optimization_parameters = dict(\n",
    "    apply_lora=\"true\",\n",
    "    apply_deepspeed=\"true\",\n",
    "    deepspeed_stage=2,\n",
    ")\n",
    "# let's construct finetuning parameters using training and optimization paramters.\n",
    "finetune_parameters = {**training_parameters, **optimization_parameters}\n",
    "\n",
    "# each model finetuning works best with certain fine-tuning parameters which are packed with model as `model_specific_defaults`.\n",
    "# let's override the \"finetune_parameters\" in case the model has some custom defaults.\n",
    "if \"model_specific_defaults\" in foundation_model.tags:\n",
    "    print(\"Warning! Model specific defaults exist. The defaults could be overridden.\")\n",
    "    finetune_parameters.update(\n",
    "        ast.literal_eval(  # convert string to python dict\n",
    "            foundation_model.tags[\"model_specific_defaults\"]\n",
    "        )\n",
    "    )\n",
    "print(\n",
    "    f\"The following fine-tuning parameters are going to be set for the run: {finetune_parameters}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680fb676-45f7-49fb-8b69-11a1d5f14e16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set the pipeline display name for distinguishing different runs from the name\n",
    "def get_pipeline_display_name():\n",
    "    batch_size = (\n",
    "        int(finetune_parameters.get(\"per_device_train_batch_size\", 1))\n",
    "        * int(finetune_parameters.get(\"gradient_accumulation_steps\", 1))\n",
    "        * int(gpus_per_node)\n",
    "        * int(finetune_parameters.get(\"num_nodes_finetune\", 1))\n",
    "    )\n",
    "    scheduler = finetune_parameters.get(\"lr_scheduler_type\", \"linear\")\n",
    "    deepspeed = finetune_parameters.get(\"apply_deepspeed\", \"false\")\n",
    "    ds_stage = finetune_parameters.get(\"deepspeed_stage\", \"2\")\n",
    "    if deepspeed == \"true\":\n",
    "        ds_string = f\"ds{ds_stage}\"\n",
    "    else:\n",
    "        ds_string = \"nods\"\n",
    "    lora = finetune_parameters.get(\"apply_lora\", \"false\")\n",
    "    if lora == \"true\":\n",
    "        lora_string = \"lora\"\n",
    "    else:\n",
    "        lora_string = \"nolora\"\n",
    "    save_limit = finetune_parameters.get(\"save_total_limit\", -1)\n",
    "    seq_len = finetune_parameters.get(\"max_seq_length\", -1)\n",
    "    return (\n",
    "        model_name\n",
    "        + \"-\"\n",
    "        + \"ultrachat\"\n",
    "        + \"-\"\n",
    "        + f\"bs{batch_size}\"\n",
    "        + \"-\"\n",
    "        + f\"{scheduler}\"\n",
    "        + \"-\"\n",
    "        + ds_string\n",
    "        + \"-\"\n",
    "        + lora_string\n",
    "        + f\"-save_limit{save_limit}\"\n",
    "        + f\"-seqlen{seq_len}\"\n",
    "    )\n",
    "\n",
    "pipeline_display_name = get_pipeline_display_name()\n",
    "print(f\"Display name used for the run: {pipeline_display_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "605cafcc-1543-425d-92c1-4fbc566aa495",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_train_epochs': 1,\n",
       " 'per_device_train_batch_size': 1,\n",
       " 'per_device_eval_batch_size': 1,\n",
       " 'learning_rate': 5e-06,\n",
       " 'lr_scheduler_type': 'cosine',\n",
       " 'apply_lora': 'true',\n",
       " 'apply_deepspeed': 'true',\n",
       " 'deepspeed_stage': 3,\n",
       " 'apply_ort': 'false',\n",
       " 'precision': 16,\n",
       " 'ignore_mismatched_sizes': 'false',\n",
       " 'gradient_accumulation_steps': 1,\n",
       " 'logging_strategy': 'steps',\n",
       " 'logging_steps': 10,\n",
       " 'save_total_limit': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_parameters_modified = finetune_parameters.copy()\n",
    "#del finetune_parameters_modified['learning_rate_min']\n",
    "#del finetune_parameters_modified['learning_rate_max']\n",
    "finetune_parameters_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf621bf4-9739-4a95-bcfb-cddfad70cc1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml import Input\n",
    "\n",
    "# fetch the pipeline component\n",
    "pipeline_component_func = registry_ml_client.components.get(\n",
    "    name=\"chat_completion_pipeline\", label=\"latest\"\n",
    ")\n",
    "\n",
    "# define the pipeline job\n",
    "@pipeline(name=pipeline_display_name)\n",
    "def create_pipeline():\n",
    "    chat_completion_pipeline = pipeline_component_func(\n",
    "        mlflow_model_path=foundation_model.id,\n",
    "        compute_model_import=compute_cluster,\n",
    "        compute_preprocess=compute_cluster,\n",
    "        compute_finetune=compute_cluster,\n",
    "        compute_model_evaluation=compute_cluster,\n",
    "        # map the dataset splits to parameters\n",
    "        train_file_path=Input(\n",
    "            type=\"uri_file\", path=\"./ultrachat_200k_dataset/train_sft.jsonl\"\n",
    "        ),\n",
    "        test_file_path=Input(\n",
    "            type=\"uri_file\", path=\"./ultrachat_200k_dataset/test_sft.jsonl\"\n",
    "        ),\n",
    "        # training settings\n",
    "        number_of_gpu_to_use_finetuning=gpus_per_node,  # set to the number of GPUs available in the compute\n",
    "        **finetune_parameters\n",
    "    )\n",
    "    return {\n",
    "        # map the output of the fine-tuning job to the output of pipeline job so that we can easily register the fine-tuned model\n",
    "        # registering the model is required to deploy the model to an online or batch endpoint\n",
    "        \"trained_model\": chat_completion_pipeline.outputs.mlflow_model_folder\n",
    "    }\n",
    "\n",
    "pipeline_object = create_pipeline()\n",
    "\n",
    "# don't use cached results from previous jobs\n",
    "pipeline_object.settings.force_rerun = True\n",
    "\n",
    "# set continue on step failure to False\n",
    "pipeline_object.settings.continue_on_step_failure = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67810f5-119b-4e17-9cf7-1662d4f03ab2",
   "metadata": {},
   "source": [
    "Submit the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda4358c-31de-48db-9c69-451ecfc496bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submit the pipeline job\n",
    "pipeline_job = workspace_ml_client.jobs.create_or_update(\n",
    "    pipeline_object, experiment_name=experiment_name\n",
    ")\n",
    "# wait for the pipeline job to complete\n",
    "workspace_ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39da404-5bc8-4b34-b632-da18a1679a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# check if the `trained_model` output is available\n",
    "print(\"pipeline job outputs: \", workspace_ml_client.jobs.get(pipeline_job.name).outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fda042-2ff2-44c9-8d47-5c1e4955a0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the model from pipeline job output - not working, hence fetching from fine-tuning child job\n",
    "model_path_from_job = \"azureml://jobs/{0}/outputs/{1}\".format(\n",
    "    pipeline_job.name, \"trained_model\"\n",
    ")\n",
    "model_path_from_job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b39a7c-6b53-4e59-85f6-4b9273f667e5",
   "metadata": {},
   "source": [
    "### Step 6: Register The Fine-Tuned Model\n",
    "\n",
    "We will register the model from the output of the fine-tuning job. This will track lineage between the fine-tuned model and the fine-tuning job. The fine-tuning job, further, tracks lineage to the foundation model, data and training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779c9135-3e60-4066-980f-8a3e423f4ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name the fine-tuned model\n",
    "finetuned_model_name = model_name + \"-ultrachat-200k\"\n",
    "finetuned_model_name = finetuned_model_name.replace(\"/\", \"-\")\n",
    "\n",
    "# prepare to register the model from pipeline job output\n",
    "print(\"path to register model: \", model_path_from_job)\n",
    "prepare_to_register_model = Model(\n",
    "    path=model_path_from_job,\n",
    "    type=AssetTypes.MLFLOW_MODEL,\n",
    "    name=finetuned_model_name,\n",
    "    version=timestamp,  # use timestamp as version to avoid version conflict\n",
    "    description=model_name + \" fine-tuned model for ultrachat 200k chat-completion\",\n",
    ")\n",
    "print(\"prepare to register model: \\n\", prepare_to_register_model)\n",
    "\n",
    "# start registering the model\n",
    "registered_model = workspace_ml_client.models.create_or_update(\n",
    "    prepare_to_register_model\n",
    ")\n",
    "print(\"registered model: \\n\", registered_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fbc458-7152-4ca0-92d0-221a59d6aa18",
   "metadata": {},
   "source": [
    "### Step 7: Deploy The Fine-Tuned Model To An Online Endpoint \n",
    "\n",
    "__Note__: Only one deployment is permitted for a customized model. An error occurs if you select an already-deployed customized model.  \n",
    "\n",
    "Online endpoints give a durable REST API that can be used to integrate with applications that need to use the model.\n",
    "\n",
    "The deployment process may take 10 to 20 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c899b4fe-8775-4603-9d85-f40540ed4142",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    ProbeSettings,\n",
    "    OnlineRequestSettings,\n",
    ")\n",
    "\n",
    "# create online endpoint - endpoint names need to be unique in a region, hence using timestamp to create unique endpoint name\n",
    "online_endpoint_name = \"ultrachat-completion-\" + timestamp\n",
    "\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"Online endpoint for \"\n",
    "    + registered_model.name\n",
    "    + \", fine-tuned model for ultrachat-200k-chat-completion\",\n",
    "    auth_mode=\"key\",\n",
    ")\n",
    "\n",
    "workspace_ml_client.begin_create_or_update(endpoint).wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee866dd8-02c2-41ed-a4d0-c6946b3590ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# note that you should deploy using A100 GPU if the model is fine-tuned with A100 GPU. \n",
    "instance_type =  \"Standard_NC24ads_A100_v4\" \n",
    "\n",
    "# inference compute allow list that supports deployment\n",
    "if \"inference_compute_allow_list\" in foundation_model.tags:\n",
    "    inference_computes_allow_list = ast.literal_eval(\n",
    "        foundation_model.tags[\"inference_compute_allow_list\"]\n",
    "    )  # convert string to python list\n",
    "    print(f\"Please create a compute from the above list - {computes_allow_list}\")\n",
    "else:\n",
    "    inference_computes_allow_list = None\n",
    "    print(\"`inference_compute_allow_list` is not part of model tags\")\n",
    "\n",
    "# check if the compute is in the allow listed computes\n",
    "if (\n",
    "    inference_computes_allow_list is not None\n",
    "    and instance_type not in inference_computes_allow_list\n",
    "):\n",
    "    print(\n",
    "        f\"`instance_type` is not in the allow listed compute. Please select a value from {inference_computes_allow_list}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59df9053-3548-4c84-936e-2146bb0ae9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the deployment\n",
    "demo_deployment = ManagedOnlineDeployment(\n",
    "    name=\"demo\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=registered_model.id,\n",
    "    instance_type=instance_type,\n",
    "    instance_count=1,\n",
    "    liveness_probe=ProbeSettings(initial_delay=1200, timeout=20),\n",
    "    request_settings=OnlineRequestSettings(request_timeout_ms=90000),\n",
    ")\n",
    "\n",
    "workspace_ml_client.online_deployments.begin_create_or_update(demo_deployment).wait()\n",
    "\n",
    "endpoint.traffic = {\"demo\": 100}\n",
    "\n",
    "workspace_ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0500d7cc-9774-46b7-807c-d002107720c1",
   "metadata": {},
   "source": [
    "### Step 8: Test the Deployed Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf232305-33a0-4006-8b97-4b157c7bfee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read ./ultrachat_200k_dataset/test_gen.jsonl into a pandas dataframe\n",
    "test_df = pd.read_json(\"./ultrachat_200k_dataset/test_gen.jsonl\", lines=True)\n",
    "\n",
    "# take few random samples\n",
    "test_df = test_df.sample(n=1)\n",
    "\n",
    "# rebuild index\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "test_df.info()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c528b86-4e82-4075-a8b6-8f3d9e07b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# create a json object with the key as \"input_data\" and value as a list of values from the text column of the test dataframe\n",
    "parameters = {\n",
    "    \"temperature\": 0.6,\n",
    "    \"top_p\": 0.9,\n",
    "    \"do_sample\": True,\n",
    "    \"max_new_tokens\": 200,\n",
    "}\n",
    "test_json = {\n",
    "    \"input_data\": {\n",
    "        \"input_string\": [test_df[\"messages\"][0]],\n",
    "        \"parameters\": parameters,\n",
    "    },\n",
    "    \"params\": {},\n",
    "}\n",
    "\n",
    "# save the json object to a file named sample_score.json in the ./samsum-dataset folder\n",
    "with open(\"./ultrachat_200k_dataset/sample_score.json\", \"w\") as f:\n",
    "    json.dump(test_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a87a48-a217-44be-94d2-d1c84b066bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the sample_score.json file using the online endpoint with the azureml endpoint invoking method\n",
    "response = workspace_ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    deployment_name=\"demo\",\n",
    "    request_file=\"./ultrachat_200k_dataset/sample_score.json\",\n",
    ")\n",
    "print(\"raw response: \\n\", response, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4fa519-3900-4eed-bf82-57ee60a83ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the online endpoint API using the http request method\n",
    "import urllib.request\n",
    "import json\n",
    "\n",
    "# request data goes here\n",
    "# the example below assumes JSON formatting which may be updated\n",
    "# depending on the format your endpoint expects.\n",
    "# more information can be found here:\n",
    "# https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script\n",
    "data = {\n",
    "  \"input_data\": {\n",
    "    \"input_string\": [\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I am going to Paris, what should I see?\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\\n\\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\\n2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\\n\\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is so great about #1?\"\n",
    "      }\n",
    "    ],\n",
    "    \"parameters\": {\n",
    "      \"max_new_tokens\": 4096\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "body = str.encode(json.dumps(data))\n",
    "\n",
    "url = 'https://ultrachat-completion-1751183129.germanywestcentral.inference.ml.azure.com/score'\n",
    "\n",
    "# replace this with the primary/secondary key, AMLToken, or Microsoft Entra ID token for the endpoint\n",
    "api_key = \"xxx\"\n",
    "if not api_key:\n",
    "    raise Exception(\"A key should be provided to invoke the endpoint\")\n",
    "\n",
    "headers = {'Content-Type':'application/json', 'Accept': 'application/json', 'Authorization':('Bearer '+ api_key)}\n",
    "\n",
    "req = urllib.request.Request(url, body, headers)\n",
    "\n",
    "try:\n",
    "    response = urllib.request.urlopen(req)\n",
    "    result = response.read()\n",
    "    print(result)\n",
    "except urllib.error.HTTPError as error:\n",
    "    print(\"The request failed with status code: \" + str(error.code))\n",
    "    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "    print(error.info())\n",
    "    print(error.read().decode(\"utf8\", 'ignore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b696095-997c-4b87-aacc-8483d77d84f3",
   "metadata": {},
   "source": [
    "### Step 9: Delete The Online Endpoint\n",
    "Don't forget to delete the online endpoint, else you will leave the billing meter running for the compute used by the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcff289d-cca3-4f4a-afd7-da5b6e0ba8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_ml_client.online_endpoints.begin_delete(name=online_endpoint_name).wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ed95a6-b2f9-4820-82b5-21b385391852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704ff6d3-2eb2-41e9-9878-1f6912ca374f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
