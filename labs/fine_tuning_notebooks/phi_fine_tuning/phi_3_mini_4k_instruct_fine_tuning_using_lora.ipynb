{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d02fbbf8",
   "metadata": {},
   "source": [
    "See reference: https://github.com/microsoft/Phi-3CookBook/blob/main/code/04.Finetuning/Phi-3-finetune-lora-python.ipynb\n",
    "\n",
    "He Zhang, Oct. 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c11157",
   "metadata": {},
   "source": [
    "## Installing and loading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d44a440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade bitsandbytes\n",
    "#%pip install --upgrade transformers\n",
    "#%pip install --upgrade peft \n",
    "#%pip install --upgrade accelerate \n",
    "#%pip install --upgrade datasets \n",
    "#%pip install --upgrade trl \n",
    "#%pip install --upgrade flash_attn \n",
    "#%pip install --upgrade torch \n",
    "#%pip install --upgrade wandb\n",
    "\n",
    "#%pip install huggingface_hub\n",
    "#%pip install python-dotenv\n",
    "#%pip install ipywidgets\n",
    "\n",
    "#%pip install --upgrade absl-py \n",
    "#%pip install --upgrade nltk \n",
    "#%pip install --upgrade rouge_score\n",
    "#%pip install --upgrade evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f378d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from random import randrange\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, TaskType, PeftModel\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c6b4f6",
   "metadata": {},
   "source": [
    "## Setting Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f8438b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'model_id' and 'model_name' are the identifiers for the pre-trained model that you want to fine-tune. \n",
    "# In this case, it's the 'Phi-3-mini-4k-instruct' model from Microsoft.\n",
    "# microsoft/Phi-3-mini-4k-instruct\n",
    "# microsoft/Phi-3-small-128k-instruct\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# In this case, it's the 'python_code_instructions_18k_alpaca' dataset from iamtarun (Ex: iamtarun/python_code_instructions_18k_alpaca).\n",
    "# Update Dataset Name to your dataset name\n",
    "dataset_name = \"iamtarun/python_code_instructions_18k_alpaca\"\n",
    "\n",
    "# 'dataset_split' is the split of the dataset that you want to use for training. \n",
    "# In this case, it's the 'train' split.\n",
    "dataset_split= \"train\"\n",
    "\n",
    "# 'new_model' is the name that you want to give to the fine-tuned model.\n",
    "new_model = \"phi-3-mini-4k-ft-code-gen\"\n",
    "\n",
    "# 'hf_model_repo' is the repository on the Hugging Face Model Hub where the fine-tuned model will be saved. Update UserName to your Hugging Face Username\n",
    "hf_model_repo=\"HeZ/\"+new_model\n",
    "\n",
    "# 'device_map' is a dictionary that maps the model to the GPU device. \n",
    "# In this case, the entire model is loaded on GPU 0.\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# The following are parameters for the LoRA (Learning from Random Architecture) model.\n",
    "\n",
    "# 'lora_r' is the dimension of the LoRA attention.\n",
    "lora_r = 16\n",
    "\n",
    "# 'lora_alpha' is the alpha parameter for LoRA scaling.\n",
    "lora_alpha = 16\n",
    "\n",
    "# 'lora_dropout' is the dropout probability for LoRA layers.\n",
    "lora_dropout = 0.05\n",
    "\n",
    "# 'target_modules' is a list of the modules in the model that will be replaced with LoRA layers.\n",
    "target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    "\n",
    "# 'set_seed' is a function that sets the seed for generating random numbers, \n",
    "# which is used for reproducibility of the results.\n",
    "set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ad731d",
   "metadata": {},
   "source": [
    "## Connect to Huggingface Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3242197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/azureuser/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# This code block is used to log in to the Hugging Face Model Hub using an API token stored in an environment variable.\n",
    "\n",
    "# 'login' is a function from the 'huggingface_hub' library that logs you in to the Hugging Face Model Hub using an API token.\n",
    "from huggingface_hub import login, model_info\n",
    "\n",
    "# 'load_dotenv' is a function from the 'python-dotenv' library that loads environment variables from a .env file.\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 'os' is a standard Python library that provides functions for interacting with the operating system.\n",
    "import os\n",
    "\n",
    "# Call the 'load_dotenv' function to load the environment variables from the .env file.\n",
    "load_dotenv(\"hf_token.env\")\n",
    "\n",
    "# Call the 'login' function with the 'HF_HUB_TOKEN' environment variable to log in to the Hugging Face Model Hub.\n",
    "# 'os.getenv' is a function that gets the value of an environment variable.\n",
    "login(token=os.getenv(\"HF_HUB_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f27d156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelInfo(id='HeZ/test-repo-v1', author='HeZ', sha='379f90c9789e65163946e8b0518641e365fcbfec', created_at=datetime.datetime(2024, 10, 1, 8, 34, 7, tzinfo=datetime.timezone.utc), last_modified=datetime.datetime(2024, 10, 1, 8, 34, 7, tzinfo=datetime.timezone.utc), private=False, disabled=False, downloads=0, downloads_all_time=None, gated=False, gguf=None, inference=None, likes=0, library_name=None, tags=['region:us'], pipeline_tag=None, mask_token=None, card_data=None, widget_data=None, model_index=None, config=None, transformers_info=None, trending_score=None, siblings=[RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)], spaces=[], safetensors=None)\n"
     ]
    }
   ],
   "source": [
    "# login check to see some model information\n",
    "test_model_id = \"HeZ/test-repo-v1\" \n",
    "info = model_info(test_model_id)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e416675c",
   "metadata": {},
   "source": [
    "## Load the dataset with the instruction set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "435bee34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 18612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Create a function to calculate the sum of a sequence of integers.',\n",
       " 'input': '[1, 2, 3, 4, 5]',\n",
       " 'output': '# Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum',\n",
       " 'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCreate a function to calculate the sum of a sequence of integers.\\n\\n### Input:\\n[1, 2, 3, 4, 5]\\n\\n### Output:\\n# Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Create a function to calculate the sum of a sequence of integers.\n",
      "\n",
      "### Input:\n",
      "[1, 2, 3, 4, 5]\n",
      "\n",
      "### Output:\n",
      "# Python code\n",
      "def sum_sequence(sequence):\n",
      "  sum = 0\n",
      "  for num in sequence:\n",
      "    sum += num\n",
      "  return sum\n"
     ]
    }
   ],
   "source": [
    "# This code block is used to load a dataset from the Hugging Face Dataset Hub, print its size, and show a random example from the dataset.\n",
    "\n",
    "# 'load_dataset' is a function from the 'datasets' library that loads a dataset from the Hugging Face Dataset Hub.\n",
    "# 'dataset_name' is the name of the dataset to load, and 'dataset_split' is the split of the dataset to load (e.g., 'train', 'test').\n",
    "dataset = load_dataset(dataset_name, split=dataset_split)\n",
    "\n",
    "# The 'len' function is used to get the size of the dataset, which is then printed.\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "\n",
    "display(dataset[0])\n",
    "print(dataset[0][\"prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8229ceee",
   "metadata": {},
   "source": [
    "## Load the tokenizer to prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23e57de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'tokenizer_id' is set to the 'model_id', which is the identifier for the pre-trained model.\n",
    "# This assumes that the tokenizer associated with the model has the same identifier as the model.\n",
    "tokenizer_id = model_id\n",
    "\n",
    "# 'AutoTokenizer.from_pretrained' is a method that loads a tokenizer from the Hugging Face Model Hub.\n",
    "# 'tokenizer_id' is passed as an argument to specify which tokenizer to load.\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "\n",
    "# 'tokenizer.padding_side' is a property that specifies which side to pad when the input sequence is shorter than the maximum sequence length.\n",
    "# Setting it to 'right' means that padding tokens will be added to the right (end) of the sequence.\n",
    "# This is done to prevent warnings that can occur when the padding side is not explicitly set.\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7603bc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block defines two functions that are used to format the dataset for training a chat model.\n",
    "\n",
    "# 'create_message_column' is a function that takes a row from the dataset and returns a dictionary \n",
    "# with a 'messages' key and a list of 'user' and 'assistant' messages as its value.\n",
    "def create_message_column(row):\n",
    "    # Initialize an empty list to store the messages.\n",
    "    messages = []\n",
    "    \n",
    "    # Create a 'user' message dictionary with 'content' and 'role' keys.\n",
    "    user = {\n",
    "        \"content\": f\"{row['instruction']}\\n Input: {row['input']}\",\n",
    "        \"role\": \"user\"\n",
    "    }\n",
    "    \n",
    "    # Append the 'user' message to the 'messages' list.\n",
    "    messages.append(user)\n",
    "    \n",
    "    # Create an 'assistant' message dictionary with 'content' and 'role' keys.\n",
    "    assistant = {\n",
    "        \"content\": f\"{row['output']}\",\n",
    "        \"role\": \"assistant\"\n",
    "    }\n",
    "    \n",
    "    # Append the 'assistant' message to the 'messages' list.\n",
    "    messages.append(assistant)\n",
    "    \n",
    "    # Return a dictionary with a 'messages' key and the 'messages' list as its value.\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "# 'format_dataset_chatml' is a function that takes a row from the dataset and returns a dictionary \n",
    "# with a 'text' key and a string of formatted chat messages as its value.\n",
    "def format_dataset_chatml(row):\n",
    "    # 'tokenizer.apply_chat_template' is a method that formats a list of chat messages into a single string.\n",
    "    # 'add_generation_prompt' is set to False to not add a generation prompt at the end of the string.\n",
    "    # 'tokenize' is set to False to return a string instead of a list of tokens.\n",
    "    return {\"text\": tokenizer.apply_chat_template(row[\"messages\"], add_generation_prompt=False, tokenize=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46fcd119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to prepare the 'dataset' for training a chat model.\n",
    "\n",
    "# 'dataset.map' is a method that applies a function to each example in the 'dataset'.\n",
    "# 'create_message_column' is a function that formats each example into a 'messages' format suitable for a chat model.\n",
    "# The result is a new 'dataset_chatml' with the formatted examples.\n",
    "dataset_chatml = dataset.map(create_message_column)\n",
    "\n",
    "# 'dataset_chatml.map' is a method that applies a function to each example in the 'dataset_chatml'.\n",
    "# 'format_dataset_chatml' is a function that further formats each example into a single string of chat messages.\n",
    "# The result is an updated 'dataset_chatml' with the further formatted examples.\n",
    "dataset_chatml = dataset_chatml.map(format_dataset_chatml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9414721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'prompt', 'messages', 'text'],\n",
       "    num_rows: 18612\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Create a function to calculate the sum of a sequence of integers.',\n",
       " 'input': '[1, 2, 3, 4, 5]',\n",
       " 'output': '# Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum',\n",
       " 'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCreate a function to calculate the sum of a sequence of integers.\\n\\n### Input:\\n[1, 2, 3, 4, 5]\\n\\n### Output:\\n# Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum',\n",
       " 'messages': [{'content': 'Create a function to calculate the sum of a sequence of integers.\\n Input: [1, 2, 3, 4, 5]',\n",
       "   'role': 'user'},\n",
       "  {'content': '# Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum',\n",
       "   'role': 'assistant'}],\n",
       " 'text': '<|user|>\\nCreate a function to calculate the sum of a sequence of integers.\\n Input: [1, 2, 3, 4, 5]<|end|>\\n<|assistant|>\\n# Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum<|end|>\\n<|endoftext|>'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Create a function to calculate the sum of a sequence of integers.\n",
      " Input: [1, 2, 3, 4, 5]<|end|>\n",
      "<|assistant|>\n",
      "# Python code\n",
      "def sum_sequence(sequence):\n",
      "  sum = 0\n",
      "  for num in sequence:\n",
      "    sum += num\n",
      "  return sum<|end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# This line of code is used to access and display the first example from the 'dataset_chatml'.\n",
    "\n",
    "# 'dataset_chatml[0]' uses indexing to access the first example in the 'dataset_chatml'.\n",
    "# In Python, indexing starts at 0, so 'dataset_chatml[0]' refers to the first example.\n",
    "# The result is a dictionary with a 'text' key and a string of formatted chat messages as its value.\n",
    "display(dataset_chatml)\n",
    "\n",
    "display(dataset_chatml[0])\n",
    "\n",
    "print(dataset_chatml[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73f978cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'prompt', 'messages', 'text'],\n",
       "        num_rows: 17681\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'prompt', 'messages', 'text'],\n",
       "        num_rows: 931\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code block is used to split the 'dataset_chatml' into training and testing sets.\n",
    "\n",
    "# 'dataset_chatml.train_test_split' is a method that splits the 'dataset_chatml' into a training set and a testing set.\n",
    "# 'test_size' is a parameter that specifies the proportion of the 'dataset_chatml' to include in the testing set. Here it's set to 0.05, meaning that 5% of the 'dataset_chatml' will be included in the testing set.\n",
    "# 'seed' is a parameter that sets the seed for the random number generator. This is used to ensure that the split is reproducible. Here it's set to 1234.\n",
    "dataset_chatml = dataset_chatml.train_test_split(test_size=0.05, seed=1234)\n",
    "\n",
    "# This line of code is used to display the structure of the 'dataset_chatml' after the split.\n",
    "# It will typically show information such as the number of rows in the training set and the testing set.\n",
    "dataset_chatml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e714bc43",
   "metadata": {},
   "source": [
    "## Instruction fine-tune a Phi-3-mini model using LORA and trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf0c09e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flash_attention_2\n"
     ]
    }
   ],
   "source": [
    "# This code block is used to set the compute data type and attention implementation based on whether bfloat16 is supported on the current CUDA device.\n",
    "\n",
    "# 'torch.cuda.is_bf16_supported()' is a function that checks if bfloat16 is supported on the current CUDA device.\n",
    "# If bfloat16 is supported, 'compute_dtype' is set to 'torch.bfloat16' and 'attn_implementation' is set to 'flash_attention_2'.\n",
    "if torch.cuda.is_bf16_supported():\n",
    "    compute_dtype = torch.bfloat16\n",
    "    attn_implementation = 'flash_attention_2'\n",
    "# If bfloat16 is not supported, 'compute_dtype' is set to 'torch.float16' and 'attn_implementation' is set to 'sdpa'.\n",
    "else:\n",
    "    compute_dtype = torch.float16\n",
    "    attn_implementation = 'sdpa'\n",
    "\n",
    "# This line of code is used to print the value of 'attn_implementation', which indicates the chosen attention implementation.\n",
    "print(attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f3c2bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set flash attention to be 'eager' mode, if you run this notebook using Nvidia V100 GPU\n",
    "attn_implementation = 'eager'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a255a0eb",
   "metadata": {},
   "source": [
    "## Load the tokenizer and model to finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0fcca46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a4a6e5ea2e44a283149021060dce70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This code block is used to load a pre-trained model and its associated tokenizer from the Hugging Face Model Hub.\n",
    "\n",
    "# 'model_name' is set to the identifier of the pre-trained model.\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# 'AutoTokenizer.from_pretrained' is a method that loads a tokenizer from the Hugging Face Model Hub.\n",
    "# 'model_id' is passed as an argument to specify which tokenizer to load.\n",
    "# 'trust_remote_code' is set to True to trust the remote code in the tokenizer files.\n",
    "# 'add_eos_token' is set to True to add an end-of-sentence token to the tokenizer.\n",
    "# 'use_fast' is set to True to use the fast version of the tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, add_eos_token=True, use_fast=True)\n",
    "\n",
    "# The padding token is set to the unknown token.\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "# The ID of the padding token is set to the ID of the unknown token.\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "\n",
    "# The padding side is set to 'left', meaning that padding tokens will be added to the left (start) of the sequence.\n",
    "tokenizer.padding_side = \"right\" #'left'\n",
    "\n",
    "# 'AutoModelForCausalLM.from_pretrained' is a method that loads a pre-trained model for causal language modeling from the Hugging Face Model Hub.\n",
    "# 'model_id' is passed as an argument to specify which model to load.\n",
    "# 'torch_dtype' is set to the compute data type determined earlier.\n",
    "# 'trust_remote_code' is set to True to trust the remote code in the model files.\n",
    "# 'device_map' is passed as an argument to specify the device mapping for distributed training.\n",
    "# 'attn_implementation' is set to the attention implementation determined earlier.\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                             torch_dtype=compute_dtype, \n",
    "                                             trust_remote_code=True, \n",
    "                                             device_map=device_map,\n",
    "                                             attn_implementation=attn_implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c947f43",
   "metadata": {},
   "source": [
    "### Configure training hyper-parameters and LoRA parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8e72d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to define the training arguments for the model.\n",
    "\n",
    "# 'TrainingArguments' is a class that holds the arguments for training a model.\n",
    "# 'output_dir' is the directory where the model and its checkpoints will be saved.\n",
    "# 'evaluation_strategy' is set to \"steps\", meaning that evaluation will be performed after a certain number of training steps.\n",
    "# 'do_eval' is set to True, meaning that evaluation will be performed.\n",
    "# 'optim' is set to \"adamw_torch\", meaning that the AdamW optimizer from PyTorch will be used.\n",
    "# 'per_device_train_batch_size' and 'per_device_eval_batch_size' are set to 8, meaning that the batch size for training and evaluation will be 8 per device.\n",
    "# 'gradient_accumulation_steps' is set to 4, meaning that gradients will be accumulated over 4 steps before performing a backward/update pass.\n",
    "# 'log_level' is set to \"debug\", meaning that all log messages will be printed.\n",
    "# 'save_strategy' is set to \"epoch\", meaning that the model will be saved after each epoch.\n",
    "# 'logging_steps' is set to 100, meaning that log messages will be printed every 100 steps.\n",
    "# 'learning_rate' is set to 1e-4, which is the learning rate for the optimizer.\n",
    "# 'fp16' is set to the opposite of whether bfloat16 is supported on the current CUDA device.\n",
    "# 'bf16' is set to whether bfloat16 is supported on the current CUDA device.\n",
    "# 'eval_steps' is set to 100, meaning that evaluation will be performed every 100 steps.\n",
    "# 'num_train_epochs' is set to 3, meaning that the model will be trained for 3 epochs.\n",
    "# 'warmup_ratio' is set to 0.1, meaning that 10% of the total training steps will be used for the warmup phase.\n",
    "# 'lr_scheduler_type' is set to \"linear\", meaning that a linear learning rate scheduler will be used.\n",
    "# 'report_to' is set to \"wandb\", meaning that training and evaluation metrics will be reported to Weights & Biases.\n",
    "# 'seed' is set to 42, which is the seed for the random number generator.\n",
    "\n",
    "# LoraConfig object is created with the following parameters:\n",
    "# 'r' (rank of the low-rank approximation) is set to 16,\n",
    "# 'lora_alpha' (scaling factor) is set to 16,\n",
    "# 'lora_dropout' dropout probability for Lora layers is set to 0.05,\n",
    "# 'task_type' (set to TaskType.CAUSAL_LM indicating the task type),\n",
    "# 'target_modules' (the modules to which LoRA is applied) choosing linear layers except the output layer..\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "        output_dir=\"./phi-3-mini-LoRA\",\n",
    "        eval_strategy=\"steps\",\n",
    "        do_eval=True,\n",
    "        optim=\"adamw_torch\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=40,\n",
    "        per_device_eval_batch_size=1,\n",
    "        log_level=\"debug\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=100,\n",
    "        learning_rate=1e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        eval_steps=100,\n",
    "        num_train_epochs=1,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        report_to=\"wandb\",\n",
    "        seed=42,\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=target_modules,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "130e3955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhezhang33\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code block is used to initialize Weights & Biases (wandb), a tool for tracking and visualizing machine learning experiments.\n",
    "\n",
    "# 'import wandb' is used to import the wandb library.\n",
    "import wandb\n",
    "\n",
    "# 'wandb.login()' is a method that logs you into your Weights & Biases account.\n",
    "# If you're not already logged in, it will prompt you to log in.\n",
    "# Once you're logged in, you can use Weights & Biases to track and visualize your experiments.\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c903dd6",
   "metadata": {},
   "source": [
    "## Establish Connection with wandb and Initiate the Project and Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51e4a910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/batch/tasks/shared/LS_root/mounts/clusters/gpu-nc6s-v3/code/Users/Phi3_Fine_Tuning/wandb/run-20241003_083035-juaoyx61</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hezhang33/Phi3-mini-ft-python-code/runs/juaoyx61' target=\"_blank\">phi-3-mini-ft-py-3e-run-1</a></strong> to <a href='https://wandb.ai/hezhang33/Phi3-mini-ft-python-code' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hezhang33/Phi3-mini-ft-python-code' target=\"_blank\">https://wandb.ai/hezhang33/Phi3-mini-ft-python-code</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hezhang33/Phi3-mini-ft-python-code/runs/juaoyx61' target=\"_blank\">https://wandb.ai/hezhang33/Phi3-mini-ft-python-code/runs/juaoyx61</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hezhang33/Phi3-mini-ft-python-code/runs/juaoyx61?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f83752452a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code block is used to initialize a Weights & Biases (wandb) run.\n",
    "\n",
    "# 'project_name' is set to the name of the project in Weights & Biases.\n",
    "project_name = \"Phi3-mini-ft-python-code\"\n",
    "\n",
    "# 'wandb.init' is a method that initializes a new Weights & Biases run.\n",
    "# 'project' is set to 'project_name', meaning that the run will be associated with this project.\n",
    "# 'name' is set to \"phi-3-mini-ft-py-3e\", which is the name of the run.\n",
    "# Each run has a unique name which can be used to identify it in the Weights & Biases dashboard.\n",
    "wandb.init(project=project_name, name=\"phi-3-mini-ft-py-3e-run-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41a61bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "# This code block is used to initialize the SFTTrainer, which is used to train the model.\n",
    "\n",
    "# 'model' is the model that will be trained.\n",
    "# 'train_dataset' and 'eval_dataset' are the datasets that will be used for training and evaluation, respectively.\n",
    "# 'peft_config' is the configuration for peft, which is used for instruction tuning.\n",
    "# 'dataset_text_field' is set to \"text\", meaning that the 'text' field of the dataset will be used as the input for the model.\n",
    "# 'max_seq_length' is set to 512, meaning that the maximum length of the sequences that will be fed to the model is 512 tokens.\n",
    "# 'tokenizer' is the tokenizer that will be used to tokenize the input text.\n",
    "# 'args' are the training arguments that were defined earlier.\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset_chatml['train'],\n",
    "        eval_dataset=dataset_chatml['test'],\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=512,\n",
    "        tokenizer=tokenizer,\n",
    "        args=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aa993b",
   "metadata": {},
   "source": [
    "#### Note by He Zhang: total number of training steps = (total samples * number of epochs) / gradient accumulation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1e40aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently training with a batch size of: 1\n",
      "***** Running training *****\n",
      "  Num examples = 17,681\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 40\n",
      "  Gradient Accumulation steps = 40\n",
      "  Total optimization steps = 442\n",
      "  Number of trainable parameters = 8,912,896\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='442' max='442' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [442/442 2:54:10, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.781700</td>\n",
       "      <td>0.592855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.596900</td>\n",
       "      <td>0.581609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>0.577479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.581000</td>\n",
       "      <td>0.575877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 1\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 1\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 1\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 931\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./phi-3-mini-LoRA/checkpoint-442\n",
      "loading configuration file config.json from cache at /home/azureuser/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./phi-3-mini-LoRA/checkpoint-442/tokenizer_config.json\n",
      "Special tokens file saved in ./phi-3-mini-LoRA/checkpoint-442/special_tokens_map.json\n",
      "Saving model checkpoint to ./phi-3-mini-LoRA/checkpoint-442\n",
      "loading configuration file config.json from cache at /home/azureuser/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./phi-3-mini-LoRA/checkpoint-442/tokenizer_config.json\n",
      "Special tokens file saved in ./phi-3-mini-LoRA/checkpoint-442/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 14min 17s, sys: 40min 24s, total: 2h 54min 42s\n",
      "Wall time: 2h 54min 34s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=442, training_loss=0.6315486182993894, metrics={'train_runtime': 10473.6152, 'train_samples_per_second': 1.688, 'train_steps_per_second': 0.042, 'total_flos': 6.528613645473792e+16, 'train_loss': 0.6315486182993894, 'epoch': 0.9999434421130027})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# This code block is used to train the model and save it locally.\n",
    "\n",
    "# 'trainer.train()' is a method that starts the training of the model.\n",
    "# It uses the training dataset, evaluation dataset, and training arguments that were provided when the trainer was initialized.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cde29b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi-3-mini-LoRA\n",
      "loading configuration file config.json from cache at /home/azureuser/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./phi-3-mini-LoRA/tokenizer_config.json\n",
      "Special tokens file saved in ./phi-3-mini-LoRA/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# 'trainer.save_model()' is a method that saves the trained model locally.\n",
    "# The model will be saved in the directory specified by 'output_dir' in the training arguments.\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "972cd3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi-3-mini-LoRA\n",
      "loading configuration file config.json from cache at /home/azureuser/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./phi-3-mini-LoRA/tokenizer_config.json\n",
      "Special tokens file saved in ./phi-3-mini-LoRA/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ffd10e10d9485bb2d7e012263427ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/35.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3042480078d4c338fcea2b17a880299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be58d9b04f04bb3a23b52dbdefc9edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/HeZ/phi-3-mini-LoRA/commit/46ca79e1bd249649e74eaf93fa769b8472ecc47e', commit_message='HeZ/phi-3-mini-4k-ft-code-gen', commit_description='', oid='46ca79e1bd249649e74eaf93fa769b8472ecc47e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/HeZ/phi-3-mini-LoRA', endpoint='https://huggingface.co', repo_type='model', repo_id='HeZ/phi-3-mini-LoRA'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code block is used to save the adapter to the Hugging Face Model Hub.\n",
    "\n",
    "# 'trainer.push_to_hub' is a method that pushes the trained model (or adapter in this case) to the Hugging Face Model Hub.\n",
    "# In this example, hf_model_repo=\"HeZ/phi-3-mini-4k-ft-code-gen\", i.e. the name of the repository on the Hugging Face Model Hub where the adapter will be saved.\n",
    "trainer.push_to_hub(hf_model_repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c15e9dc",
   "metadata": {},
   "source": [
    "## Merge the model and the adapter and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78f0cbea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code block is used to free up GPU memory.\n",
    "\n",
    "# 'del model' and 'del trainer' are used to delete the 'model' and 'trainer' objects. \n",
    "# This removes the references to these objects, allowing Python's garbage collector to free up the memory they were using.\n",
    "\n",
    "del model\n",
    "del trainer\n",
    "\n",
    "# 'import gc' is used to import Python's garbage collector module.\n",
    "import gc\n",
    "\n",
    "# 'gc.collect()' is a method that triggers a full garbage collection, which can help to free up memory.\n",
    "# It's called twice here to ensure that all unreachable objects are collected.\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11860bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'torch.cuda.empty_cache()' is a PyTorch method that releases all unoccupied cached memory currently held by \n",
    "# the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3762d0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'gc.collect()' is a method that triggers a full garbage collection in Python.\n",
    "# It forces the garbage collector to release unreferenced memory, which can be helpful in managing memory usage, especially in a resource-constrained environment.\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20d0464",
   "metadata": {},
   "source": [
    "#### Load the previously trained and stored model, combine it, and then save the complete model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8bb0085b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/azureuser/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/config.json\n",
      "loading configuration file config.json from cache at /home/azureuser/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/config.json\n",
      "Model config Phi3Config {\n",
      "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/azureuser/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/model.safetensors.index.json\n",
      "Instantiating Phi3ForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce8eb9a0e094f0d8336d73ca457650b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
      "\n",
      "All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/azureuser/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n",
      "loading file tokenizer.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32011. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    }
   ],
   "source": [
    "# This code block is used to load the trained model, merge it, and save the merged model.\n",
    "\n",
    "# 'AutoPeftModelForCausalLM' is a class from the 'peft' library that provides a causal language model with PEFT (Performance Efficient Fine-Tuning) support.\n",
    "\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# 'AutoPeftModelForCausalLM.from_pretrained' is a method that loads a pre-trained model (adapter model) and its base model.\n",
    "#  The adapter model is loaded from 'args.output_dir', which is the directory where the trained model was saved.\n",
    "# 'low_cpu_mem_usage' is set to True, which means that the model will use less CPU memory.\n",
    "# 'return_dict' is set to True, which means that the model will return a 'ModelOutput' (a named tuple) instead of a plain tuple.\n",
    "# 'torch_dtype' is set to 'torch.bfloat16', which means that the model will use bfloat16 precision for its computations.\n",
    "# 'trust_remote_code' is set to True, which means that the model will trust and execute remote code.\n",
    "# 'device_map' is the device map that will be used by the model.\n",
    "\n",
    "new_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.bfloat16, #torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=device_map,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "257f8e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in merged_model/config.json\n",
      "Configuration saved in merged_model/generation_config.json\n",
      "The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at merged_model/model.safetensors.index.json.\n",
      "tokenizer config file saved in merged_model/tokenizer_config.json\n",
      "Special tokens file saved in merged_model/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('merged_model/tokenizer_config.json',\n",
       " 'merged_model/special_tokens_map.json',\n",
       " 'merged_model/tokenizer.json')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'new_model.merge_and_unload' is a method that merges the model and unloads it from memory.\n",
    "# The merged model is stored in 'merged_model'.\n",
    "\n",
    "merged_model = new_model.merge_and_unload()\n",
    "\n",
    "# 'merged_model.save_pretrained' is a method that saves the merged model.\n",
    "# The model is saved in the directory \"merged_model\".\n",
    "# 'trust_remote_code' is set to True, which means that the model will trust and execute remote code.\n",
    "# 'safe_serialization' is set to True, which means that the model will use safe serialization.\n",
    "\n",
    "merged_model.save_pretrained(\"merged_model\", trust_remote_code=True, safe_serialization=True)\n",
    "\n",
    "# 'tokenizer.save_pretrained' is a method that saves the tokenizer.\n",
    "# The tokenizer is saved in the directory \"merged_model\".\n",
    "\n",
    "tokenizer.save_pretrained(\"merged_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15416e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /tmp/tmpnasl5jb_/config.json\n",
      "Configuration saved in /tmp/tmpnasl5jb_/generation_config.json\n",
      "The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/tmpnasl5jb_/model.safetensors.index.json.\n",
      "Uploading the following files to HeZ/phi-3-mini-4k-ft-code-gen: model.safetensors.index.json,config.json,README.md,generation_config.json,model-00001-of-00002.safetensors,model-00002-of-00002.safetensors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e807b7816742a582b67f32969a23ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b57fecd7d0497cbb6a6ffa01d104bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b4fa5fc45643d8951b90ff69a23e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1640f78d429242b792b7b68c7ad37e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in /tmp/tmp0n0e3ud4/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/tmp0n0e3ud4/special_tokens_map.json\n",
      "Uploading the following files to HeZ/phi-3-mini-4k-ft-code-gen: tokenizer_config.json,README.md,special_tokens_map.json,tokenizer.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/HeZ/phi-3-mini-4k-ft-code-gen/commit/39386a420bd668977b6fc7cbd6ce5659c10dcde0', commit_message='Upload tokenizer', commit_description='', oid='39386a420bd668977b6fc7cbd6ce5659c10dcde0', pr_url=None, repo_url=RepoUrl('https://huggingface.co/HeZ/phi-3-mini-4k-ft-code-gen', endpoint='https://huggingface.co', repo_type='model', repo_id='HeZ/phi-3-mini-4k-ft-code-gen'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code block is used to push the merged model and the tokenizer to the Hugging Face Model Hub.\n",
    "\n",
    "# 'merged_model.push_to_hub' is a method that pushes the merged model to the Hugging Face Model Hub.\n",
    "# 'hf_model_repo' is the name of the repository on the Hugging Face Model Hub where the model will be saved.\n",
    "merged_model.push_to_hub(hf_model_repo)\n",
    "\n",
    "# 'tokenizer.push_to_hub' is a method that pushes the tokenizer to the Hugging Face Model Hub.\n",
    "# 'hf_model_repo' is the name of the repository on the Hugging Face Model Hub where the tokenizer will be saved.\n",
    "tokenizer.push_to_hub(hf_model_repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9fa1f6",
   "metadata": {},
   "source": [
    "## Model Inference and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d02e7e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HeZ/phi-3-mini-4k-ft-code-gen'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'hf_model_repo' is a variable that holds the name of the repository on the Hugging Face Model Hub.\n",
    "# This is where the trained and merged model, as well as the tokenizer, have been saved.\n",
    "hf_model_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c4c96d",
   "metadata": {},
   "source": [
    "#### Retrieve the model and tokenizer from the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32061e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc59ca82467a477e8e002fec9c3632cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This code block is used to load the model and tokenizer from the Hugging Face Model Hub.\n",
    "\n",
    "# 'torch' is a library that provides a wide range of functionalities for tensor computations with strong GPU acceleration support.\n",
    "# 'AutoTokenizer' and 'AutoModelForCausalLM' are classes from the 'transformers' library that provide a tokenizer and a causal language model, respectively.\n",
    "# 'set_seed' is a function from the 'transformers' library that sets the seed for generating random numbers, which can be used for reproducibility.\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "\n",
    "# 'set_seed(1234)' sets the seed for generating random numbers to 1234.\n",
    "set_seed(1234)  # For reproducibility\n",
    "\n",
    "# 'AutoTokenizer.from_pretrained' is a method that loads a pre-trained tokenizer.\n",
    "# The tokenizer is loaded from 'hf_model_repo', which is the name of the repository on the Hugging Face Model Hub where the tokenizer was saved.\n",
    "# 'trust_remote_code' is set to True, which means that the tokenizer will trust and execute remote code.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_repo,trust_remote_code=True)\n",
    "\n",
    "# 'AutoModelForCausalLM.from_pretrained' is a method that loads a pre-trained causal language model.\n",
    "# The model is loaded from 'hf_model_repo', which is the name of the repository on the Hugging Face Model Hub where the model was saved.\n",
    "# 'trust_remote_code' is set to True, which means that the model will trust and execute remote code.\n",
    "# 'torch_dtype' is set to \"auto\", which means that the model will automatically choose the data type for its computations.\n",
    "# 'device_map' is set to \"cuda\", which means that the model will use the CUDA device for its computations.\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_model_repo, trust_remote_code=True, torch_dtype=\"auto\", device_map=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8647900",
   "metadata": {},
   "source": [
    "#### Inference without pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dd30e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To solve the equation 2x + 3 = 7, we need to isolate the variable x. First, we can subtract 3 from both sides of the equation to get 2x = 4. Then, we can divide both sides by 2 to get x = 2. Therefore, the solution to the equation is x = 2.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23446525",
   "metadata": {},
   "source": [
    "#### Inference with pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34c77243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'prompt', 'messages', 'text'],\n",
       "        num_rows: 17681\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'prompt', 'messages', 'text'],\n",
       "        num_rows: 931\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_chatml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e2346bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Create an algorithm in Python to sort an array of numbers.',\n",
       " 'input': '[9, 3, 5, 1, 6]',\n",
       " 'output': 'def sortArray(arr):\\n    for i in range(len(arr)):\\n        for j in range(i+1,len(arr)):\\n            if arr[i] > arr[j]:\\n                temp = arr[i]\\n                arr[i] = arr[j]\\n                arr[j] = temp\\n    return arr\\n\\narr = [9, 3, 5, 1, 6]\\nprint(sortArray(arr))',\n",
       " 'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCreate an algorithm in Python to sort an array of numbers.\\n\\n### Input:\\n[9, 3, 5, 1, 6]\\n\\n### Output:\\ndef sortArray(arr):\\n    for i in range(len(arr)):\\n        for j in range(i+1,len(arr)):\\n            if arr[i] > arr[j]:\\n                temp = arr[i]\\n                arr[i] = arr[j]\\n                arr[j] = temp\\n    return arr\\n\\narr = [9, 3, 5, 1, 6]\\nprint(sortArray(arr))',\n",
       " 'messages': [{'content': 'Create an algorithm in Python to sort an array of numbers.\\n Input: [9, 3, 5, 1, 6]',\n",
       "   'role': 'user'},\n",
       "  {'content': 'def sortArray(arr):\\n    for i in range(len(arr)):\\n        for j in range(i+1,len(arr)):\\n            if arr[i] > arr[j]:\\n                temp = arr[i]\\n                arr[i] = arr[j]\\n                arr[j] = temp\\n    return arr\\n\\narr = [9, 3, 5, 1, 6]\\nprint(sortArray(arr))',\n",
       "   'role': 'assistant'}],\n",
       " 'text': '<|user|>\\nCreate an algorithm in Python to sort an array of numbers.\\n Input: [9, 3, 5, 1, 6]<|end|>\\n<|assistant|>\\ndef sortArray(arr):\\n    for i in range(len(arr)):\\n        for j in range(i+1,len(arr)):\\n            if arr[i] > arr[j]:\\n                temp = arr[i]\\n                arr[i] = arr[j]\\n                arr[j] = temp\\n    return arr\\n\\narr = [9, 3, 5, 1, 6]\\nprint(sortArray(arr))<|end|>\\n<|endoftext|>'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'dataset_chatml['test'][0]' is used to access the first element of the test set in the 'dataset_chatml' dataset.\n",
    "# This can be used to inspect the first test sample to understand its structure and contents.\n",
    "dataset_chatml['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c26a1cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'pipeline' is a function from the 'transformers' library that creates a pipeline for text generation.\n",
    "# 'text-generation' is the task that the pipeline will perform.\n",
    "# 'model' is the pre-trained model that the pipeline will use.\n",
    "# 'tokenizer' is the tokenizer that the pipeline will use to tokenize the input text.\n",
    "# The created pipeline is stored in the 'pipe' variable.\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665ced3b",
   "metadata": {},
   "source": [
    "Develop a function that organizes the input and performs inference on an individual sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "99783677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block defines a function 'test_inference' that performs inference on a given prompt.\n",
    "\n",
    "# 'prompt' is the input to the function. It is the text that the model will generate a response to.\n",
    "\n",
    "# 'pipe.tokenizer.apply_chat_template' is a method that applies the chat template to the prompt.\n",
    "# The prompt is wrapped in a list and formatted as a dictionary with \"role\" set to \"user\" and \"content\" set to the prompt.\n",
    "# 'tokenize' is set to False, which means that the method will not tokenize the prompt.\n",
    "# 'add_generation_prompt' is set to True, which means that the method will add a generation prompt to the prompt.\n",
    "# The formatted prompt is stored back in the 'prompt' variable.\n",
    "\n",
    "# 'pipe' is the text generation pipeline that was created earlier.\n",
    "# It is called with the formatted prompt and several parameters that control the text generation process.\n",
    "# 'max_new_tokens=256' limits the maximum number of new tokens that can be generated.\n",
    "# 'do_sample=True' enables sampling, which means that the model will generate diverse responses.\n",
    "# 'num_beams=1' sets the number of beams for beam search to 1, which means that the model will generate one response.\n",
    "# 'temperature=0.3' controls the randomness of the responses. Lower values make the responses more deterministic.\n",
    "# 'top_k=50' limits the number of highest probability vocabulary tokens to consider for each step.\n",
    "# 'top_p=0.95' enables nucleus sampling and sets the cumulative probability of parameter tokens to 0.95.\n",
    "# 'max_time=180' limits the maximum time for the generation process to 180 seconds.\n",
    "# The generated responses are stored in the 'outputs' variable.\n",
    "\n",
    "# The function returns the first generated response.\n",
    "# The response is stripped of the prompt and any leading or trailing whitespace.\n",
    "\n",
    "\n",
    "# Original version from the notebook - not able to call model pipeline\n",
    "\"\"\"\n",
    "def test_inference(prompt):\n",
    "    prompt = pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, num_beams=1, temperature=0.3, top_k=50, top_p=0.95,\n",
    "                   max_time= 180) #, eos_token_id=eos_token)\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()\n",
    "\"\"\"\n",
    "\n",
    "# Modified version by He Zhang Oct. 2024 - able to call model pipeline with success!\n",
    "def test_inference(prompt):\n",
    "    prompt = pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe([{\"role\":\"user\", \"content\":prompt}], max_new_tokens=256, do_sample=True, num_beams=1, temperature=0.3, top_k=50, top_p=0.95,\n",
    "                   max_time= 180) #, eos_token_id=eos_token)\n",
    "    return outputs[0]['generated_text'][1][\"content\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8f65b639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def sort_array(arr):\n",
      "    for i in range(len(arr)):\n",
      "        for j in range(i+1, len(arr)):\n",
      "            if arr[i] > arr[j]:\n",
      "                arr[i], arr[j] = arr[j], arr[i]\n",
      "    return arr\n",
      "\n",
      "print(sort_array([9, 3, 5, 1, 6]))\n"
     ]
    }
   ],
   "source": [
    "# This code block calls the 'test_inference' function with the first message in the test set of 'dataset_chatml' as the prompt.\n",
    "# 'test_inference' performs inference on the prompt and returns a generated response.\n",
    "# The response is printed to the console.\n",
    "print(test_inference(dataset_chatml['test'][0]['messages'][0]['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3e8d6c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "# He Zhang Oct. 2024: testing the phi3 generated response - works correctly\n",
    "def sort_array(arr):\n",
    "    for i in range(len(arr)):\n",
    "        for j in range(i+1, len(arr)):\n",
    "            if arr[i] > arr[j]:\n",
    "                arr[i], arr[j] = arr[j], arr[i]\n",
    "    return arr\n",
    "\n",
    "print(sort_array([9, 3, 5, 1, 6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2abc0fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Use matplotlib in Python to create a pie chart depicting a survey result.\\n Input: topic = \"Favorite fruits\"\\nresponses = { \\'banana\\': 5, \\'apple\\': 7, \\'strawberry\\': 10, \\'mango\\': 6 }'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_chatml['test'][1]['messages'][0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d3feb539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import matplotlib.pyplot as plt\\n \\nlabels = list(responses.keys())\\nvalues = list(responses.values())\\ncolors = ['#F08080', '#F8A458', '#9BC808', '#000080']\\n \\nplt.pie(values, labels = labels, colors = colors, autopct='%1.2f%%')\\nplt.title('Favorite Fruits')\\nplt.axis('equal')\\nplt.show()\""
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_chatml['test'][1][\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2d1d791a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import matplotlib.pyplot as plt\n",
      "\n",
      "topic = \"Favorite fruits\"\n",
      "responses = { 'banana': 5, 'apple': 7,'strawberry': 10,'mango': 6 }\n",
      "\n",
      "labels = responses.keys()\n",
      "sizes = responses.values()\n",
      "\n",
      "plt.pie(sizes, labels=labels, autopct='%1.1f%%')\n",
      "plt.title(topic)\n",
      "plt.show()\n"
     ]
    }
   ],
   "source": [
    "print(test_inference(dataset_chatml['test'][1]['messages'][0]['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d1ba42",
   "metadata": {},
   "source": [
    "## Evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c8eca865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ed85bdf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "909f6e96971b4edcac4b7462b1012ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 'load_metric' is a function from the 'datasets' library that loads a metric for evaluating the model.\n",
    "# Metrics are used to measure the performance of the model on certain tasks.\n",
    "\n",
    "#from datasets import load_metric # note by He Zhang, Oct. 2024: after datasets version 3.0.0, there is no load_metric() function any more!\n",
    "\n",
    "import evaluate\n",
    "\n",
    "rouge_metric = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7ad33e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.7058823529411764,\n",
       " 'rouge2': 0.5333333333333333,\n",
       " 'rougeL': 0.7058823529411764,\n",
       " 'rougeLsum': 0.7058823529411764}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the rouge score\n",
    "rouge_metric.compute(predictions=[\"hi, my name is He Zhang\"], \n",
    "                     references=[\"hi, i am a boy, and my name is He Zhang.\"], \n",
    "                     use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6371aac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block defines a function 'calculate_rogue' that calculates the ROUGE score for a given row in the dataset.\n",
    "\n",
    "# 'row' is the input to the function. It is a row in the dataset that contains a message and its corresponding output.\n",
    "\n",
    "# 'test_inference(row['messages'][0]['content'])' calls the 'test_inference' function with the first message in the row as the prompt.\n",
    "# 'test_inference' performs inference on the prompt and returns a generated response.\n",
    "# The response is stored in the 'response' variable.\n",
    "\n",
    "# 'rouge_metric.compute' is a method that calculates the ROUGE score for the generated response and the corresponding output in the row.\n",
    "# 'predictions' is set to the generated response and 'references' is set to the output in the row.\n",
    "# 'use_stemmer' is set to True, which means that the method will use a stemmer to reduce words to their root form.\n",
    "# The calculated ROUGE score is stored in the 'result' variable.\n",
    "\n",
    "# The 'result' dictionary is updated to contain the F-measure of each ROUGE score multiplied by 100.\n",
    "# The F-measure is a measure of a test's accuracy that considers both the precision and the recall of the test.\n",
    "\n",
    "# The 'response' is added to the 'result' dictionary.\n",
    "\n",
    "# The function returns the 'result' dictionary.\n",
    "def calculate_rogue(row):\n",
    "    response = test_inference(row['messages'][0]['content'])\n",
    "    result = rouge_metric.compute(predictions=[response], references=[row['output']], use_stemmer=True)\n",
    "    #result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    result['response']=response\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8653b11c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdcb978bbe8342778a18a43784ce7459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.8 s, sys: 111 ms, total: 55.9 s\n",
      "Wall time: 55.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# '%%time' is a magic command in Jupyter notebooks that measures the execution time of the cell.\n",
    "\n",
    "# 'dataset_chatml['test'].select(range(0,500))' selects the first 500 elements from the test set in the 'dataset_chatml' dataset.\n",
    "\n",
    "# '.map(calculate_rogue, batched=False)' applies the 'calculate_rogue' function to each element in the selected subset.\n",
    "# 'calculate_rogue' calculates the ROUGE score for each element.\n",
    "# 'batched' is set to False, which means that the function will be applied to each element individually, not in batches.\n",
    "\n",
    "# The results are stored in the 'metricas' variable.\n",
    "metricas = dataset_chatml['test'].select(range(0, 10)).map(calculate_rogue, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8e194cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'prompt', 'messages', 'text', 'rouge1', 'rouge2', 'rougeL', 'rougeLsum', 'response'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2c036694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Create an algorithm in Python to sort an array of numbers.',\n",
       " 'input': '[9, 3, 5, 1, 6]',\n",
       " 'output': 'def sortArray(arr):\\n    for i in range(len(arr)):\\n        for j in range(i+1,len(arr)):\\n            if arr[i] > arr[j]:\\n                temp = arr[i]\\n                arr[i] = arr[j]\\n                arr[j] = temp\\n    return arr\\n\\narr = [9, 3, 5, 1, 6]\\nprint(sortArray(arr))',\n",
       " 'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCreate an algorithm in Python to sort an array of numbers.\\n\\n### Input:\\n[9, 3, 5, 1, 6]\\n\\n### Output:\\ndef sortArray(arr):\\n    for i in range(len(arr)):\\n        for j in range(i+1,len(arr)):\\n            if arr[i] > arr[j]:\\n                temp = arr[i]\\n                arr[i] = arr[j]\\n                arr[j] = temp\\n    return arr\\n\\narr = [9, 3, 5, 1, 6]\\nprint(sortArray(arr))',\n",
       " 'messages': [{'content': 'Create an algorithm in Python to sort an array of numbers.\\n Input: [9, 3, 5, 1, 6]',\n",
       "   'role': 'user'},\n",
       "  {'content': 'def sortArray(arr):\\n    for i in range(len(arr)):\\n        for j in range(i+1,len(arr)):\\n            if arr[i] > arr[j]:\\n                temp = arr[i]\\n                arr[i] = arr[j]\\n                arr[j] = temp\\n    return arr\\n\\narr = [9, 3, 5, 1, 6]\\nprint(sortArray(arr))',\n",
       "   'role': 'assistant'}],\n",
       " 'text': '<|user|>\\nCreate an algorithm in Python to sort an array of numbers.\\n Input: [9, 3, 5, 1, 6]<|end|>\\n<|assistant|>\\ndef sortArray(arr):\\n    for i in range(len(arr)):\\n        for j in range(i+1,len(arr)):\\n            if arr[i] > arr[j]:\\n                temp = arr[i]\\n                arr[i] = arr[j]\\n                arr[j] = temp\\n    return arr\\n\\narr = [9, 3, 5, 1, 6]\\nprint(sortArray(arr))<|end|>\\n<|endoftext|>',\n",
       " 'rouge1': 0.8157894736842105,\n",
       " 'rouge2': 0.7027027027027026,\n",
       " 'rougeL': 0.7631578947368421,\n",
       " 'rougeLsum': 0.8157894736842105,\n",
       " 'response': 'def sort_array(arr):\\n    for i in range(len(arr)):\\n        for j in range(i + 1, len(arr)):\\n            if arr[i] > arr[j]:\\n                arr[i], arr[j] = arr[j], arr[i]\\n    return arr'}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "dd82c9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e37aa22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge 1 Mean:  0.5582906957625786\n",
      "Rouge 2 Mean:  0.3610550297553985\n",
      "Rouge L Mean:  0.503785396220792\n",
      "Rouge Lsum Mean:  0.5568414204002597\n"
     ]
    }
   ],
   "source": [
    "# This code block prints the mean of the ROUGE-1, ROUGE-2, ROUGE-L, and ROUGE-Lsum scores in the 'metricas' dictionary.\n",
    "\n",
    "# 'np.mean(metricas['rouge1'])' calculates the mean of the ROUGE-1 scores.\n",
    "# 'np.mean(metricas['rouge2'])' calculates the mean of the ROUGE-2 scores.\n",
    "# 'np.mean(metricas['rougeL'])' calculates the mean of the ROUGE-L scores.\n",
    "# 'np.mean(metricas['rougeLsum'])' calculates the mean of the ROUGE-Lsum scores.\n",
    "\n",
    "# 'print' is used to print the calculated means to the console.\n",
    "print(\"Rouge 1 Mean: \",np.mean(metricas['rouge1']))\n",
    "print(\"Rouge 2 Mean: \",np.mean(metricas['rouge2']))\n",
    "print(\"Rouge L Mean: \",np.mean(metricas['rougeL']))\n",
    "print(\"Rouge Lsum Mean: \",np.mean(metricas['rougeLsum']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04073c37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ed7949",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
