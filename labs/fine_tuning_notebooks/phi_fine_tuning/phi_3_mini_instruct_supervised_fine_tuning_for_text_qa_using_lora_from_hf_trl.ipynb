{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d36156e",
   "metadata": {},
   "source": [
    "## Supervised Fine-Tuning Phi-3-Mini for Text Q&A using LoRA from Hugging Face TRL Library - A Python SDK Experience\n",
    "\n",
    "Learn how to fine-tune the <code>Phi-3-Mini-4k</code> model using LoRA from Hugging Face Open-Source TRL Python Library.\n",
    "\n",
    "This notebook is based on the published demo by Microsoft PhiCookBook (https://github.com/microsoft/PhiCookBook). The latest run was on an <code>AML Tesla V100 GPU Compute Standard_NC6s_v3</code> with Kernel type <code>Python 3.10 - SDK v2</code>.  \n",
    "\n",
    "He Zhang, Jul. 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41b1e14",
   "metadata": {},
   "source": [
    "### Installing and loading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12494b5-0b65-49e6-b072-69343ee94e11",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install --upgrade bitsandbytes\n",
    "#%pip install --upgrade transformers\n",
    "#%pip install --upgrade peft \n",
    "#%pip install --upgrade accelerate \n",
    "#%pip install --upgrade datasets \n",
    "#%pip install --upgrade flash_attn \n",
    "#%pip install --upgrade torch \n",
    "#%pip install --upgrade wandb\n",
    "\n",
    "#%pip install huggingface_hub\n",
    "#%pip install python-dotenv\n",
    "#%pip install ipywidgets\n",
    "\n",
    "#%pip install --upgrade absl-py \n",
    "#%pip install --upgrade nltk \n",
    "#%pip install --upgrade rouge_score\n",
    "#%pip install --upgrade evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be8b408-fa98-4798-817d-06ab21b2a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note by He Zhang July 2025: install below 'trl' version to avoid 'missing completion' errors when initiating 'SFTTrainer'\n",
    "#%pip install trl==0.12.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51e79e4-99b4-4ade-97e8-1cfe6234cf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from random import randrange\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from peft import LoraConfig, TaskType, AutoPeftModelForCausalLM\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b823c4c0",
   "metadata": {},
   "source": [
    "### Setting Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a2ac37-283d-4acb-8bbb-f893c5daca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'model_id' and 'model_name' are the identifiers for the pre-trained model that you want to fine-tune. \n",
    "# In this case, it's the 'Phi-3-mini-4k-instruct' model from Microsoft.\n",
    "# microsoft/Phi-3-mini-4k-instruct\n",
    "# microsoft/Phi-3-small-128k-instruct\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# In this case, it's the 'python_code_instructions_18k_alpaca' dataset from iamtarun (Ex: iamtarun/python_code_instructions_18k_alpaca).\n",
    "# Update Dataset Name to your dataset name\n",
    "dataset_name = \"iamtarun/python_code_instructions_18k_alpaca\"\n",
    "\n",
    "# 'dataset_split' is the split of the dataset that you want to use for training. \n",
    "# In this case, it's the 'train' split.\n",
    "dataset_split= \"train\"\n",
    "\n",
    "# 'new_model' is the name that you want to give to the fine-tuned model.\n",
    "new_model = \"phi-3-mini-4k-ft-code-gen-v2\"\n",
    "\n",
    "# 'hf_model_repo' is the repository on the Hugging Face Model Hub where the fine-tuned model will be saved. Update UserName to your Hugging Face Username\n",
    "hf_model_repo=\"HeZ/\"+new_model\n",
    "\n",
    "# 'device_map' is a dictionary that maps the model to the GPU device. \n",
    "# In this case, the entire model is loaded on GPU 0.\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# The following are parameters for the LoRA (Learning from Random Architecture) model.\n",
    "\n",
    "# 'lora_r' is the dimension of the LoRA attention.\n",
    "lora_r = 16\n",
    "\n",
    "# 'lora_alpha' is the alpha parameter for LoRA scaling.\n",
    "lora_alpha = 16\n",
    "\n",
    "# 'lora_dropout' is the dropout probability for LoRA layers.\n",
    "lora_dropout = 0.05\n",
    "\n",
    "# 'target_modules' is a list of the modules in the model that will be replaced with LoRA layers.\n",
    "target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    "\n",
    "# 'set_seed' is a function that sets the seed for generating random numbers, \n",
    "# which is used for reproducibility of the results.\n",
    "set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76173208",
   "metadata": {},
   "source": [
    "### Connect to Huggingface Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc55ea3-e16f-4d5e-88a7-46d5dd9aaf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to log in to the Hugging Face Model Hub using an API token stored in an environment variable.\n",
    "\n",
    "# 'login' is a function from the 'huggingface_hub' library that logs you in to the Hugging Face Model Hub using an API token.\n",
    "from huggingface_hub import login, model_info\n",
    "\n",
    "# 'load_dotenv' is a function from the 'python-dotenv' library that loads environment variables from a .env file.\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 'os' is a standard Python library that provides functions for interacting with the operating system.\n",
    "import os\n",
    "\n",
    "# Call the 'load_dotenv' function to load the environment variables from the .env file.\n",
    "load_dotenv(\"hf_token.env\")\n",
    "\n",
    "# Call the 'login' function with the 'HF_HUB_TOKEN' environment variable to log in to the Hugging Face Model Hub.\n",
    "# 'os.getenv' is a function that gets the value of an environment variable.\n",
    "login(token=os.getenv(\"HF_HUB_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c7564c-781b-45bb-bb22-bb54fb567bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# login check to see some model information\n",
    "test_model_id = \"HeZ/test-repo-v1\" \n",
    "info = model_info(test_model_id)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c920c4e1",
   "metadata": {},
   "source": [
    "### Load the Dataset with the Instruction Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac0c93a-7aaf-4a73-87d9-c13721765ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to load a dataset from the Hugging Face Dataset Hub, print its size, and show a random example from the dataset.\n",
    "\n",
    "# 'load_dataset' is a function from the 'datasets' library that loads a dataset from the Hugging Face Dataset Hub.\n",
    "# 'dataset_name' is the name of the dataset to load, and 'dataset_split' is the split of the dataset to load (e.g., 'train', 'test').\n",
    "dataset = load_dataset(dataset_name, split=dataset_split)\n",
    "\n",
    "# The 'len' function is used to get the size of the dataset, which is then printed.\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "\n",
    "display(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dfc98a-909e-4a27-bd71-bdaba7ec9999",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5d2fa-d76d-4253-bb78-14c380e525ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0][\"prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e6e127",
   "metadata": {},
   "source": [
    "### Load the Tokenizer to Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43308910-41ee-43cc-a2a1-b5ec48b7714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'tokenizer_id' is set to the 'model_id', which is the identifier for the pre-trained model.\n",
    "# This assumes that the tokenizer associated with the model has the same identifier as the model.\n",
    "tokenizer_id = model_id\n",
    "\n",
    "# 'AutoTokenizer.from_pretrained' is a method that loads a tokenizer from the Hugging Face Model Hub.\n",
    "# 'tokenizer_id' is passed as an argument to specify which tokenizer to load.\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "\n",
    "# 'tokenizer.padding_side' is a property that specifies which side to pad when the input sequence is shorter than the maximum sequence length.\n",
    "# Setting it to 'right' means that padding tokens will be added to the right (end) of the sequence.\n",
    "# This is done to prevent warnings that can occur when the padding side is not explicitly set.\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce570f63-3db2-476f-862a-9f643b965f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block defines two functions that are used to format the dataset for training a chat model.\n",
    "\n",
    "# 'create_message_column' is a function that takes a row from the dataset and returns a dictionary \n",
    "# with a 'messages' key and a list of 'user' and 'assistant' messages as its value.\n",
    "def create_message_column(row):\n",
    "    # Initialize an empty list to store the messages.\n",
    "    messages = []\n",
    "    \n",
    "    # Create a 'user' message dictionary with 'content' and 'role' keys.\n",
    "    user = {\n",
    "        \"content\": f\"{row['instruction']}\\n Input: {row['input']}\",\n",
    "        \"role\": \"user\"\n",
    "    }\n",
    "    \n",
    "    # Append the 'user' message to the 'messages' list.\n",
    "    messages.append(user)\n",
    "    \n",
    "    # Create an 'assistant' message dictionary with 'content' and 'role' keys.\n",
    "    assistant = {\n",
    "        \"content\": f\"{row['output']}\",\n",
    "        \"role\": \"assistant\"\n",
    "    }\n",
    "    \n",
    "    # Append the 'assistant' message to the 'messages' list.\n",
    "    messages.append(assistant)\n",
    "    \n",
    "    # Return a dictionary with a 'messages' key and the 'messages' list as its value.\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "# 'format_dataset_chatml' is a function that takes a row from the dataset and returns a dictionary \n",
    "# with a 'text' key and a string of formatted chat messages as its value.\n",
    "def format_dataset_chatml(row):\n",
    "    # 'tokenizer.apply_chat_template' is a method that formats a list of chat messages into a single string.\n",
    "    # 'add_generation_prompt' is set to False to not add a generation prompt at the end of the string.\n",
    "    # 'tokenize' is set to False to return a string instead of a list of tokens.\n",
    "    return {\"text\": tokenizer.apply_chat_template(row[\"messages\"], add_generation_prompt=False, tokenize=False)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc452fb-1ed5-40db-8e41-dcab11ac2a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to prepare the 'dataset' for training a chat model.\n",
    "\n",
    "# 'dataset.map' is a method that applies a function to each example in the 'dataset'.\n",
    "# 'create_message_column' is a function that formats each example into a 'messages' format suitable for a chat model.\n",
    "# The result is a new 'dataset_chatml' with the formatted examples.\n",
    "dataset_chatml = dataset.map(create_message_column)\n",
    "\n",
    "# 'dataset_chatml.map' is a method that applies a function to each example in the 'dataset_chatml'.\n",
    "# 'format_dataset_chatml' is a function that further formats each example into a single string of chat messages.\n",
    "# The result is an updated 'dataset_chatml' with the further formatted examples.\n",
    "dataset_chatml = dataset_chatml.map(format_dataset_chatml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc56583-a8dc-4b32-9d20-95dfdda2ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line of code is used to access and display the first example from the 'dataset_chatml'.\n",
    "\n",
    "# 'dataset_chatml[0]' uses indexing to access the first example in the 'dataset_chatml'.\n",
    "# In Python, indexing starts at 0, so 'dataset_chatml[0]' refers to the first example.\n",
    "# The result is a dictionary with a 'text' key and a string of formatted chat messages as its value.\n",
    "display(dataset_chatml)\n",
    "\n",
    "display(dataset_chatml[0])\n",
    "\n",
    "print(\"\\n\", dataset_chatml[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ecde4f-81a0-45e0-9a10-f67fb6326c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to split the 'dataset_chatml' into training and testing sets.\n",
    "\n",
    "# 'dataset_chatml.train_test_split' is a method that splits the 'dataset_chatml' into a training set and a testing set.\n",
    "# 'test_size' is a parameter that specifies the proportion of the 'dataset_chatml' to include in the testing set. Here it's set to 0.05, meaning that 5% of the 'dataset_chatml' will be included in the testing set.\n",
    "# 'seed' is a parameter that sets the seed for the random number generator. This is used to ensure that the split is reproducible. Here it's set to 1234.\n",
    "dataset_chatml = dataset_chatml.train_test_split(test_size=0.05, seed=1234)\n",
    "\n",
    "# This line of code is used to display the structure of the 'dataset_chatml' after the split.\n",
    "# It will typically show information such as the number of rows in the training set and the testing set.\n",
    "dataset_chatml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2ff09b-ecff-43a4-a609-ac32965f6c01",
   "metadata": {},
   "source": [
    "#### Test Model Inferencing \n",
    "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c3cce1-d485-4a58-b225-97a3c11b4ca8",
   "metadata": {},
   "source": [
    "<code>\n",
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n",
    "torch.random.manual_seed(0) \n",
    "    \n",
    "model = AutoModelForCausalLM.from_pretrained( \n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",  \n",
    "    device_map=\"cuda\",  \n",
    "    torch_dtype=\"auto\",  \n",
    "    trust_remote_code=False,  \n",
    "    attn_implementation=\"eager\"\n",
    ") \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "messages = [ \n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, \n",
    "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"}, \n",
    "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"}, \n",
    "] \n",
    "\n",
    "pipe = pipeline( \n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    ") \n",
    "\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 500, \n",
    "    \"return_full_text\": False, \n",
    "    \"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "} \n",
    "\n",
    "output = pipe(messages, **generation_args) \n",
    "print(output[0]['generated_text'])\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394623e6",
   "metadata": {},
   "source": [
    "### Instruction Fine-Tune the Phi-3-Mini Model using LoRA from Hugging Face TRL Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4af49-56ff-42da-9eb6-e8a1ca1b3b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to set the compute data type and attention implementation based on whether bfloat16 is supported on the current CUDA device.\n",
    "\n",
    "# 'torch.cuda.is_bf16_supported()' is a function that checks if bfloat16 is supported on the current CUDA device.\n",
    "# If bfloat16 is supported, 'compute_dtype' is set to 'torch.bfloat16' and 'attn_implementation' is set to 'flash_attention_2'.\n",
    "if torch.cuda.is_bf16_supported():\n",
    "    compute_dtype = torch.bfloat16\n",
    "    attn_implementation = 'flash_attention_2'\n",
    "# If bfloat16 is not supported, 'compute_dtype' is set to 'torch.float16' and 'attn_implementation' is set to 'sdpa'.\n",
    "else:\n",
    "    compute_dtype = torch.float16\n",
    "    attn_implementation = 'sdpa'\n",
    "\n",
    "# This line of code is used to print the value of 'attn_implementation', which indicates the chosen attention implementation.\n",
    "print(attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2796c2ab-4ff9-4677-bcbf-7bf306eacce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set flash attention to be 'eager' mode, if you run this notebook using Nvidia V100 GPU\n",
    "attn_implementation = 'eager'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdda69c",
   "metadata": {},
   "source": [
    "#### Load the Tokenizer and Model to Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7a2d4-28f6-4a73-9f54-5946ea106663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to load a pre-trained model and its associated tokenizer from the Hugging Face Model Hub.\n",
    "\n",
    "# 'model_name' is set to the identifier of the pre-trained model.\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# 'AutoTokenizer.from_pretrained' is a method that loads a tokenizer from the Hugging Face Model Hub.\n",
    "# 'model_id' is passed as an argument to specify which tokenizer to load.\n",
    "# 'trust_remote_code' is set to True to trust the remote code in the tokenizer files.\n",
    "# 'add_eos_token' is set to True to add an end-of-sentence token to the tokenizer.\n",
    "# 'use_fast' is set to True to use the fast version of the tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=False, add_eos_token=True, use_fast=True)\n",
    "\n",
    "# The padding token is set to the unknown token.\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "# The ID of the padding token is set to the ID of the unknown token.\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "\n",
    "# The padding side is set to 'left', meaning that padding tokens will be added to the left (start) of the sequence.\n",
    "tokenizer.padding_side = \"right\" #'left'\n",
    "\n",
    "# 'AutoModelForCausalLM.from_pretrained' is a method that loads a pre-trained model for causal language modeling from the Hugging Face Model Hub.\n",
    "# 'model_id' is passed as an argument to specify which model to load.\n",
    "# 'torch_dtype' is set to the compute data type determined earlier.\n",
    "# 'trust_remote_code' is set to True to trust the remote code in the model files.\n",
    "# 'device_map' is passed as an argument to specify the device mapping for distributed training.\n",
    "# 'attn_implementation' is set to the attention implementation determined earlier.\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                             torch_dtype=compute_dtype, \n",
    "                                             trust_remote_code=False, \n",
    "                                             device_map=device_map,\n",
    "                                             attn_implementation=attn_implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5647b08c",
   "metadata": {},
   "source": [
    "#### Configure Training Hyper-Parameters and LoRA Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd12da0-8944-41a8-bd56-ab660d2ecd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to define the training arguments for the model.\n",
    "\n",
    "# 'TrainingArguments' is a class that holds the arguments for training a model.\n",
    "# 'output_dir' is the directory where the model and its checkpoints will be saved.\n",
    "# 'evaluation_strategy' is set to \"steps\", meaning that evaluation will be performed after a certain number of training steps.\n",
    "# 'do_eval' is set to True, meaning that evaluation will be performed.\n",
    "# 'optim' is set to \"adamw_torch\", meaning that the AdamW optimizer from PyTorch will be used.\n",
    "# 'per_device_train_batch_size' and 'per_device_eval_batch_size' are set to 8, meaning that the batch size for training and evaluation will be 8 per device.\n",
    "# 'gradient_accumulation_steps' is set to 4, meaning that gradients will be accumulated over 4 steps before performing a backward/update pass.\n",
    "# 'log_level' is set to \"debug\", meaning that all log messages will be printed.\n",
    "# 'save_strategy' is set to \"epoch\", meaning that the model will be saved after each epoch.\n",
    "# 'logging_steps' is set to 100, meaning that log messages will be printed every 100 steps.\n",
    "# 'learning_rate' is set to 1e-4, which is the learning rate for the optimizer.\n",
    "# 'fp16' is set to the opposite of whether bfloat16 is supported on the current CUDA device.\n",
    "# 'bf16' is set to whether bfloat16 is supported on the current CUDA device.\n",
    "# 'eval_steps' is set to 100, meaning that evaluation will be performed every 100 steps.\n",
    "# 'num_train_epochs' is set to 3, meaning that the model will be trained for 3 epochs.\n",
    "# 'warmup_ratio' is set to 0.1, meaning that 10% of the total training steps will be used for the warmup phase.\n",
    "# 'lr_scheduler_type' is set to \"linear\", meaning that a linear learning rate scheduler will be used.\n",
    "# 'report_to' is set to \"wandb\", meaning that training and evaluation metrics will be reported to Weights & Biases.\n",
    "# 'seed' is set to 42, which is the seed for the random number generator.\n",
    "\n",
    "# LoraConfig object is created with the following parameters:\n",
    "# 'r' (rank of the low-rank approximation) is set to 16,\n",
    "# 'lora_alpha' (scaling factor) is set to 16,\n",
    "# 'lora_dropout' dropout probability for Lora layers is set to 0.05,\n",
    "# 'task_type' (set to TaskType.CAUSAL_LM indicating the task type),\n",
    "# 'target_modules' (the modules to which LoRA is applied) choosing linear layers except the output layer..\n",
    "\n",
    "args = TrainingArguments(\n",
    "        output_dir=\"./phi-3-mini-LoRA\",\n",
    "        eval_strategy=\"steps\",\n",
    "        do_eval=True,\n",
    "        optim=\"adamw_torch\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=40,\n",
    "        per_device_eval_batch_size=1,\n",
    "        log_level=\"debug\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=100,\n",
    "        learning_rate=1e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        eval_steps=100,\n",
    "        num_train_epochs=1,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        report_to=\"wandb\",\n",
    "        seed=42,\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=target_modules,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3301e8",
   "metadata": {},
   "source": [
    "#### Establish Connection with Wandb and Initiate the Project and Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568312f8-644b-469f-9bcc-3adda96e3018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to initialize Weights & Biases (wandb), a tool for tracking and visualizing machine learning experiments.\n",
    "\n",
    "# 'import wandb' is used to import the wandb library.\n",
    "import wandb\n",
    "\n",
    "# 'wandb.login()' is a method that logs you into your Weights & Biases account.\n",
    "# If you're not already logged in, it will prompt you to log in.\n",
    "# Once you're logged in, you can use Weights & Biases to track and visualize your experiments.\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9739fd-9e2a-4c45-9a64-aa03db3d01e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to initialize a Weights & Biases (wandb) run.\n",
    "\n",
    "# 'project_name' is set to the name of the project in Weights & Biases.\n",
    "project_name = \"Phi3-mini-ft-python-code\"\n",
    "\n",
    "# 'wandb.init' is a method that initializes a new Weights & Biases run.\n",
    "# 'project' is set to 'project_name', meaning that the run will be associated with this project.\n",
    "# 'name' is set to \"phi-3-mini-ft-py-3e\", which is the name of the run.\n",
    "# Each run has a unique name which can be used to identify it in the Weights & Biases dashboard.\n",
    "wandb.init(project=project_name, name=\"phi-3-mini-ft-py-3e-run-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5368af-a09b-4e71-8e0b-59a44e7abff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to initialize the SFTTrainer, which is used to train the model.\n",
    "\n",
    "# 'model' is the model that will be trained.\n",
    "# 'train_dataset' and 'eval_dataset' are the datasets that will be used for training and evaluation, respectively.\n",
    "# 'peft_config' is the configuration for peft, which is used for instruction tuning.\n",
    "# 'dataset_text_field' is set to \"text\", meaning that the 'text' field of the dataset will be used as the input for the model.\n",
    "# 'max_seq_length' is set to 512, meaning that the maximum length of the sequences that will be fed to the model is 512 tokens.\n",
    "# 'tokenizer' is the tokenizer that will be used to tokenize the input text.\n",
    "# 'args' are the training arguments that were defined earlier.\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset_chatml['train'],\n",
    "        eval_dataset=dataset_chatml['test'],\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=512,\n",
    "        tokenizer=tokenizer,\n",
    "        args=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e0b36c-909c-47c5-9621-78edf30edfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This code block is used to train the model and save it locally.\n",
    "\n",
    "# 'trainer.train()' is a method that starts the training of the model.\n",
    "# It uses the training dataset, evaluation dataset, and training arguments that were provided when the trainer was initialized.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41159e6a-a86a-457d-ba24-7ecc78e07fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'trainer.save_model()' is a method that saves the trained model locally.\n",
    "# The model will be saved in the directory specified by 'output_dir' in the training arguments.\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d140fe2c-1453-47de-8d15-1037bbd0df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to save the adapter to the Hugging Face Model Hub.\n",
    "\n",
    "# 'trainer.push_to_hub' is a method that pushes the trained model (or adapter in this case) to the Hugging Face Model Hub.\n",
    "# In this example, hf_model_repo=\"HeZ/phi-3-mini-4k-ft-code-gen-v2\", i.e. the name of the repository on the Hugging Face Model Hub where the adapter will be saved.\n",
    "trainer.push_to_hub(hf_model_repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40571c1d",
   "metadata": {},
   "source": [
    "#### Merge the Model and the Adapter and Save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23837e7-0828-4c0c-85a0-8b7c6d821c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to free up GPU memory.\n",
    "\n",
    "# 'del model' and 'del trainer' are used to delete the 'model' and 'trainer' objects. \n",
    "# This removes the references to these objects, allowing Python's garbage collector to free up the memory they were using.\n",
    "\n",
    "del model\n",
    "del trainer\n",
    "\n",
    "# 'import gc' is used to import Python's garbage collector module.\n",
    "import gc\n",
    "\n",
    "# 'gc.collect()' is a method that triggers a full garbage collection, which can help to free up memory.\n",
    "# It's called twice here to ensure that all unreachable objects are collected.\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "\n",
    "# 'torch.cuda.empty_cache()' is a PyTorch method that releases all unoccupied cached memory currently held by \n",
    "# the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4cb2d9-1c3c-42b8-864c-55641bbc8293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to load the trained model, merge it, and save the merged model.\n",
    "\n",
    "# 'AutoPeftModelForCausalLM' is a class from the 'peft' library that provides a causal language model with PEFT (Performance Efficient Fine-Tuning) support.\n",
    "\n",
    "# 'AutoPeftModelForCausalLM.from_pretrained' is a method that loads a pre-trained model (adapter model) and its base model.\n",
    "#  The adapter model is loaded from 'args.output_dir', which is the directory where the trained model was saved.\n",
    "# 'low_cpu_mem_usage' is set to True, which means that the model will use less CPU memory.\n",
    "# 'return_dict' is set to True, which means that the model will return a 'ModelOutput' (a named tuple) instead of a plain tuple.\n",
    "# 'torch_dtype' is set to 'torch.bfloat16', which means that the model will use bfloat16 precision for its computations.\n",
    "# 'trust_remote_code' is set to True, which means that the model will trust and execute remote code.\n",
    "# 'device_map' is the device map that will be used by the model.\n",
    "\n",
    "new_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.bfloat16, #torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=device_map,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670a2747-4c5f-4293-9bef-f63201820254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'new_model.merge_and_unload' is a method that merges the model and unloads it from memory.\n",
    "# The merged model is stored in 'merged_model'.\n",
    "\n",
    "merged_model = new_model.merge_and_unload()\n",
    "\n",
    "# 'merged_model.save_pretrained' is a method that saves the merged model.\n",
    "# The model is saved in the directory \"merged_model\".\n",
    "# 'trust_remote_code' is set to True, which means that the model will trust and execute remote code.\n",
    "# 'safe_serialization' is set to True, which means that the model will use safe serialization.\n",
    "\n",
    "merged_model.save_pretrained(\"merged_model\", trust_remote_code=True, safe_serialization=True)\n",
    "\n",
    "# 'tokenizer.save_pretrained' is a method that saves the tokenizer.\n",
    "# The tokenizer is saved in the directory \"merged_model\".\n",
    "\n",
    "tokenizer.save_pretrained(\"merged_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cf4598-3d2e-44d7-8534-4509d887f010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to push the merged model and the tokenizer to the Hugging Face Model Hub.\n",
    "\n",
    "# 'merged_model.push_to_hub' is a method that pushes the merged model to the Hugging Face Model Hub.\n",
    "# 'hf_model_repo' is the name of the repository on the Hugging Face Model Hub where the model will be saved.\n",
    "merged_model.push_to_hub(hf_model_repo)\n",
    "\n",
    "# 'tokenizer.push_to_hub' is a method that pushes the tokenizer to the Hugging Face Model Hub.\n",
    "# 'hf_model_repo' is the name of the repository on the Hugging Face Model Hub where the tokenizer will be saved.\n",
    "tokenizer.push_to_hub(hf_model_repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acf6ca8",
   "metadata": {},
   "source": [
    "### Model Inference and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e507c8b0-223d-443b-a7b9-f6f09c7c4ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'hf_model_repo' is a variable that holds the name of the repository on the Hugging Face Model Hub.\n",
    "# This is where the trained and merged model, as well as the tokenizer, have been saved.\n",
    "hf_model_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1999dfb1",
   "metadata": {},
   "source": [
    "#### Retrieve the Model and Tokenizer from the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76df7587-0cc6-44cf-a0a1-5be4b018c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to load the model and tokenizer from the Hugging Face Model Hub.\n",
    "\n",
    "# 'torch' is a library that provides a wide range of functionalities for tensor computations with strong GPU acceleration support.\n",
    "# 'AutoTokenizer' and 'AutoModelForCausalLM' are classes from the 'transformers' library that provide a tokenizer and a causal language model, respectively.\n",
    "# 'set_seed' is a function from the 'transformers' library that sets the seed for generating random numbers, which can be used for reproducibility.\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "\n",
    "# 'set_seed(1234)' sets the seed for generating random numbers to 1234.\n",
    "set_seed(1234)  # For reproducibility\n",
    "\n",
    "# 'AutoTokenizer.from_pretrained' is a method that loads a pre-trained tokenizer.\n",
    "# The tokenizer is loaded from 'hf_model_repo', which is the name of the repository on the Hugging Face Model Hub where the tokenizer was saved.\n",
    "# 'trust_remote_code' is set to True, which means that the tokenizer will trust and execute remote code.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"merged_model\")\n",
    "\n",
    "# 'AutoModelForCausalLM.from_pretrained' is a method that loads a pre-trained causal language model.\n",
    "# The model is loaded from 'hf_model_repo', which is the name of the repository on the Hugging Face Model Hub where the model was saved.\n",
    "# 'trust_remote_code' is set to True, which means that the model will trust and execute remote code.\n",
    "# 'torch_dtype' is set to \"auto\", which means that the model will automatically choose the data type for its computations.\n",
    "# 'device_map' is set to \"cuda\", which means that the model will use the CUDA device for its computations.\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"merged_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115d524c",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b831730-d225-42c1-a56b-eab3ce47aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cf321d-c065-48c0-86b4-6c4aff1ef064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block defines a function 'test_inference' that performs inference on a given prompt.\n",
    "\n",
    "# 'prompt' is the input to the function. It is the text that the model will generate a response to.\n",
    "\n",
    "# 'pipe.tokenizer.apply_chat_template' is a method that applies the chat template to the prompt.\n",
    "# The prompt is wrapped in a list and formatted as a dictionary with \"role\" set to \"user\" and \"content\" set to the prompt.\n",
    "# 'tokenize' is set to False, which means that the method will not tokenize the prompt.\n",
    "# 'add_generation_prompt' is set to True, which means that the method will add a generation prompt to the prompt.\n",
    "# The formatted prompt is stored back in the 'prompt' variable.\n",
    "\n",
    "# 'pipe' is the text generation pipeline that was created earlier.\n",
    "# It is called with the formatted prompt and several parameters that control the text generation process.\n",
    "# 'max_new_tokens=256' limits the maximum number of new tokens that can be generated.\n",
    "# 'do_sample=True' enables sampling, which means that the model will generate diverse responses.\n",
    "# 'num_beams=1' sets the number of beams for beam search to 1, which means that the model will generate one response.\n",
    "# 'temperature=0.3' controls the randomness of the responses. Lower values make the responses more deterministic.\n",
    "# 'top_k=50' limits the number of highest probability vocabulary tokens to consider for each step.\n",
    "# 'top_p=0.95' enables nucleus sampling and sets the cumulative probability of parameter tokens to 0.95.\n",
    "# 'max_time=180' limits the maximum time for the generation process to 180 seconds.\n",
    "# The generated responses are stored in the 'outputs' variable.\n",
    "\n",
    "# The function returns the first generated response.\n",
    "# The response is stripped of the prompt and any leading or trailing whitespace.\n",
    "\n",
    "\n",
    "# Original version from the notebook - not able to call model pipeline\n",
    "\"\"\"\n",
    "def test_inference(prompt):\n",
    "    prompt = pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, num_beams=1, temperature=0.3, top_k=50, top_p=0.95,\n",
    "                   max_time= 180) #, eos_token_id=eos_token)\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()\n",
    "\"\"\"\n",
    "\n",
    "# Modified version by He Zhang Oct. 2024 - able to call model pipeline with success!\n",
    "def test_inference(prompt):\n",
    "    prompt = pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe([{\"role\":\"user\", \"content\":prompt}], max_new_tokens=256, do_sample=True, num_beams=1, temperature=0.3, top_k=50, top_p=0.95,\n",
    "                   max_time= 180) #, eos_token_id=eos_token)\n",
    "    return outputs[0]['generated_text'][1][\"content\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42659139-d639-4006-85cd-756a613875c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block calls the 'test_inference' function with the first message in the test set of 'dataset_chatml' as the prompt.\n",
    "# 'test_inference' performs inference on the prompt and returns a generated response.\n",
    "# The response is printed to the console.\n",
    "print(test_inference(dataset_chatml['test'][0]['messages'][0]['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c20ff81",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798a2325-bc1f-40dd-b9a6-45d1bc786ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa952e6-1bac-410e-86e4-7ca1aa6171e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics are used to measure the performance of the model on certain tasks.\n",
    "import evaluate\n",
    "\n",
    "rouge_metric = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee527335-6256-4367-b020-113ea9426556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the rouge score\n",
    "rouge_metric.compute(predictions=[\"hi, my name is He Zhang\"], \n",
    "                     references=[\"hello, this is He Zhang\"], \n",
    "                     use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98904cd-0761-4aa3-be69-5a8b1d5e9948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the rouge score\n",
    "rouge_metric.compute(predictions=[\"hi, my name is He Zhang\"], \n",
    "                     references=[\"hi, this is Lily.\"], \n",
    "                     use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a6e58-f0ec-4396-8ee1-ec6837ff4d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block defines a function 'calculate_rogue' that calculates the ROUGE score for a given row in the dataset.\n",
    "\n",
    "# 'row' is the input to the function. It is a row in the dataset that contains a message and its corresponding output.\n",
    "\n",
    "# 'test_inference(row['messages'][0]['content'])' calls the 'test_inference' function with the first message in the row as the prompt.\n",
    "# 'test_inference' performs inference on the prompt and returns a generated response.\n",
    "# The response is stored in the 'response' variable.\n",
    "\n",
    "# 'rouge_metric.compute' is a method that calculates the ROUGE score for the generated response and the corresponding output in the row.\n",
    "# 'predictions' is set to the generated response and 'references' is set to the output in the row.\n",
    "# 'use_stemmer' is set to True, which means that the method will use a stemmer to reduce words to their root form.\n",
    "# The calculated ROUGE score is stored in the 'result' variable.\n",
    "\n",
    "# The 'result' dictionary is updated to contain the F-measure of each ROUGE score multiplied by 100.\n",
    "# The F-measure is a measure of a test's accuracy that considers both the precision and the recall of the test.\n",
    "\n",
    "# The 'response' is added to the 'result' dictionary.\n",
    "\n",
    "# The function returns the 'result' dictionary.\n",
    "def calculate_rogue(row):\n",
    "    response = test_inference(row['messages'][0]['content'])\n",
    "    result = rouge_metric.compute(predictions=[response], references=[row['output']], use_stemmer=True)\n",
    "    #result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    result['response']=response\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e49ece6-8719-4795-b14e-b7881be240e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# '%%time' is a magic command in Jupyter notebooks that measures the execution time of the cell.\n",
    "\n",
    "# 'dataset_chatml['test'].select(range(0,500))' selects the first 500 elements from the test set in the 'dataset_chatml' dataset.\n",
    "\n",
    "# '.map(calculate_rogue, batched=False)' applies the 'calculate_rogue' function to each element in the selected subset.\n",
    "# 'calculate_rogue' calculates the ROUGE score for each element.\n",
    "# 'batched' is set to False, which means that the function will be applied to each element individually, not in batches.\n",
    "\n",
    "# The results are stored in the 'metricas' variable.\n",
    "metricas = dataset_chatml['test'].select(range(0, 10)).map(calculate_rogue, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50227501-eed5-4860-8992-390927ec2efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b64cf0d-905c-46af-9d62-4f3c4926c640",
   "metadata": {},
   "outputs": [],
   "source": [
    "metricas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08e363b-dc90-4eb2-b129-937ef1226166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block prints the mean of the ROUGE-1, ROUGE-2, ROUGE-L, and ROUGE-Lsum scores in the 'metricas' dictionary.\n",
    "import numpy as np\n",
    "# 'np.mean(metricas['rouge1'])' calculates the mean of the ROUGE-1 scores.\n",
    "# 'np.mean(metricas['rouge2'])' calculates the mean of the ROUGE-2 scores.\n",
    "# 'np.mean(metricas['rougeL'])' calculates the mean of the ROUGE-L scores.\n",
    "# 'np.mean(metricas['rougeLsum'])' calculates the mean of the ROUGE-Lsum scores.\n",
    "\n",
    "# 'print' is used to print the calculated means to the console.\n",
    "print(\"Rouge 1 Mean: \",np.mean(metricas['rouge1']))\n",
    "print(\"Rouge 2 Mean: \",np.mean(metricas['rouge2']))\n",
    "print(\"Rouge L Mean: \",np.mean(metricas['rougeL']))\n",
    "print(\"Rouge Lsum Mean: \",np.mean(metricas['rougeLsum']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afff25d4-c467-4005-b98d-ed14be86258a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1ce7a3-465d-4070-9501-535db66409e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
