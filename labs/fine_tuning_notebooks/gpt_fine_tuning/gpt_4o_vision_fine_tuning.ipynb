{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5771947d",
   "metadata": {},
   "source": [
    "## Vision Fine-Tuning GPT-4o Model - A Python SDK Experience\n",
    "\n",
    "Learn how to vision fine-tune the <code>gpt-4o-2024-08-06</code> model using Python SDK. \n",
    "\n",
    "This notebook is inspired by the vision fine-tuning [notebook](https://github.com/Azure/gen-cv/blob/main/vision-fine-tuning/01-AOAI-vision-fine-tuning-starter/README.md) from Andreas Kopp's [GenCV Accelerator](https://github.com/Azure/gen-cv).\n",
    "\n",
    "You can either run this notebook locally or run on an <code>AML CPU Compute Standard_D13_v2</code> with Kernel type <code>Python 3.10 - SDK v2</code>  \n",
    "\n",
    "He Zhang, Jan. 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a877936",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "* Learn the [what, why, and when to use fine-tuning.](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/fine-tuning-considerations)\n",
    "* An Azure subscription.\n",
    "* Access to Azure OpenAI Service.\n",
    "* An Azure OpenAI resource created in the supported fine-tuning region (e.g. Sweden Central).\n",
    "* A deployment of <code>gpt-4o</code> base model, with its deployment name as \"gpt-4o\" for simplicity.  \n",
    "* Prepare Training and Validation datasets:\n",
    "  * at least 50 high-quality samples (preferably 1,000s) are required.\n",
    "  * must be formatted in the JSON Lines (JSONL) document with UTF-8 encoding.\n",
    "  * for this test notebook, we utilize ChartQA dataset presented in [ChartQA: ACL 2022](https://aclanthology.org/2022.findings-acl.177). \n",
    "* Python version at least: <code>3.10</code>\n",
    "* Python libraries: <code>json, requests, os, pandas, PIL, base64, IPython, tqdm, python-dotenv, tenacity, datasets, matplotlib, azure.identity, openai</code>\n",
    "* The OpenAI Python library version for this test notebook: <code>1.58.1</code>\n",
    "* [Jupyter Notebooks](https://jupyter.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4140c3f1",
   "metadata": {},
   "source": [
    "### Step 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b341d311",
   "metadata": {},
   "source": [
    "#### Retrieve the Azure OpenAI API key and endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aad8ca",
   "metadata": {},
   "source": [
    "Go to your Azure OpenAI resource in the Azure portal. The Endpoint and Keys can be found in the Resource Management section.  \n",
    "\n",
    "<img src=\"../../images/screenshot-aoai-keys-and-endpoint.png\" alt=\"Screenshot of the Azure OpenAI resource management pane.\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1ab250",
   "metadata": {},
   "source": [
    "#### Configure credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f7f2ef",
   "metadata": {},
   "source": [
    "Copy the <code>Endpoint</code> and access <code>KEY</code> (you can use either <code>KEY 1</code> or <code>KEY 2</code>), and paste them accordingly to the variables in the file <code>azure.env</code>. Save the file and close it. **Do not** distribute this file as this contains credential information! \n",
    "<img src=\"../../images/screenshot-azure-env-file.png\" alt=\"Screenshot of the azure.env file that contains credential information - do not show it to others!\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaa5d63",
   "metadata": {},
   "source": [
    "#### Install required Python libraries (if not done yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eac0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai\n",
    "%pip install tenacity\n",
    "%pip install datasets\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7cf340",
   "metadata": {},
   "source": [
    "#### Import required Python libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e07b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from openai import AzureOpenAI\n",
    "from io import BytesIO, StringIO\n",
    "from datasets import load_dataset\n",
    "from IPython.display import display\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671043c0",
   "metadata": {},
   "source": [
    "#### Load environmental variables to assign credentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72cf0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env. file\n",
    "load_dotenv(\"azure.env\")\n",
    "\n",
    "# Assign Azure resources  \n",
    "subscription_id = os.getenv(\"SUBSCRIPTION_ID\") # name of the Azure Subscription ID\n",
    "resource_name = os.getenv(\"AOAI_RESOURCE\") # name of the AOAI resource\n",
    "rg_name = os.getenv(\"RESOURCE_GROUP\") # name of the resource group\n",
    "\n",
    "# Assign AOAI credentials \n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-10-21\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d5aa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test AOAI connection\n",
    "completion = client.chat.completions.create(  \n",
    "    model=\"gpt-4o\",  \n",
    "    messages=[{\"role\":\"user\", \"content\":\"hello\"}],  \n",
    "    max_tokens=500,  \n",
    "    temperature=0.7)\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d4c020",
   "metadata": {},
   "source": [
    "#### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385fd631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image, quality=100):\n",
    "    \"\"\" Encode an image into a base64 string in JPEG format. \"\"\"\n",
    "\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')  # Convert to RGB\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"JPEG\", quality=quality) \n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "def date_sorted_df(details_dict):\n",
    "    \"\"\" Create a pandas DataFrame from a dictionary and sort it by a 'created' or 'created_at' timestamp column for displaying OpenAI API tables. \"\"\"\n",
    "    df = pd.DataFrame(details_dict)\n",
    "    \n",
    "    if 'created' in df.columns:\n",
    "        df.rename(columns={'created': 'created_at'}, inplace=True)\n",
    "    \n",
    "    # Convert 'created_at' from Unix timestamp to human-readable date/time format\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'], unit='s').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    if 'finished_at' in df.columns:\n",
    "        # Convert 'finished_at' from Unix timestamp to human-readable date/time format, keeping null values as is\n",
    "        df['finished_at'] = pd.to_datetime(df['finished_at'], unit='s', errors='coerce').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Sort DataFrame by 'created_at' in descending order\n",
    "    df = df.sort_values(by='created_at', ascending=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "def show_ft_metrics(results_df, window_size=5):\n",
    "    \"\"\" Plot fine-tuning metrics including loss and accuracy for training and validation. \"\"\"\n",
    "\n",
    "    # Drop rows where valid_loss is NaN or valid_loss is -1.0\n",
    "    filtered_df = results_df.dropna(subset=['valid_loss'])\n",
    "    filtered_df = filtered_df.loc[filtered_df['valid_loss'] != -1.0]\n",
    "\n",
    "    # Compute rolling means\n",
    "    results_df_smooth = results_df.rolling(window=window_size).mean()\n",
    "    filtered_df_smooth = filtered_df.rolling(window=window_size).mean()\n",
    "\n",
    "    # Plot the curves\n",
    "    plt.figure(figsize=(16, 10))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(results_df_smooth['step'], results_df_smooth['train_loss'],  color='blue')\n",
    "    plt.title('Train Loss')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(results_df_smooth['step'], results_df_smooth['train_mean_token_accuracy'], color='green')\n",
    "    plt.title('Train Mean Token Accuracy')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(filtered_df_smooth['step'], filtered_df_smooth['valid_loss'], color='red')\n",
    "    plt.title('Validation Loss')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(filtered_df_smooth['step'], filtered_df_smooth['valid_mean_token_accuracy'], color='orange')\n",
    "    plt.title('Validation Mean Token Accuracy')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31a5fb5",
   "metadata": {},
   "source": [
    "### Step 2: Prepare Training, Validation, and Testing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ff3a96",
   "metadata": {},
   "source": [
    "Fine-tuning for images is possible with JSONL dataset files similar to the process of sending images as input to the chat completion API.\n",
    "Images can be provided as HTTP URLs (as shown below) or data URLs containing base64-encoded images.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    { \"role\": \"system\", \"content\": \"You are an assistant that identifies uncommon cheeses.\" },\n",
    "    { \"role\": \"user\", \"content\": \"What is this cheese?\" },\n",
    "    { \"role\": \"user\", \"content\": [\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": \"https://upload.wikimedia.org/wikipedia/commons/3/36/Danbo_Cheese.jpg\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    { \"role\": \"assistant\", \"content\": \"Danbo\" }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea8bf47",
   "metadata": {},
   "source": [
    "This demo notebooks utilizes the ChartQA dataset introduced by Masry et al. in their paper, *ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning* (Findings of ACL 2022). For more details, one can refer to their publication: [ChartQA: ACL 2022](https://aclanthology.org/2022.findings-acl.177)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1fb8a3",
   "metadata": {},
   "source": [
    "The following cells converts the ChartQA dataset from HuggingFace into this JSONL format by using base64-encoded images. Depending on your training data format, you will likely need to perform a few changes for reusing the code for your other use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf98668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ChartQA dataset from Hugging Face server\n",
    "ds = load_dataset(\"HuggingFaceM4/ChartQA\")\n",
    "display(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35a2eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a subset of training, validation, and test examples for simplicity\n",
    "train_samples = 8000\n",
    "val_samples = 1000\n",
    "test_samples = 800\n",
    "\n",
    "ds_train = ds['train'].shuffle(seed=42).select(range(train_samples))\n",
    "ds_val = ds['val'].shuffle(seed=42).select(range(val_samples))\n",
    "ds_test = ds['test'].shuffle(seed=42).select(range(test_samples))\n",
    "\n",
    "# convert to pandas dataframe\n",
    "ds_train = ds_train.to_pandas()\n",
    "ds_val = ds_val.to_pandas()\n",
    "ds_test = ds_test.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d8629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check some samples at this stage\n",
    "ds_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee315d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert byte strings to images\n",
    "ds_train['image'] = ds_train['image'].apply(lambda x: Image.open(BytesIO(x['bytes'])))\n",
    "ds_val['image'] = ds_val['image'].apply(lambda x: Image.open(BytesIO(x['bytes'])))\n",
    "ds_test['image'] = ds_test['image'].apply(lambda x: Image.open(BytesIO(x['bytes'])))\n",
    "\n",
    "# Convert array type of 'label' column into string only if the current data type is object\n",
    "if ds_train['label'].dtype == 'object':\n",
    "    ds_train['label'] = ds_train['label'].apply(lambda x: x[0])\n",
    "\n",
    "if ds_val['label'].dtype == 'object':\n",
    "    ds_val['label'] = ds_val['label'].apply(lambda x: x[0])\n",
    "\n",
    "if ds_test['label'].dtype == 'object':\n",
    "    ds_test['label'] = ds_test['label'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4880def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check some samples at this stage\n",
    "ds_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f0814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename certain columns\n",
    "ds_train = ds_train.rename(columns={'query': 'question', 'label': 'answer'})\n",
    "ds_val = ds_val.rename(columns={'query': 'question', 'label': 'answer'})\n",
    "ds_test = ds_test.rename(columns={'query': 'question', 'label': 'answer'})\n",
    "\n",
    "# Select certain columns\n",
    "ds_train = ds_train[['question', 'answer', 'image']]\n",
    "ds_val = ds_val[['question', 'answer', 'image']]\n",
    "ds_test = ds_test[['question', 'answer', 'image']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6982971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check some samples at this stage\n",
    "ds_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1e74e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review a random training example\n",
    "idx=3\n",
    "print('QUESTION:', ds_train.iloc[idx]['question'])\n",
    "display(ds_train.iloc[idx]['image'])\n",
    "print('ANSWER:', ds_train.iloc[idx]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb6f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset splits as local JSONL files\n",
    "project_name = \"chart-qa-v4\"\n",
    "splits = ['train', 'val', 'test']\n",
    "datasets = [ds_train, ds_val, ds_test]\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a Vision Language Model specialized in interpreting visual data from chart images.\n",
    "Your task is to analyze the provided chart image and respond to queries with concise answers, usually a single word, number, or short phrase.\n",
    "The charts include a variety of types (e.g., line charts, bar charts) and contain colors, labels, and text.\n",
    "Focus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.\"\"\"\n",
    "\n",
    "for split, dataset in zip(splits, datasets):\n",
    "    dataset_file = f\"{project_name}-{split}.jsonl\"\n",
    "    print(f\"Generating {dataset_file} with {dataset.shape[0]} samples.\")\n",
    "    \n",
    "    json_data = []\n",
    "    base64_prefix = \"data:image/jpeg;base64,\"\n",
    "    \n",
    "    for idx, example in tqdm(enumerate(dataset.itertuples()), total=dataset.shape[0]):\n",
    "        try:\n",
    "            system_message = {\"role\": \"system\", \"content\": SYSTEM_PROMPT}\n",
    "            \n",
    "            encoded_image = encode_image(example.image, quality=80)\n",
    "            user_message = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": f\"Question [{idx}]: {example.question}\"},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"{base64_prefix}{encoded_image}\"}}\n",
    "                ]\n",
    "            }\n",
    "            assistant_message = {\"role\": \"assistant\", \"content\": example.answer}\n",
    "\n",
    "            json_data.append({\"messages\": [system_message, user_message, assistant_message]})\n",
    "        except KeyError as e:\n",
    "            print(f\"Missing field in example {idx}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example {idx}: {e}\")\n",
    "    \n",
    "    with open(dataset_file, \"w\") as f:\n",
    "        for message in json_data:\n",
    "            json.dump(message, f)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a16c5c",
   "metadata": {},
   "source": [
    "### Step 3: Upload Datasets for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501269ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload training file\n",
    "train_file = client.files.create(\n",
    "  file=open(f\"{project_name}-train.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "# upload validation file\n",
    "val_file = client.files.create(\n",
    "  file=open(f\"{project_name}-val.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b293c9",
   "metadata": {},
   "source": [
    "### Step 4: Configure and Start Fine-Tuning Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93dd1bf",
   "metadata": {},
   "source": [
    "Here is some guidance if you want to adjust the hyperparameters of the fine-tuning process. You can keep them as `None` to use default values. \n",
    "\n",
    "| Hyperparameter                       | Description                                                                                                                                                                              |\n",
    "|-----------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| `Batch size`                            | The batch size to use for training. When set to default, batch_size is calculated as 0.2% of examples in training set and the max is 256.                                                           |\n",
    "| `Learning rate multiplier` | The fine-tuning learning rate is the original learning rate used for pre-training multiplied by this multiplier. We recommend experimenting with values between 0.5 and 2. Empirically, we've found that larger learning rates often perform better with larger batch sizes. Must be between 0.0 and 5.0. |\n",
    "| `Number of epochs`       | Number of training epochs. An epoch refers to one full cycle through the data set. If set to default, number of epochs will be determined dynamically based on the input data. |\n",
    "| `Seed`  | The seed controls the reproducibility of the job. Passing in the same seed and job parameters should produce the same results, but may differ in rare cases. If a seed is not specified, one will be generated for you. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a2e12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create fine tuning job\n",
    "file_train = train_file.id\n",
    "file_val = val_file.id\n",
    "\n",
    "ft_job = client.fine_tuning.jobs.create(\n",
    "  suffix=project_name,\n",
    "  training_file=file_train,\n",
    "  validation_file=file_val, # validation file is optional\n",
    "  model=\"gpt-4o-2024-08-06\", # baseline model name (not the deployment name)\n",
    "  seed=None,\n",
    "  hyperparameters={\n",
    "    \"n_epochs\" : None,\n",
    "    \"batch_size\" : None,\n",
    "    \"learning_rate_multiplier\" : None,\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6ae85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the fine-tuning job status\n",
    "client.fine_tuning.jobs.list(limit=1).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10a1c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List 5 recent fine-tuning jobs\n",
    "ft_jobs = client.fine_tuning.jobs.list(limit=5).to_dict()\n",
    "\n",
    "display(date_sorted_df(pd.DataFrame(ft_jobs['data'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0ca64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the name of a fine-tuned model\n",
    "ft_job = client.fine_tuning.jobs.retrieve(\"ftjob-0a4c9b22f32e44b4a133c83edc31107b\")\n",
    "fine_tuned_model = ft_job.to_dict()['fine_tuned_model']\n",
    "fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a29b2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve fine-tuning metrics from result file\n",
    "result_file_id = ft_job.to_dict()['result_files'][0]\n",
    "results_content = client.files.content(result_file_id).content.decode()\n",
    "\n",
    "data_io = StringIO(results_content)\n",
    "results_df = pd.read_csv(data_io)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687ea355",
   "metadata": {},
   "source": [
    "Take a look at this table for an interpretation of above diagrams:  \n",
    "\n",
    "| Metric                       | Description                                                                                                                                                                              |\n",
    "|-----------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| `step`                            | The number of the training step. A training step represents a single pass, forward and backward, on a batch of training data.                                                           |\n",
    "| `train_loss`, `validation_loss` | The loss for the training / validation batch |\n",
    "| `train_mean_token_accuracy`       | The percentage of tokens in the training batch correctly predicted by the model. For example, if the batch size is set to 3 and your data contains completions [[1, 2], [0, 5], [4, 2]], this value is set to 0.83 (5 of 6) if the model predicted [[1, 1], [0, 5], [4, 2]]. |\n",
    "| `validation_mean_token_accuracy`  | The percentage of tokens in the validation batch correctly predicted by the model. For example, if the batch size is set to 3 and your data contains completions [[1, 2], [0, 5], [4, 2]], this value is set to 0.83 (5 of 6) if the model predicted [[1, 1], [0, 5], [4, 2]]. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54756bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation metrics\n",
    "show_ft_metrics(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682f8b26",
   "metadata": {},
   "source": [
    "### Step 6: Deploy the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01b5268",
   "metadata": {},
   "source": [
    "__Note__: Only one deployment is permitted for a customized model. An error occurs if you select an already-deployed customized model.  \n",
    "\n",
    "The code below shows how to deploy the model using the Control Plane API. Take a look at the [Azure OpenAI fine-tuning documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo&pivots=programming-language-python#deploy-fine-tuned-model) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66486ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List existing models\n",
    "my_models = client.models.list().to_dict()\n",
    "models_df = date_sorted_df(my_models['data'])\n",
    "\n",
    "cols = ['status', 'capabilities', 'lifecycle_status', 'id', 'created_at', 'model']\n",
    "bold_start, bold_end = '\\033[1m', '\\033[0m'\n",
    "\n",
    "print(f'Models of AOAI resource {bold_start}{resource_name}{bold_end}:')\n",
    "display(models_df[cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e400039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the fine-tuned model as an Azure Managed Online Endpoint\n",
    "aoai_deployment_name = project_name # AOAI deployment name. Use as model parameter for inferencing\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "token = credential.get_token(\"https://management.azure.com/.default\").token\n",
    "\n",
    "deploy_params = {'api-version': \"2023-05-01\"} \n",
    "deploy_headers = {'Authorization': 'Bearer {}'.format(token), 'Content-Type': 'application/json'}\n",
    "\n",
    "deploy_data = {\n",
    "    \"sku\": {\"name\": \"standard\", \"capacity\": 1}, \n",
    "    \"properties\": {\n",
    "        \"model\": {\n",
    "            \"format\": \"OpenAI\",\n",
    "            \"name\": fine_tuned_model, # retrieve this value from the previous calls, it will look like gpt-35-turbo-0613.ft-b044a9d3cf9c4228b5d393567f693b83\n",
    "            \"version\": \"1\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "deploy_data = json.dumps(deploy_data)\n",
    "\n",
    "request_url = f'https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{rg_name}/providers/Microsoft.CognitiveServices/accounts/{resource_name}/deployments/{aoai_deployment_name}'\n",
    "\n",
    "print('Creating a new deployment...')\n",
    "\n",
    "r = requests.put(request_url, params=deploy_params, headers=deploy_headers, data=deploy_data)\n",
    "\n",
    "print(r)\n",
    "print(r.reason)\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efa764e",
   "metadata": {},
   "source": [
    "### Step 7: Test the Deployed Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23718e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to query the vision fine-tuned model\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_fixed(10))\n",
    "def query_image(image, question, deployment='gpt-4o'):\n",
    "\n",
    "    encoded_image_url = f\"data:image/jpeg;base64,{encode_image(image, quality=50)}\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": question},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": encoded_image_url}}\n",
    "            ]}\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e41eb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "question = ds_test.iloc[idx]['question']\n",
    "img = ds_test.iloc[idx]['image']\n",
    "answer = ds_test.iloc[idx]['answer']\n",
    "\n",
    "display(question)\n",
    "display(img)\n",
    "display(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cfc0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check fine-tuned model result\n",
    "print(query_image(img, question, aoai_deployment_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f332d68",
   "metadata": {},
   "source": [
    "### Step 8: Evaluate the Base GPT-4o and the Fine-Tuned GPT-4o Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaaf429",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test_eval = ds_test.copy().head(100)\n",
    "ds_test_eval.info()\n",
    "ds_test_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333644f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Process test dataset with baseline model\n",
    "ds_test_eval['gpt-4o-base-pred'] = ds_test_eval.apply(lambda row: query_image(row['image'], row['question'], 'gpt-4o'), axis=1)\n",
    "ds_test_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc96a298",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Process test dataset with fine-tuned model\n",
    "ds_test_eval['gpt-4o-ft-pred'] = ds_test_eval.apply(lambda row: query_image(row['image'], row['question'], 'chart-qa-v4'), axis=1)\n",
    "ds_test_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7d5e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to use LLM for results comparisons\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_fixed(10))\n",
    "def evaluate(question, ground_truth_answer, predicted_answer, deployment='gpt-4o'):\n",
    "    \n",
    "    EVAL_SYSTEM_PROMPT = \"\"\"You evaluate the factual correctness of a predicted answer about a diagram with a ground truth answer. \n",
    "                            The predicted answer might be formulated in a different way. Your only concern is if the predicted answer is correct from a factual perspective. \n",
    "                            You are provided with the original question, the ground truth answer and the predicted answer.\n",
    "                            You respond with either CORRECT or INCORRECT\"\"\"\n",
    "\n",
    "    user_prompt = f\"Original question: {question} \\nGround truth answer: {ground_truth_answer}\\nPredicted answer: {predicted_answer}\" \n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": EVAL_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ee0c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the evaluation function above\n",
    "print(evaluate('what is the diagram title?', 'comparison of tax rates in US states', 'Tax Rate Comparison Across US States'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9365f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate prediction accuracy of baseline model\n",
    "ds_test_eval['gpt-4o-base-eval'] = ds_test_eval.apply(lambda row: evaluate(row['question'], row['answer'], row['gpt-4o-base-pred'], 'gpt-4o'), axis=1)\n",
    "ds_test_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc98a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate prediction accuracy of fine-tuned model\n",
    "ds_test_eval['gpt-4o-ft-eval'] = ds_test_eval.apply(lambda row: evaluate(row['question'], row['answer'], row['gpt-4o-ft-pred'], 'gpt-4o'), axis=1)\n",
    "ds_test_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5484e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a bar chart to show the accuracy comparison result\n",
    "base_correct_count = ds_test_eval['gpt-4o-base-eval'].value_counts().get(\"CORRECT\", 0)\n",
    "base_eval_observations = ds_test_eval.shape[0]\n",
    "ft_correct_count = ds_test_eval['gpt-4o-ft-eval'].value_counts().get(\"CORRECT\", 0)\n",
    "ft_eval_observations = ds_test_eval.shape[0]\n",
    "\n",
    "chart_data = {\n",
    "    'title' : 'GPT-4o ChartQA accuracy - baseline vs fine-tuned model', \n",
    "    'baseline' : 'GPT-4o 0806',\n",
    "    'fine-tuned' : 'GPT-4o 0806 fine-tuned',\n",
    "    'baseline accuracy' : base_correct_count / base_eval_observations,\n",
    "    'fine-tuned accuracy' : ft_correct_count / ft_eval_observations,\n",
    "    \n",
    "}\n",
    "\n",
    "# Extract data for plotting\n",
    "models = [chart_data['baseline'], chart_data['fine-tuned']]\n",
    "accuracies = [chart_data['baseline accuracy'], chart_data['fine-tuned accuracy']]\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(models, accuracies, color=['blue', 'green'])\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title(chart_data['title'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Model')\n",
    "\n",
    "# Annotate bars with accuracy values\n",
    "for i, acc in enumerate(accuracies):\n",
    "    plt.text(i, acc + 0.005, f\"{acc:.4f}\", ha='center', fontsize=10)\n",
    "\n",
    "# Display the chart\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234544e1",
   "metadata": {},
   "source": [
    "### Step 9: Delete the Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb4fff9",
   "metadata": {},
   "source": [
    "It is **strongly recommended** that once you're done with this tutorial and have tested a few chat completion calls against your fine-tuned model, that you delete the model deployment, since the fine-tuned / customized models have an [hourly hosting cost](https://azure.microsoft.com/zh-cn/pricing/details/cognitive-services/openai-service/#pricing) associated with them once they are deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6655aa10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80537ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
