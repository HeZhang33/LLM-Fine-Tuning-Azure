{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae4ca7d-3ded-46a4-9ec4-b9dbd5996d13",
   "metadata": {},
   "source": [
    "## Deploying DeepSeek-R1-1.5B - A Python SDK Experience\n",
    "\n",
    "Learn how to deploy <code>DeepSeek-R1-1.5B</code> model via vLLM and AML Online Endpoint. \n",
    "\n",
    "This notebook follows a similar approach to Jihua Liu's [AML-DeepSeek](https://github.com/liougehooa/azureml-deepseek/tree/main) [notebook](https://github.com/liougehooa/azureml-deepseek/blob/main/distilled/deepseek_aml_vllm_1.5b.ipynb), but streamlines the process and includes API concurrency evaluations at the end.\n",
    "\n",
    "You can either run this notebook locally or run on an <code>AML CPU Compute Standard_D13_v2</code> with Kernel type <code>Python 3.10 - SDK v2</code>. \n",
    "\n",
    "Note that you need to have at least a <code>16G RAM v100 </code> GPU instance <code>Standard_NC6s_v3</code> available in order to successfully run this demo.\n",
    "\n",
    "He Zhang, Mar. 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf13d8f-e47d-409c-8c27-5fdbc4f47124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install azure-ai-ml\n",
    "#%pip install azure-identity\n",
    "#pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5facbc-db2f-41c2-9b90-53ab5ce054a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import time\n",
    "import asyncio\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from openai import OpenAI\n",
    "from time import perf_counter\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Environment,\n",
    "    BuildContext, \n",
    "    OnlineRequestSettings,\n",
    "    ProbeSettings\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azureml.core.workspace import Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c3a782-2b0f-4b16-91e8-b8c84b9fb0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter details of your Azure Machine Learning workspace\n",
    "subscription_id = \"xxx\"\n",
    "resource_group = \"xxx\"\n",
    "workspace = \"xxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc0605e-c9d7-45c7-99f3-97bf5afe84c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    DefaultAzureCredential(), subscription_id, resource_group, workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21144bf9-98fa-480b-8e46-4f07e6d49c6f",
   "metadata": {},
   "source": [
    "## Deploying DeepSeek as an AML Online Endpoint / API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d64f646-d1a4-4887-ac40-9e79d08ad85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create docker folder\n",
    "!mkdir -p docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2eec6b-fb44-4ef4-8c93-62c5aaf6cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker/Dockerfile\n",
    "\n",
    "## vllm cuda12.1, azure image cuda vision 18.x\n",
    "FROM vllm/vllm-openai:latest\n",
    "ENV MODEL_NAME deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
    "ENTRYPOINT python3 -m vllm.entrypoints.openai.api_server --model $MODEL_NAME $VLLM_ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fe44c3-29dc-4a8a-854f-218d6c04595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment using Docker Context\n",
    "env_docker_context = Environment(\n",
    "    build=BuildContext(path=\"docker\", dockerfile_path='Dockerfile'),\n",
    "    name=\"deepseek-r1-1p5b-qwen\",\n",
    "    description=\"Environment created from a Docker context for vLLM: DeepSeek-R1-Distill-Qwen-1.5B.\",\n",
    "    inference_config = {\n",
    "        \"liveness_route\": {\"port\": 8000, \"path\": \"/health\"},\n",
    "        \"readiness_route\": {\"port\": 8000, \"path\": \"/health\"},\n",
    "        \"scoring_route\": {\"port\": 8000, \"path\": \"/\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "ml_client.environments.create_or_update(env_docker_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f687ab-1448-457d-897c-cf8bccfdd37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an endpoint name\n",
    "endpoint_name_prefix = \"vllms-deepseek-r1-1p5b-qwen\"\n",
    "endpoint_name = f'{endpoint_name_prefix}-ep'\n",
    "\n",
    "# define an Managed Online Endpoint\n",
    "blue_deployment = ManagedOnlineDeployment(\n",
    "    name=\"blue\",\n",
    "    endpoint_name=endpoint_name,\n",
    "    environment=env_docker_context,\n",
    "    environment_variables={\n",
    "        \"MODEL_NAME\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "        \"VLLM_ARGS\": \"--max-model-len 8000 --max-num-seqs 16 --enforce-eager --dtype float16 --trust-remote-code\",\n",
    "        \"HUGGING_FACE_HUB_TOKEN\":\"xxx\"\n",
    "    },\n",
    "    instance_type=\"Standard_NC6s_v3\",\n",
    "    instance_count=1,\n",
    "    request_settings= OnlineRequestSettings(\n",
    "        max_concurrent_requests_per_instance=10,\n",
    "        request_timeout_ms = 120000,\n",
    "        max_queue_wait_ms=240000,\n",
    "    ),\n",
    "    liveness_probe = ProbeSettings(initial_delay=200,\n",
    "                                    period=30,\n",
    "                                    timeout=10,\n",
    "                                    success_threshold=1,\n",
    "                                    failure_threshold=30),\n",
    "    readiness_probe = ProbeSettings(initial_delay=200,\n",
    "                                    period=10,\n",
    "                                    timeout=2,\n",
    "                                    success_threshold=1,\n",
    "                                    failure_threshold=30)\n",
    ")\n",
    "\n",
    "blue_deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173baadb-343a-449b-91f8-6e20d4b9c4f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name = endpoint_name, \n",
    "    description=\"this is an endpoint for vllms: DeepSeek-R1-Distill-Qwen-1.5B.\",\n",
    "    auth_mode=\"key\"\n",
    ")\n",
    "\n",
    "ml_client.online_endpoints.begin_create_or_update(endpoint)\n",
    "ml_client.online_endpoints.get(name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be4cf88-67a8-4e74-9ae5-b1c8dbdde19a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a deployment to the endpoint (this will usually take 5 to 15 minutes)\n",
    "#ml_client.online_deployments.begin_create_or_update(blue_deployment)\n",
    "ml_client.online_deployments.begin_create_or_update(blue_deployment).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e29de4-2e10-4849-a1e4-29b6082d5ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all endpoints\n",
    "for indx, endpoint in enumerate(ml_client.online_endpoints.list()):\n",
    "    print(\"Endpoint:\", indx+1)\n",
    "    print(\"  Kind:\", endpoint.kind)\n",
    "    print(\"  Location:\", endpoint.location)\n",
    "    print(\"  Name:\", endpoint.name)\n",
    "    print(\"  Endpoint:\", endpoint.scoring_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f5b473-4848-4c84-98a0-c8142052707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check deployment log\n",
    "logs = ml_client.online_deployments.get_logs(name=\"blue\", endpoint_name=endpoint_name, lines=500)\n",
    "print(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77504130-61a9-4747-82f4-c54a85306140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint.traffic = {\"blue\": 100, \"green\": 0}\n",
    "endpoint_remote = ml_client.online_endpoints.get(name=endpoint_name)\n",
    "endpoint_remote.traffic = {\"blue\": 100}\n",
    "ml_client.begin_create_or_update(endpoint_remote).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b598f36-9059-496d-85dd-bbb9a75cafe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get endpoint url\n",
    "endpoint = ml_client.online_endpoints.get(name=endpoint_name)\n",
    "endpoint_url = endpoint.scoring_uri\n",
    "print(\"endpoint_url:\", endpoint_url)\n",
    "\n",
    "# get endpoint key\n",
    "keys = ml_client.online_endpoints.get_keys(name=endpoint_name)\n",
    "endpoint_key = keys.primary_key\n",
    "print(\"endpoint_key:\", endpoint_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd5903-edfe-43a9-8925-ea58aa60d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# call the API endpoint using HTTP Requests Completion\n",
    "api_url = endpoint_url + \"v1/completions\"\n",
    "api_key = endpoint_key\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    \"prompt\": \"The city of Shanghai is\",\n",
    "    \"max_tokens\": 300,\n",
    "    \"temperature\": 0.6\n",
    "}\n",
    "\n",
    "response = requests.post(api_url, headers=headers, json=data)\n",
    "print(response.json()[\"choices\"][0][\"text\"])\n",
    "print(\"\\n\", response.json()[\"usage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ff6a9a-7c1b-4390-ac48-6effb86767a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# call the API endpoint using OpenAI Completion\n",
    "openai_base_url = endpoint_url + \"v1\"\n",
    "openai_api_key = endpoint_key\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key,\n",
    "                base_url=openai_base_url)\n",
    "\n",
    "completion = client.completions.create(model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "                                       prompt=\"The city of Shanghai is\",\n",
    "                                       max_tokens=300,\n",
    "                                       temperature=0.6)\n",
    "\n",
    "print(completion.choices[0].text)\n",
    "print(\"\\n\", completion.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7693273-4413-482c-8088-e35356cc26d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# call the API endpoint using OpenAI Chat Completion\n",
    "openai_base_url = endpoint_url + \"v1\"\n",
    "openai_api_key = endpoint_key\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key,\n",
    "                base_url=openai_base_url)\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Tell me something about Shanghai China.\"}\n",
    "    ],\n",
    "    max_tokens=300,\n",
    "    temperature=0.6)\n",
    "\n",
    "print(chat_completion.choices[0].message.content)\n",
    "print(\"\\n\", chat_completion.usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b388cf83-9b91-4a8d-8550-248247602912",
   "metadata": {},
   "source": [
    "## Testing API Performance in Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40de01a-8fbe-4145-9679-dceba137bfa8",
   "metadata": {},
   "source": [
    "### Evaluate API Sequential Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf0956d-cd45-48cd-9ae7-8283f9ab44c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to do sequential calls to the API \n",
    "def call_llm_sequential(\n",
    "    model_path: str = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\", \n",
    "    num_infers: int = 5,\n",
    "    aoai_client: object = client,\n",
    "    messages: list = [{\"role\": \"user\", \"content\": \"Hello.\"}], \n",
    "    max_tokens: int = 500,\n",
    "    temperature: float = 0.6) -> dict:\n",
    "    \"\"\"Evaluate LLM performance in sequential calls.\"\"\"\n",
    "\n",
    "    print(\"=== Measuring latency ===\")\n",
    "    print(f\"model_path={model_path}, num_infers={num_infers}, max_tokens={max_tokens}, temperature={temperature}\")\n",
    "    \n",
    "    # warm up\n",
    "    warmup_start = perf_counter()\n",
    "    _ = aoai_client.chat.completions.create(\n",
    "        model=model_path,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Please just say: Warm-up is done.\"}],\n",
    "        max_tokens=100,\n",
    "        temperature=0.6)\n",
    "    \n",
    "    warmup_latency = perf_counter() - warmup_start\n",
    "    print(\"\\nWarm-up is done! (it takes {:.2f} seconds.)\".format(warmup_latency))\n",
    "\n",
    "    # test serial calls \n",
    "    total_test_start = time.time()\n",
    "    latencies_sec = []\n",
    "    chat_completions = []\n",
    "    for curr_infer in range(num_infers):\n",
    "        single_test_start = perf_counter()\n",
    "        if (curr_infer % 5) == 0:\n",
    "            print(f\"\\nCalling API for the {curr_infer}th time ...\")\n",
    "            \n",
    "        chat_completion = aoai_client.chat.completions.create(\n",
    "            model=model_path,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature)\n",
    "        \n",
    "        single_test_latency = perf_counter() - single_test_start\n",
    "        latencies_sec.append(single_test_latency)\n",
    "        chat_completions.append(chat_completion)\n",
    "    \n",
    "    total_test_end = time.time()\n",
    "    \n",
    "    # compute various metrics\n",
    "    total_dur_sec = np.round((total_test_end - total_test_start), 2)\n",
    "    \n",
    "    # output metrics in a dict.\n",
    "    results = {\n",
    "        \"total_duration_sec\": total_dur_sec,\n",
    "        \"latencies_sec\": latencies_sec,\n",
    "        \"chat_completions\": chat_completions\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa6cca5-6ba0-4c7c-90a0-69ec0690d6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# test API in sequential way\n",
    "model_path = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "num_infers = 5\n",
    "aoai_client = client\n",
    "max_tokens = 1000\n",
    "temperature = 0.6\n",
    "messages = [{\"role\": \"user\", \n",
    "             \"content\": \"What is Shanghai China famous for? Give me top 3 points only.\"}]\n",
    "\n",
    "results = call_llm_sequential(model_path, num_infers, client, messages, max_tokens, temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c45def-e5bf-4da6-9f3c-abfc95767e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate latency metrics\n",
    "latency_avg_sec = np.mean(results[\"latencies_sec\"]).round(2)\n",
    "latency_std_sec = np.std(results[\"latencies_sec\"]).round(2)\n",
    "latency_p95_sec = np.percentile(results[\"latencies_sec\"], 95).round(2)\n",
    "latency_p99_sec = np.percentile(results[\"latencies_sec\"], 99).round(2)\n",
    "total_duration_sec = results[\"total_duration_sec\"]\n",
    "print(\"latency_avg_sec:\", latency_avg_sec)\n",
    "print(\"latency_std_sec:\", latency_std_sec)\n",
    "print(\"latency_p95_sec:\", latency_p95_sec)\n",
    "print(\"latency_p99_sec:\", latency_p99_sec)\n",
    "print(\"total_duration_sec:\", total_duration_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c9df8e-d10a-4e06-ae05-eb187464c55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate token metrics\n",
    "sequential_calls_completion_tokens = [result.usage.completion_tokens for result in results[\"chat_completions\"]]\n",
    "avg_tokens_per_sec = np.sum(sequential_calls_completion_tokens) / total_duration_sec\n",
    "print(\"avg_tokens_per_sec:\", np.round(avg_tokens_per_sec, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83141972-642a-4122-9a79-71cad01cc97f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check individual completions\n",
    "for indx, cm in enumerate(results[\"chat_completions\"]):\n",
    "    print(\"Completion =\", indx+1, \"\\n--------------\\n\", cm.choices[0].message.content, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0676c7-1b6d-4675-acb9-2cda85453842",
   "metadata": {},
   "source": [
    "### Evaluate API Concurrent Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df83e020-375f-4310-b423-2e07995031a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to evaluate API concurrent calls \n",
    "async def call_llm_concurrent(aoai_client, model_path, concurrent_requests, messages, max_tokens, temperature):\n",
    "    \"\"\"Run multiple concurrent OpenAI API requests.\"\"\"\n",
    "    \n",
    "    async def call_openai(aoai_client, model_path, request_id):\n",
    "        \"\"\"Make an asynchronous OpenAI API call using the OpenAI client.\"\"\"\n",
    "        response = await asyncio.to_thread(aoai_client.chat.completions.create,\n",
    "            model=model_path,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature)\n",
    "        return response\n",
    "    \n",
    "    tasks = [call_openai(aoai_client, model_path, i) for i in range(concurrent_requests)]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fb026d-5304-4668-9bf2-e38017efd6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test API in concurrent way\n",
    "model_path = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "concurrent_requests = 5\n",
    "aoai_client = client\n",
    "max_tokens = 1000\n",
    "temperature = 0.6\n",
    "messages = [{\"role\": \"user\", \n",
    "             \"content\": \"What is Shanghai China famous for? Give me top 3 points only.\"}]\n",
    "\n",
    "begin_time = time.time()\n",
    "results = asyncio.run(call_llm_concurrent(aoai_client, model_path, concurrent_requests, messages, max_tokens, temperature))\n",
    "end_time = time.time()\n",
    "total_dur_sec = np.round((end_time - begin_time), 2)\n",
    "print(\"Total time in running {} concurrent API calls: {:.2f}\".format(concurrent_requests, total_dur_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516aca38-abb0-4e56-a6fe-166d3ff9accb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate token metrics\n",
    "concurrent_calls_completion_tokens = [result.usage.completion_tokens for result in results]\n",
    "avg_tokens_per_sec = np.sum(sequential_calls_completion_tokens) / total_duration_sec\n",
    "print(\"avg_tokens_per_sec:\", np.round(avg_tokens_per_sec, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab5c662-c81d-4757-b3e2-9f44b54b54f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check individual completions\n",
    "for indx, result in enumerate(results):\n",
    "    print(\"Completion =\", indx+1, \"\\n--------------\\n\", result.choices[0].message.content, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54d9acc-1cce-4ed6-b2e4-8ee40722b16c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d51e714-e019-4c8d-8eec-3de8df3d22c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
